{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP on SEC Forms Using Doc2vec with Gensim\n",
    "## Introduction\n",
    "Throughout this notebook, we reference <a href=\"https://arxiv.org/pdf/1405.4053.pdf\">Le and Mikolov 2014</a>. \n",
    "\n",
    "### Bag-of-words Model\n",
    "Traditional state-of-the-art document representations are based on the <a href=\"https://en.wikipedia.org/wiki/Bag-of-words_model\">bag-of-words model</a>, which represent input documents as a fixed-length vector. For example, borrowing from the Wikipedia article, the two documents  \n",
    "(1) `John likes to watch movies. Mary likes movies too.`  \n",
    "(2) `John also likes to watch football games.`  \n",
    "are used to construct a length 10 list of words  \n",
    "`[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\"]`  \n",
    "so then we can represent the two documents as fixed length vectors whose elements are the frequencies of the corresponding words in our list  \n",
    "(1) `[1, 2, 1, 1, 2, 1, 1, 0, 0, 0]`  \n",
    "(2) `[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]`  \n",
    "Bag-of-words models are surprisingly effective but lose information about word order. Bag of <a href=\"https://en.wikipedia.org/wiki/N-gram\">n-grams</a> models consider word phrases of length n to capture local word order but suffer from data sparsity and high dimensionality.\n",
    "\n",
    "### Word2vec Model\n",
    "Word2vec uses a shallow neural network to embed words in a high-dimensional vector space. In the resulting vector space, close word vectors have similar contextual meanings and distant word vectors have different contextual meanings. For example, `strong` and `powerful` would be closer together than `strong` and `Paris`. Word2vec models can be trained using two prediction tasks which represent the skip-gram and continuous-bag-of-words models.\n",
    "\n",
    "\n",
    "#### Word2vec - Skip-gram Model\n",
    "The Skip-gram <a href=\"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\">Word2vec</a> Model takes in pairs (word1, word2) generated by moving a window across text data and trains a 1-hidden-layer neural network based on the fake task giving us a predicted probability distribution of nearby words to a given input word. The hidden-to-output weights in the neural network become the word embeddings. So if the hidden layer has 300 neurons, this network woulld give us 300-dimensional word embeddings. We use <a href=\"https://en.wikipedia.org/wiki/One-hot\">one-hot</a> encoding for the words.\n",
    "\n",
    "#### Word2vec - Continuous-bag-of-words Model\n",
    "The Continuous-bag-of-words Word2vec Model is also a 1-hidden-layer neural network. This time, the fake task is to predict the center word based on context words in a window around the center word. Again, the hidden-to-output weights become the word embeddings and we use one-hot encoding.\n",
    "\n",
    "### Paragraph Vector\n",
    "Le and Mikolov 2014 introduces the <i>Paragraph Vector</i>, which outperforms representing a documents by averaging or concatenating the word vectors of a document. We determine the embedding of the paragraph in its vector space in the same way as words - by training a shallow neural network on a fake task. Paragraph Vectors consider local word order but also give us a dense vector representations.\n",
    "\n",
    "#### Paragraph Vector - Distributed Memory (PV-DM)\n",
    "This is the Paragraph Vector Model analogous to the Continuous-bag-of-words Word2vec Model. Paragraph Vectors are obtained by training a neural network on the fake task of inferring a center word based on context words and a context paragraph. A paragraph is a context for all words in the paragraph.\n",
    "\n",
    "#### Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "This is the Paragraph Vector Model analogous to the Skip-gram Word2vec Model. Paragraph Vectors are obtained by training a neural network on the fake task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph.\n",
    "\n",
    "### Requirements\n",
    "The following python modules are dependencies for this notebook:\n",
    "* spacy  \n",
    "* smart_open\n",
    "* testfixtures\n",
    "* sklearn\n",
    "* gensim\n",
    "* python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import smart_open\n",
    "import os.path\n",
    "import spacy\n",
    "import time\n",
    "import glob\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "dirname = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from gensim.models import Doc2Vec\n",
    "from IPython.display import HTML\n",
    "from timeit import default_timer\n",
    "import gensim.models.doc2vec\n",
    "import statsmodels.api as sm\n",
    "from random import shuffle\n",
    "from random import sample\n",
    "import multiprocessing\n",
    "from os import remove\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "import locale\n",
    "import gensim\n",
    "import random\n",
    "import sys\n",
    "import re\n",
    "#import sklearn\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PrepData - OOV Removal, No Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process started...\n",
      "Finished with 2000 files\n",
      "Last file size 200 batch processing running time was 5.919139385223389\n",
      "Finished with 4000 files\n",
      "Last file size 200 batch processing running time was 4.711833715438843\n",
      "Finished with 6000 files\n",
      "Last file size 200 batch processing running time was 4.077444314956665\n",
      "Finished with 8000 files\n",
      "Last file size 200 batch processing running time was 4.98043155670166\n",
      "Finished with 10000 files\n",
      "Last file size 200 batch processing running time was 4.967067241668701\n",
      "Finished with 12000 files\n",
      "Last file size 200 batch processing running time was 4.60893988609314\n",
      "Finished with 14000 files\n",
      "Last file size 200 batch processing running time was 4.950732231140137\n",
      "Finished with 16000 files\n",
      "Last file size 200 batch processing running time was 4.94968843460083\n",
      "Finished with 18000 files\n",
      "Last file size 200 batch processing running time was 5.979551315307617\n",
      "Finished with 20000 files\n",
      "Last file size 200 batch processing running time was 5.809570550918579\n",
      "Finished with 22000 files\n",
      "Last file size 200 batch processing running time was 6.2075958251953125\n",
      "Finished with 24000 files\n",
      "Last file size 200 batch processing running time was 5.627568006515503\n",
      "Finished with 26000 files\n",
      "Last file size 200 batch processing running time was 5.629806995391846\n",
      "Finished with 28000 files\n",
      "Last file size 200 batch processing running time was 3.8712058067321777\n",
      "Finished with 30000 files\n",
      "Last file size 200 batch processing running time was 5.886588096618652\n",
      "Finished with 32000 files\n",
      "Last file size 200 batch processing running time was 5.240128517150879\n",
      "Finished with 34000 files\n",
      "Last file size 200 batch processing running time was 5.430172681808472\n",
      "Finished with 36000 files\n",
      "Last file size 200 batch processing running time was 5.738692045211792\n",
      "Finished with 38000 files\n",
      "Last file size 200 batch processing running time was 4.443613767623901\n",
      "Finished with 40000 files\n",
      "Last file size 200 batch processing running time was 5.109848499298096\n",
      "Finished with 42000 files\n",
      "Last file size 200 batch processing running time was 4.859089374542236\n",
      "Finished with 44000 files\n",
      "Last file size 200 batch processing running time was 4.80933952331543\n",
      "Finished with 46000 files\n",
      "Last file size 200 batch processing running time was 6.571285247802734\n",
      "Finished with 48000 files\n",
      "Last file size 200 batch processing running time was 7.684190034866333\n",
      "Finished with 50000 files\n",
      "Last file size 200 batch processing running time was 6.286274194717407\n",
      "Finished with 52000 files\n",
      "Last file size 200 batch processing running time was 7.628331184387207\n",
      "Finished with 54000 files\n",
      "Last file size 200 batch processing running time was 5.6706554889678955\n",
      "Finished with 56000 files\n",
      "Last file size 200 batch processing running time was 6.146936893463135\n",
      "Finished with 58000 files\n",
      "Last file size 200 batch processing running time was 5.708842515945435\n",
      "Finished with 60000 files\n",
      "Last file size 200 batch processing running time was 6.758649587631226\n",
      "Finished with 62000 files\n",
      "Last file size 200 batch processing running time was 5.7804319858551025\n",
      "Finished with 64000 files\n",
      "Last file size 200 batch processing running time was 6.061731815338135\n",
      "Finished with 66000 files\n",
      "Last file size 200 batch processing running time was 6.107076406478882\n",
      "Finished with 68000 files\n",
      "Last file size 200 batch processing running time was 7.018797874450684\n",
      "Finished with 70000 files\n",
      "Last file size 200 batch processing running time was 7.302898645401001\n",
      "Finished with 72000 files\n",
      "Last file size 200 batch processing running time was 7.5975751876831055\n",
      "Finished with 74000 files\n",
      "Last file size 200 batch processing running time was 5.818881034851074\n",
      "Finished with 76000 files\n",
      "Last file size 200 batch processing running time was 7.094324111938477\n",
      "Finished with 78000 files\n",
      "Last file size 200 batch processing running time was 6.723113536834717\n",
      "Finished with 80000 files\n",
      "Last file size 200 batch processing running time was 7.241600036621094\n",
      "Finished with 82000 files\n",
      "Last file size 200 batch processing running time was 7.091746091842651\n",
      "Finished with 84000 files\n",
      "Last file size 200 batch processing running time was 6.185185194015503\n"
     ]
    }
   ],
   "source": [
    "print(\"Process started...\")\n",
    "\n",
    "# Redirect output to a text file\n",
    "orig_stdout = sys.stdout\n",
    "out_f = open('out_prepdata.txt', 'w')\n",
    "sys.stdout = out_f\n",
    "\n",
    "def processOne(txt):\n",
    "    with smart_open.smart_open(txt, \"rb\") as t:\n",
    "        doc = nlp.make_doc(t.read().decode(\"utf-8\"))\n",
    "        # Approximately top 500 words in a SEC Form are header\n",
    "        removed_stop_words = list(map(lambda x: x.lower_, filter(lambda token: token.is_alpha and not token.is_stop and not token.is_oov, doc)))[500:]\n",
    "        return \" \".join(removed_stop_words)\n",
    "def prepData():\n",
    "    folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg'] \n",
    "    print(\"Preparing dataset...\")\n",
    "    pool = Pool()\n",
    "    tick_counter = 0\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))\n",
    "        batch_size = 200\n",
    "        print(\"Processing {0} files, {1} at a time in {2}\".format(len(txt_files), batch_size, fol))\n",
    "        for i in range(0, len(txt_files), 200):\n",
    "            if (tick_counter != 0 and tick_counter % 10==0):\n",
    "                start_time = time.time()\n",
    "                sys.stdout = orig_stdout\n",
    "                print(\"Finished with {0} files\".format(tick_counter*200))\n",
    "                sys.stdout = out_f\n",
    "            if (i%1000==0):\n",
    "                print(\"Finished processing {0} files\".format(i))\n",
    "            if (i+200 > len(txt_files)):\n",
    "                end = len(txt_files)\n",
    "            else:\n",
    "                end = i+200\n",
    "            results = pool.map(processOne, txt_files[i:end])\n",
    "            temp += '\\n'.join(results)\n",
    "            temp += '\\n'\n",
    "            if (tick_counter != 0 and tick_counter % 10==0):\n",
    "                end_time = time.time()\n",
    "                sys.stdout = orig_stdout\n",
    "                print(\"Last file size {0} batch processing running time was {1}\".format(batch_size, end_time-start_time))\n",
    "                sys.stdout = out_f\n",
    "            tick_counter += 1\n",
    "            if (i != 0 and (i % 5000==0 or i==len(txt_files))):\n",
    "                output = 'aggregated-{0}-{1}.txt'.format(fol.replace('/', '-'), i)\n",
    "                with smart_open.smart_open(os.path.join(dirname, output), \"wb\") as f:\n",
    "                    for idx, line in enumerate(temp.split('\\n')):\n",
    "                        #num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "                        num_line = u\"{0}\\n\".format(line)\n",
    "                        f.write(num_line.encode(\"UTF-8\"))\n",
    "                temp = u''\n",
    "                print(\"{} aggregated\".format(os.path.join(dirname, output)))\n",
    "        \n",
    "start = time.time()\n",
    "prepData()\n",
    "end = time.time()\n",
    "print (\"Total running time: \".format(end-start))\n",
    "\n",
    "sys.stdout = orig_stdout\n",
    "out_f.close()\n",
    "\n",
    "print(\"Process completed\")\n",
    "print(\"Total running time: {0}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process started...\n",
      "28\n",
      "alldata-id.txt aggregated\n",
      "20\n",
      "train-all.txt aggregated\n",
      "8\n",
      "test-all.txt aggregated\n",
      "10\n",
      "train-pos.txt aggregated\n",
      "10\n",
      "train-neg.txt aggregated\n",
      "4\n",
      "test-pos.txt aggregated\n",
      "4\n",
      "test-neg.txt aggregated\n",
      "Total running time: 515.8943219184875\n",
      "Processed completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Process started...\")\n",
    "\n",
    "def aggregate_data(name, out):\n",
    "    txt_files = glob.glob(os.path.join(dirname, name))\n",
    "    open('alldata-id.txt', 'w').close() # Clear alldata-id.txt\n",
    "    print(len(txt_files))\n",
    "    with smart_open.smart_open(os.path.join(dirname, out), 'ab') as f:\n",
    "        for txt in txt_files:\n",
    "            for idx, line in enumerate(open(txt, 'rb')):\n",
    "                #num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "                num_line = u\"{0}\\n\".format(line)\n",
    "                f.write(num_line.encode(\"UTF-8\"))\n",
    "    print(\"{0} aggregated\".format(out))\n",
    "    \n",
    "start = time.time()\n",
    "aggregate_data('aggregated-*.txt', 'alldata-id.txt')\n",
    "aggregate_data('aggregated-train*.txt', 'train-all.txt')\n",
    "aggregate_data('aggregated-test*.txt', 'test-all.txt')\n",
    "aggregate_data('aggregated-train-pos*.txt', 'train-pos.txt')\n",
    "aggregate_data('aggregated-train-neg*.txt', 'train-neg.txt')\n",
    "aggregate_data('aggregated-test-pos*.txt', 'test-pos.txt')\n",
    "aggregate_data('aggregated-test-neg*.txt', 'test-neg.txt')\n",
    "end = time.time()\n",
    "print (\"Total running time: {0}\".format(end-start))\n",
    "\n",
    "print(\"Processed completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "# num_lines_alldata =  file_len(os.path.join(dirname, 'alldata-id.txt'))\n",
    "# num_lines_train =  file_len(os.path.join(dirname, 'alldata-id.txt'))\n",
    "# num_lines_test =  file_len(os.path.join(dirname, 'alldata-id.txt'))\n",
    "# num_lines_train_pos =  file_len(os.path.join(dirname, 'alldata-id.txt'))\n",
    "# num_lines_train_neg =  file_len(os.path.join(dirname, 'alldata-id.txt'))\n",
    "# num_lines_test_pos =  file_len(os.path.join(dirname, 'alldata-id.txt'))\n",
    "# num_lines_test_neg =  file_len(os.path.join(dirname, 'alldata-id.txt'))\n",
    "\n",
    "# print(\"Total number of paragraphs is {}\".format(num_lines_alldata))\n",
    "# print(\"Total number of training docs is {}\".format(num_lines_train))\n",
    "# print(\"Total number of test docs is {}\".format(num_lines_test))\n",
    "# print(\"Total number of pos training docs is {}\".format(num_lines_train_pos))\n",
    "# print(\"Total number of neg training docs is {}\".format(num_lines_train_neg))\n",
    "# print(\"Total number of pos test docs is {}\".format(num_lines_test_pos))\n",
    "# print(\"Total number of neg test docs is {}\".format(num_lines_train_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: [doc2vec-lee](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(fname):\n",
    "    for i, line in enumerate(open(fname, encoding='utf-8')):\n",
    "        # For training data, add tags\n",
    "        yield gensim.models.doc2vec.TaggedDocument(str(line), [i])\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "def read_labelled_corpus(fname, split, sentiment):\n",
    "    for i, line in enumerate(open(fname, encoding='utf-8')):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no]\n",
    "        split_enum = ['train', 'test', 'extra']\n",
    "        sentiment_enum = [1.0, 0.0, None]\n",
    "        assert split in split_enum\n",
    "        assert sentiment in sentiment_enum\n",
    "        yield SentimentDocument(words, tags, split, sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-up Doc2Vec Training & Evaluation Models\n",
    "References: [Le & Mikolov 2014](http://cs.stanford.edu/~quocle/paragraph_vector.pdf), [go.sh](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ)  \n",
    "Le and Mikolov notes that combining a paragraph vector from Distributed Bag of Words (DBOW) and Distributed Memory (DM) improves performance. We will follow, pairing the models together for evaluation. Here, we concatenate the paragraph vectors obtained from each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 cores\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a2406a28d2a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Speed up setup by sharing results of the 1st model's vocabulary scan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0msimple_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'alldata-id.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# PV-DM w/ concat requires one special NULL word so it serves as template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msimple_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[0;34m(self, sentences, keep_raw_vocab, trim_rule, progress_per, update)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \"\"\"\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_per\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# initial survey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# trim by min_count & precalculate downsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build tables & arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36mscan_vocab\u001b[0;34m(self, documents, progress_per, trim_rule, update)\u001b[0m\n\u001b[1;32m    695\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnote_doctag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_no\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m                 \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             \u001b[0mtotal_words\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "print(\"{} cores\".format(cores))\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DM w/ concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    # Every 10 million word types need about 1GB of RAM (For setting max_vocab_size)\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, max_vocab_size=100000, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, max_vocab_size=100000, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/ average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, max_vocab_size=100000, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]\n",
    "\n",
    "# Speed up setup by sharing results of the 1st model's vocabulary scan\n",
    "simple_models[0].build_vocab(read_corpus(os.path.join(dirname, 'alldata-id.txt')))  # PV-DM w/ concat requires one special NULL word so it serves as template\n",
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)\n",
    "\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[2]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])\n",
    "for model in models_by_name:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some helper methods for evaluating the performance of our Doc2vec using paragraph vectors. We will classify document sentiments using a logistic regression model based on our paragraph embeddings. We will compare the error rates based on word embeddings from our various Doc2vec models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "    \n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    logistic = linear_model.LogisticRegression(C=1e5)\n",
    "    logistic.fit(train_regressors, train_targets)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    \n",
    "    # Predict & evaluate\n",
    "    test_predictions = logistic.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Training\n",
    "We use an explicit multiple-pass, alpha-reduction approach as sketched in this [gensim doc2vec blog post](http://radimrehurek.com/2014/12/doc2vec-tutorial/) with added shuffling of corpus on each pass.\n",
    "\n",
    "Note that vector training is occurring on *all* documents of the dataset, which includes all TRAIN/TEST/DEV docs.\n",
    "\n",
    "We evaluate each model's sentiment predictive power based on error rate, and the evaluation is repeated after each pass so we can see the rates of relative improvement. The base numbers reuse the TRAIN and TEST vectors stored in the models for the logistic regression, while the _inferred_ results use newly-inferred TEST vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_error = defaultdict(lambda: 1.0)  # To selectively print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha, min_alpha, passes = (0.025, 0.001, 20)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "start = time.time()\n",
    "\n",
    "doc_list = read_corpus(os.path.join(dirname, 'alldata-id.txt'))\n",
    "train_docs = read_labelled_corpus(os.path.join(dirname, 'train-all.txt'))\n",
    "test_docs = read_labelled_corpus(os.path.join(dirname, 'test-all.txt'))\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # Shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # Train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list, total_examples=len(doc_list), epochs=1)\n",
    "            #train_model.train(alldocs, total_examples=len(alldocs), epochs=1)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # Evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*'\n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "        \n",
    "        if (epoch == passes-1):\n",
    "            train_model.save('name')\n",
    "            \n",
    "    print('Completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "\n",
    "train_docs = None\n",
    "test_docs = None \n",
    "doc_list = None # We're done using this\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))\n",
    "end = time.time()\n",
    "print(\"Time elapsed: {0}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achieved Sentiment-Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print best error rates achieved\n",
    "print(\"Err rate Model\")\n",
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are inferred vectors close to the precalculated ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # Pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do close documents seem more related than distant ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the word vectors show useful similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_models = simple_models[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(word_models[0].wv.index2word)\n",
    "    if word_models[0].wv.vocab[word].count > 10:\n",
    "        break\n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "#word = 'comedy/drama'\n",
    "similars_per_model = [str(model.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurrences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBOW words look meaningless because DBOW Model doesn't train word vectors - they remain at random initialized values (unless you use dbow_words=1, which slows training with little improvement). On the other hand, DM Models show meaningfully similar words when there are enough examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do documents have useful similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pick random doc\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)\n",
    "similars_per_model = [str([(' '.join(alldocs[a_sim[0]].words), a_sim[1]) for a_sim in model.docvecs.most_similar(doc_id, topn=1)]).replace('), ','),<br>\\n') for model in simple_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([\"Original doc\"] + ([str(model) for model in word_models])) + \n",
    "    \"</th></tr><tr><td style=\\\"vertical-align:top\\\">\" +\n",
    "    \"</td><td style=\\\"vertical-align:top\\\">\".join([' '.join(alldocs[doc_id].words)] + (similars_per_model)) +\n",
    "    \"</td></tr></table>\")\n",
    "#print(\"most similar words for {}\".format(' '.join(alldocs[doc_id].words)))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
