{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from multiprocessing import Pool\n",
    "import smart_open\n",
    "import os.path\n",
    "import time\n",
    "import glob\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from gensim.models import Doc2Vec\n",
    "from IPython.display import HTML\n",
    "from timeit import default_timer\n",
    "import gensim.models.doc2vec\n",
    "import multiprocessing\n",
    "from os import remove\n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "import locale\n",
    "import gensim\n",
    "import sys\n",
    "import re\n",
    "from sklearn import linear_model\n",
    "import fileinput\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "from sklearn.utils import resample\n",
    "\n",
    "dirname = 'data'\n",
    "\n",
    "models = [Doc2Vec.load('saved_doc2vec_models/Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)'), Doc2Vec.load('saved_doc2vec_models/Doc2Vec(dmc,d100,n5,w5,mc2,s0.001,t8)'), Doc2Vec.load('saved_doc2vec_models/Doc2Vec(dmm,d100,n5,w10,mc2,s0.001,t8)')]\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "\n",
    "classifiers = [linear_model.LogisticRegression(C=1e5), ensemble.RandomForestClassifier(), svm.SVC()]\n",
    "\n",
    "inferreds = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "num_lines_test = file_len(os.path.join(dirname, 'test-pos.txt'))\n",
    "num_lines_test += file_len(os.path.join(dirname, 'test-neg.txt'))\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, classifier, inferred):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "    \n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_targets, train_regressors = shuffle(train_targets, train_regressors)\n",
    "    classifier.fit(train_regressors, train_targets)\n",
    "    \n",
    "    if inferred:\n",
    "        infer_subsample = 0.1\n",
    "        infer_steps = 3\n",
    "        infer_alpha = 0.1\n",
    "        test_targets, test_regressors = zip(*[(doc.sentiment, test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha)) for doc in test_set])\n",
    "        if infer_subsample < 1.0:\n",
    "            test_targets, test_regressors = resample(test_targets, test_regressors, n_samples = int(infer_subsample * num_lines_test))\n",
    "    else:\n",
    "        test_targets, test_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in test_set])    \n",
    "        test_targets, test_regressors = shuffle(test_targets, test_regressors)    \n",
    "    \n",
    "    # Predict & evaluate\n",
    "    test_predictions = classifier.predict(test_regressors)\n",
    "    len_predictions = len(test_predictions)\n",
    "    corrects = sum(np.rint(test_predictions) == test_targets)\n",
    "    errors = len_predictions - corrects\n",
    "    error_rate = float(errors) / len_predictions\n",
    "    return (error_rate, errors, len_predictions, classifier, inferred)\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "def read_labeled_corpus(fpos, fneg, split):\n",
    "    f_list = [fpos, fneg]\n",
    "    i = 0\n",
    "    for f in f_list:\n",
    "        if i == 0:\n",
    "            sentiment = 1.0\n",
    "        else:\n",
    "            sentiment = 0.0\n",
    "        for line in open(f, encoding='utf-8'):\n",
    "            tokens = gensim.utils.to_unicode(line).split()\n",
    "            if(len(tokens)==0):\n",
    "                continue\n",
    "            words = tokens[1:]\n",
    "            tags = [tokens[0]]\n",
    "            yield SentimentDocument(gensim.utils.to_unicode(line).split()[1:], tags, split, sentiment)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation takes too long to run in this notebook, run the script instead.\n",
    "Configuration -  \n",
    "Doc2Vec models were trained on 8/18/17\n",
    "* PV-DM w/ concatenation - window=5 (both sides) approximates paper's 10-word total window size  \n",
    "Doc2Vec(dm=1, dm_concat=1, size=100, window=5, max_vocab_size=100000, negative=5, hs=0, min_count=2, workers=cores)\n",
    "* PV-DBOW  \n",
    "Doc2Vec(dm=0, size=100, max_vocab_size=100000, negative=5, hs=0, min_count=2, workers=cores)\n",
    "* PV-DM w/ average  \n",
    "Doc2Vec(dm=1, dm_mean=1, size=100, window=10, max_vocab_size=100000, negative=5, hs=0, min_count=2, workers=cores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "for model in models:\n",
    "    for classifier in classifiers:\n",
    "        for inferred in inferreds:\n",
    "            train_docs = read_labeled_corpus(os.path.join(dirname, 'train-pos.txt'), os.path.join(dirname, 'train-neg.txt'), 'train')\n",
    "            test_docs = read_labeled_corpus(os.path.join(dirname, 'test-pos.txt'), os.path.join(dirname, 'test-neg.txt'), 'test')\n",
    "            err, err_count, test_count, predictor, infer = error_rate_for_model(model, train_docs, test_docs, classifier, inferred)\n",
    "            results.append((err, err_count, test_count, predictor, infer))\n",
    "            print(\"Error: {0}; Error Count: {1}; Test Count {2}; Predictor: {3}; Inferred: {4}\".format(err, err_count, test_count, predictor, infer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "`\n",
    "Error: 0.49597423510466987; Error Count: 22484; Test Count 45333; Predictor: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "          max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "          tol=0.001, verbose=False); Inferred: False\n",
    "          Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
    "Error: 0.4983454665784249; Error Count: 4518; Test Count 9066; Predictor: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "          max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "          tol=0.001, verbose=False); Inferred: True\n",
    "          Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n",
    "Error: 0.4998786755784969; Error Count: 22661; Test Count 45333; Predictor: LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False); Inferred: False\n",
    "          Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
    "Error: 0.5011030222810501; Error Count: 4543; Test Count 9066; Predictor: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "          max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "          min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "          n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
    "          verbose=0, warm_start=False); Inferred: True\n",
    "          Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n",
    "Error: 0.5048419473672601; Error Count: 22886; Test Count 45333; Predictor: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "          max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "          min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "          n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
    "          verbose=0, warm_start=False); Inferred: False\n",
    "          Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n",
    "Error: 0.5054048091771454; Error Count: 4582; Test Count 9066; Predictor: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "          max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "          min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "          n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
    "          verbose=0, warm_start=False); Inferred: True\n",
    "          Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
    "Error: 0.515221707478491; Error Count: 4671; Test Count 9066; Predictor: LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False); Inferred: True\n",
    "          Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
    "Error: 0.5119126406353408; Error Count: 4641; Test Count 9066; Predictor: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "          max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "          tol=0.001, verbose=False); Inferred: True\n",
    "          Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
    "Error: 0.5103684094418707; Error Count: 4627; Test Count 9066; Predictor: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "          max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "          min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "          n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
    "          verbose=0, warm_start=False); Inferred: True\n",
    "          Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
    "Error: 0.5118023384072359; Error Count: 4640; Test Count 9066; Predictor: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "          max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "          tol=0.001, verbose=False); Inferred: True\n",
    "          Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
    "Error: 0.5298568371826263; Error Count: 24020; Test Count 45333; Predictor: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "          max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "          min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "          n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
    "          verbose=0, warm_start=False); Inferred: False\n",
    "          Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
    "Error: 0.514228987425546; Error Count: 4662; Test Count 9066; Predictor: LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False); Inferred: True\n",
    "          Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n",
    "Error: 0.5179891028610505; Error Count: 23482; Test Count 45333; Predictor: LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False); Inferred: False\n",
    "          Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n",
    "Error: 0.5181998676373263; Error Count: 4698; Test Count 9066; Predictor: LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False); Inferred: True\n",
    "          Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t8)\n",
    "Error: 0.5228200207354465; Error Count: 23701; Test Count 45333; Predictor: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "          max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "          tol=0.001, verbose=False); Inferred: False\n",
    "          Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t8)\n",
    "Error: 0.5584011647144465; Error Count: 25314; Test Count 45333; Predictor: LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
    "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
    "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
    "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False); Inferred: False\n",
    "          Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
    "Error: 0.5585114596430856; Error Count: 25319; Test Count 45333; Predictor: RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "          max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
    "          min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "          min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "          n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
    "          verbose=0, warm_start=False); Inferred: False\n",
    "          Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
    "Error: 0.5634967904175766; Error Count: 25545; Test Count 45333; Predictor: SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "          decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
    "          max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "          tol=0.001, verbose=False); Inferred: False\n",
    "          Doc2Vec(dbow,d100,n5,mc2,s0.001,t8)\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.49597423510466987 Error Rate  \n",
    "22484 Correct Predictions out of 45333  \n",
    "(22484-22666.5)/(sqrt(45333*.5*.5)) = -182.5/106.457738093574015 = -1.714295 std dev from the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
