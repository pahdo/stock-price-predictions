{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP on SEC Forms Using Doc2vec with Gensim\n",
    "## Introduction\n",
    "Throughout this notebook, we reference <a href=\"https://arxiv.org/pdf/1405.4053.pdf\">Le and Mikolov 2014</a>. \n",
    "\n",
    "### Bag-of-words Model\n",
    "Traditional state-of-the-art document representations are based on the <a href=\"https://en.wikipedia.org/wiki/Bag-of-words_model\">bag-of-words model</a>, which represent input documents as a fixed-length vector. For example, borrowing from the Wikipedia article, the two documents  \n",
    "(1) `John likes to watch movies. Mary likes movies too.`  \n",
    "(2) `John also likes to watch football games.`  \n",
    "are used to construct a length 10 list of words  \n",
    "`[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\"]`  \n",
    "so then we can represent the two documents as fixed length vectors whose elements are the frequencies of the corresponding words in our list  \n",
    "(1) `[1, 2, 1, 1, 2, 1, 1, 0, 0, 0]`  \n",
    "(2) `[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]`  \n",
    "Bag-of-words models are surprisingly effective but lose information about word order. Bag of <a href=\"https://en.wikipedia.org/wiki/N-gram\">n-grams</a> models consider word phrases of length n to capture local word order but suffer from data sparsity and high dimensionality.\n",
    "\n",
    "### Word2vec Model\n",
    "Word2vec uses a shallow neural network to embed words in a high-dimensional vector space. In the resulting vector space, close word vectors have similar contextual meanings and distant word vectors have different contextual meanings. For example, `strong` and `powerful` would be closer together than `strong` and `Paris`. Word2vec models can be trained using two prediction tasks which represent the skip-gram and continuous-bag-of-words models.\n",
    "\n",
    "\n",
    "#### Word2vec - Skip-gram Model\n",
    "The Skip-gram <a href=\"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\">Word2vec</a> Model takes in pairs (word1, word2) generated by moving a window across text data and trains a 1-hidden-layer neural network based on the fake task giving us a predicted probability distribution of nearby words to a given input word. The hidden-to-output weights in the neural network become the word embeddings. So if the hidden layer has 300 neurons, this network woulld give us 300-dimensional word embeddings. We use <a href=\"https://en.wikipedia.org/wiki/One-hot\">one-hot</a> encoding for the words.\n",
    "\n",
    "#### Word2vec - Continuous-bag-of-words Model\n",
    "The Continuous-bag-of-words Word2vec Model is also a 1-hidden-layer neural network. This time, the fake task is to predict the center word based on context words in a window around the center word. Again, the hidden-to-output weights become the word embeddings and we use one-hot encoding.\n",
    "\n",
    "### Paragraph Vector\n",
    "Le and Mikolov 2014 introduces the <i>Paragraph Vector</i>, which outperforms representing a documents by averaging or concatenating the word vectors of a document. We determine the embedding of the paragraph in its vector space in the same way as words - by training a shallow neural network on a fake task. Paragraph Vectors consider local word order but also give us a dense vector representations.\n",
    "\n",
    "#### Paragraph Vector - Distributed Memory (PV-DM)\n",
    "This is the Paragraph Vector Model analogous to the Continuous-bag-of-words Word2vec Model. Paragraph Vectors are obtained by training a neural network on the fake task of inferring a center word based on context words and a context paragraph. A paragraph is a context for all words in the paragraph.\n",
    "\n",
    "#### Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "This is the Paragraph Vector Model analogous to the Skip-gram Word2vec Model. Paragraph Vectors are obtained by training a neural network on the fake task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph.\n",
    "\n",
    "### Requirements\n",
    "The following python modules are dependencies for this notebook:\n",
    "* statsmodels\n",
    "* spacy  \n",
    "* smart_open\n",
    "* pandas\n",
    "* testfixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import smart_open\n",
    "import os.path\n",
    "import spacy\n",
    "import time\n",
    "import glob\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "dirname = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "from contextlib import contextmanager\n",
    "from collections import defaultdict\n",
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from gensim.models import Doc2Vec\n",
    "from IPython.display import HTML\n",
    "from timeit import default_timer\n",
    "import gensim.models.doc2vec\n",
    "import statsmodels.api as sm\n",
    "from random import shuffle\n",
    "from random import sample\n",
    "import multiprocessing\n",
    "from os import remove\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import datetime\n",
    "import locale\n",
    "import gensim\n",
    "import random\n",
    "import sys\n",
    "import re\n",
    "#import sklearn\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PrepData - OOV Removal, No Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n",
      "data/train-pos.txt aggregated\n",
      "data/train-neg.txt aggregated\n",
      "data/test-pos.txt aggregated\n",
      "data/test-neg.txt aggregated\n",
      "alldata-id.txt aggregated\n",
      "Total running time:  178.4518551826477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/queues.py\", line 342, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
      "    task = get()\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/queues.py\", line 341, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/Users/daniel/miniconda3/envs/nlp/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "def processOne(txt):\n",
    "    with smart_open.smart_open(txt, \"rb\") as t:\n",
    "        doc = nlp.make_doc(t.read().decode(\"utf-8\"))\n",
    "        # Approximately top 500 words in a SEC Form are header\n",
    "        removed_stop_words = list(map(lambda x: x.lower_, filter(lambda token: token.is_alpha and not token.is_stop and not token.is_oov, doc)))[500:]\n",
    "        return \" \".join(removed_stop_words)\n",
    "def prepData():\n",
    "    folders = ['train/pos', 'train/neg', 'test/pos', 'test/neg'] \n",
    "    print(\"Preparing dataset...\")\n",
    "    alldata = u''\n",
    "    pool = Pool()\n",
    "    for fol in folders:\n",
    "        temp = u''\n",
    "        txt_files = glob.glob(os.path.join(dirname, fol, '*.txt'))[:2000]\n",
    "        results = pool.map(processOne, txt_files)\n",
    "        temp += '\\n'.join(results)\n",
    "        temp += '\\n'\n",
    "        output = fol.replace('/', '-') + '.txt'\n",
    "        print(\"{} aggregated\".format(os.path.join(dirname, output)))\n",
    "        with smart_open.smart_open(os.path.join(dirname, output), \"wb\") as f:\n",
    "            for idx, line in enumerate(temp.split('\\n')):\n",
    "                #num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "                num_line = u\"{0}\\n\".format(line)\n",
    "                f.write(num_line.encode(\"UTF-8\"))\n",
    "        alldata += temp\n",
    "    with smart_open.smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(alldata.split('\\n')):\n",
    "            #num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "            num_line = u\"{0}\\n\".format(line)\n",
    "            f.write(num_line.encode(\"UTF-8\")) \n",
    "    print(\"alldata-id.txt aggregated\")\n",
    "    return alldata\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "prepData()\n",
    "end = time.time()\n",
    "print (\"Total running time: \", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity Check - Is the text corpus appropriately split between training/test and pos/neg?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2301\n",
      "2304\n",
      "Total number of paragraphs is 2301\n"
     ]
    }
   ],
   "source": [
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "num_lines_train_pos = file_len(os.path.join(dirname, 'train-pos.txt'))\n",
    "num_lines_train_neg = file_len(os.path.join(dirname, 'train-neg.txt'))\n",
    "num_lines_test_pos = file_len(os.path.join(dirname, 'test-pos.txt'))\n",
    "num_lines_test_neg = file_len(os.path.join(dirname, 'test-neg.txt'))\n",
    "num_lines_alldata =  file_len(os.path.join(dirname, 'alldata-id.txt'))\n",
    "\n",
    "print(num_lines_alldata)\n",
    "print(num_lines_train_pos + num_lines_train_neg + num_lines_test_pos + num_lines_test_neg)\n",
    "\n",
    "print(\"Total number of paragraphs is {}\".format(num_lines_alldata))\n",
    "\n",
    "train_pos_idx = 0\n",
    "train_neg_idx = train_pos_idx + num_lines_train_pos\n",
    "test_pos_idx = train_neg_idx + num_lines_train_neg\n",
    "test_neg_idx = test_pos_idx + num_lines_test_pos\n",
    "unsup_idx = test_neg_idx + num_lines_test_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag documents with their split and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "def getAllDocs():\n",
    "    alldocs = []  # Will hold all docs in original order\n",
    "    with open(os.path.join(dirname, 'alldata-id.txt'), encoding='utf-8') as alldata:\n",
    "        for line_no, line in enumerate(alldata):\n",
    "            tokens = gensim.utils.to_unicode(line).split()\n",
    "            words = tokens[1:]\n",
    "            tags = [line_no] # 'tags = [tokens[0]]' would also work at extra memory cost\n",
    "            split_enum = ['train', 'test', 'extra']\n",
    "            sentiment_enum = [1.0, 0.0, None]\n",
    "            if (train_pos_idx <= line_no and line_no < train_neg_idx):\n",
    "                split = split_enum[0]\n",
    "                sentiment = sentiment_enum[0]\n",
    "            elif (train_neg_idx <= line_no and line_no < test_pos_idx):\n",
    "                split = split_enum[0]\n",
    "                sentiment = sentiment_enum[1]\n",
    "            elif (test_pos_idx <= line_no and line_no < test_neg_idx):\n",
    "                split = split_enum[1]\n",
    "                sentiment = sentiment_enum[0]\n",
    "            elif (test_neg_idx <= line_no and line_no < unsup_idx):\n",
    "                split = split_enum[1]\n",
    "                sentiment = sentiment_enum[1]\n",
    "            else:\n",
    "                split = split_enum[2]\n",
    "                sentiment = sentiment_enum[2]\n",
    "            alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "    return alldocs\n",
    "\n",
    "alldocs = getAllDocs()\n",
    "alldata = None # Clear this out of memory because we're done using it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check - Are our SentimentDocument tuples in working order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['company', 'o', 'indicate', 'check', 'mark', 'registrant', 'shell', 'company', 'defined', 'rule', 'act', 'digital', 'realty', 'trust', 'yes', 'o', 'x', 'digital', 'realty']\n",
      "Split:train\n",
      "Sentiment: 1.0\n",
      "Words: ['ended', 'january', 'contains', 'forward', 'looking', 'statements', 'meaning', 'private', 'securities', 'litigation', 'reform', 'act', 'including', 'statements', 'future', 'results', 'operations', 'financial', 'position']\n",
      "Split:train\n",
      "Sentiment: 0.0\n",
      "Words: ['considered', 'hydrocarbons', 'liquids', 'oil', 'thousand', 'barrels', 'crude', 'oil', 'liquid', 'hydrocarbons', 'thousand', 'barrels', 'crude', 'oil', 'equivalent', 'ratio', 'mcf', 'natural', 'gas']\n",
      "Split:test\n",
      "Sentiment: 1.0\n",
      "Words: ['surgeons', 'intuitive', 'control', 'range', 'motion', 'fine', 'tissue', 'manipulation', 'capability', 'dimensional', 'high', 'definition', 'hd', 'vision', 'simultaneously', 'allowing', 'surgeons', 'work', 'small']\n",
      "Split:test\n",
      "Sentiment: 0.0\n"
     ]
    }
   ],
   "source": [
    "docs = [alldocs[train_pos_idx], alldocs[train_neg_idx], alldocs[test_pos_idx], alldocs[test_neg_idx]]\n",
    "for doc in docs:\n",
    "    print(\"Words: {0}\\nSplit:{1}\\nSentiment: {2}\".format(doc[0][:19], doc[2], doc[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: [doc2vec-lee](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-lee.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(fname):\n",
    "    with smart_open.smart_open(fname, encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(str(line), [i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set-up Doc2Vec Training & Evaluation Models\n",
    "References: [Le & Mikolov 2014](http://cs.stanford.edu/~quocle/paragraph_vector.pdf), [go.sh](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ)  \n",
    "Le and Mikolov notes that combining a paragraph vector from Distributed Bag of Words (DBOW) and Distributed Memory (DM) improves performance. We will follow, pairing the models together for evaluation. Here, we concatenate the paragraph vectors obtained from each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 cores\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)\n",
      "dbow+dmm\n",
      "dbow+dmc\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "print(\"{} cores\".format(cores))\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DM w/ concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    # Every 10 million word types need about 1GB of RAM (For setting max_vocab_size)\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, max_vocab_size=100000, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, max_vocab_size=100000, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/ average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, max_vocab_size=100000, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]\n",
    "\n",
    "# Speed up setup by sharing results of the 1st model's vocabulary scan\n",
    "simple_models[0].build_vocab(read_corpus(os.path.join(dirname, 'alldata-id.txt')))  # PV-DM w/ concat requires one special NULL word so it serves as template\n",
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)\n",
    "\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[2]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])\n",
    "for model in models_by_name:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some helper methods for evaluating the performance of our Doc2vec using paragraph vectors. We will classify document sentiments using a logistic regression model based on our paragraph embeddings. We will compare the error rates based on word embeddings from our various Doc2vec models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    # print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "    \n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # Predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "    \n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    logistic = linear_model.LogisticRegression(C=1e5)\n",
    "    logistic.fit(train_regressors, train_targets)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    \n",
    "    # Predict & evaluate\n",
    "    test_predictions = logistic.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Training\n",
    "We use an explicit multiple-pass, alpha-reduction approach as sketched in this [gensim doc2vec blog post](http://radimrehurek.com/2014/12/doc2vec-tutorial/) with added shuffling of corpus on each pass.\n",
    "\n",
    "Note that vector training is occurring on *all* documents of the dataset, which includes all TRAIN/TEST/DEV docs.\n",
    "\n",
    "We evaluate each model's sentiment predictive power based on error rate, and the evaluation is repeated after each pass so we can see the rates of relative improvement. The base numbers reuse the TRAIN and TEST vectors stored in the models for the logistic regression, while the _inferred_ results use newly-inferred TEST vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_error = defaultdict(lambda: 1.0)  # To selectively print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-df57716d0156>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0malpha_delta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_alpha\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mpasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"START %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "alpha, min_alpha, passes = (0.025, 0.001, 5)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "start = time.time()\n",
    "\n",
    "doc_list = alldocs # Make a copy of alldocs for shuffling\n",
    "train_docs = alldocs[train_pos_idx:test_pos_idx]\n",
    "test_docs = alldocs[test_pos_idx:unsup_idx]\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # Shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # Train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list, total_examples=len(doc_list), epochs=1)\n",
    "            #train_model.train(alldocs, total_examples=len(alldocs), epochs=1)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # Evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*'\n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "        \n",
    "        if (epoch == passes-1):\n",
    "            train_model.save('name')\n",
    "            \n",
    "    print('Completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "\n",
    "train_docs = None\n",
    "test_docs = None \n",
    "doc_list = None # We're done using this\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))\n",
    "end = time.time()\n",
    "print(\"Time elapsed: {0}\".format(end-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achieved Sentiment-Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Err rate Model\n",
      "0.344828 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)_inferred\n",
      "0.379310 dbow+dmc_inferred\n",
      "0.413793 Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)_inferred\n",
      "0.413793 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)_inferred\n",
      "0.471572 dbow+dmc\n",
      "0.471572 dbow+dmm\n",
      "0.488294 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)\n",
      "0.505017 Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)\n",
      "0.508361 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)\n",
      "0.517241 dbow+dmm_inferred\n"
     ]
    }
   ],
   "source": [
    "# Print best error rates achieved\n",
    "print(\"Err rate Model\")\n",
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are inferred vectors close to the precalculated ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # Pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do close documents seem more related than distant ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): «%s»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the word vectors show useful similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_models = simple_models[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(word_models[0].wv.index2word)\n",
    "    if word_models[0].wv.vocab[word].count > 10:\n",
    "        break\n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "#word = 'comedy/drama'\n",
    "similars_per_model = [str(model.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurrences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBOW words look meaningless because DBOW Model doesn't train word vectors - they remain at random initialized values (unless you use dbow_words=1, which slows training with little improvement). On the other hand, DM Models show meaningfully similar words when there are enough examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do documents have useful similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pick random doc\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)\n",
    "similars_per_model = [str([(' '.join(alldocs[a_sim[0]].words), a_sim[1]) for a_sim in model.docvecs.most_similar(doc_id, topn=1)]).replace('), ','),<br>\\n') for model in simple_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([\"Original doc\"] + ([str(model) for model in word_models])) + \n",
    "    \"</th></tr><tr><td style=\\\"vertical-align:top\\\">\" +\n",
    "    \"</td><td style=\\\"vertical-align:top\\\">\".join([' '.join(alldocs[doc_id].words)] + (similars_per_model)) +\n",
    "    \"</td></tr></table>\")\n",
    "#print(\"most similar words for {}\".format(' '.join(alldocs[doc_id].words)))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
