{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Doc2vec on Form 10-Ks and Form 10-Qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Throughout this notebook, we reference <a href=\"https://arxiv.org/pdf/1405.4053.pdf\">Le and Mikolov 2014</a>. \n",
    "\n",
    "### Bag-of-words Model\n",
    "Previous state-of-the-art document representations were based on the <a href=\"https://en.wikipedia.org/wiki/Bag-of-words_model\">bag-of-words model</a>, which represent input documents as a fixed-length vector. For example, borrowing from the Wikipedia article, the two documents  \n",
    "(1) `John likes to watch movies. Mary likes movies too.`  \n",
    "(2) `John also likes to watch football games.`  \n",
    "are used to construct a length 10 list of words  \n",
    "`[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\"]`  \n",
    "so then we can represent the two documents as fixed length vectors whose elements are the frequencies of the corresponding words in our list  \n",
    "(1) `[1, 2, 1, 1, 2, 1, 1, 0, 0, 0]`  \n",
    "(2) `[1, 1, 1, 1, 0, 0, 0, 1, 1, 1]`  \n",
    "Bag-of-words models are surprisingly effective but still lose information about word order. Bag of <a href=\"https://en.wikipedia.org/wiki/N-gram\">n-grams</a> models consider word phrases of length n to represent documents as fixed-length vectors to capture local word order but suffer from data sparsity and high dimensionality.\n",
    "\n",
    "### Word2vec Model\n",
    "Word2vec is a more recent model that embeds words in a high-dimensional vector space using a shallow neural network. The result is a set of word vectors where vectors close together in vector space have similar meanings based on context, and word vectors distant to each other have differing meanings. For example, `strong` and `powerful` would be close together and `strong` and `Paris` would be relatively far. There are two versions of this model based on skip-grams and continuous bag of words.\n",
    "\n",
    "\n",
    "#### Word2vec - Skip-gram Model\n",
    "The skip-gram <a href=\"http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/\">word2vec</a> model, for example, takes in pairs (word1, word2) generated by moving a window across text data, and trains a 1-hidden-layer neural network based on the fake task of given an input word, giving us a predicted probability distribution of nearby words to the input. The hidden-to-output weights in the neural network give us the word embeddings. So if the hidden layer has 300 neurons, this network will give us 300-dimensional word embeddings. We use <a href=\"https://en.wikipedia.org/wiki/One-hot\">one-hot</a> encoding for the words.\n",
    "\n",
    "#### Word2vec - Continuous-bag-of-words Model\n",
    "Continuous-bag-of-words Word2vec is very similar to the skip-gram model. It is also a 1-hidden-layer neural network. The fake task is based on the input context words in a window around a center word, predict the center word. Again, the hidden-to-output weights give us the word embeddings and we use one-hot encoding.\n",
    "\n",
    "### Paragraph Vector\n",
    "Le and Mikolov 2014 introduces the <i>Paragraph Vector</i>, which outperforms more naÃ¯ve representations of documents such as averaging the Word2vec word vectors of a document. The idea is straightforward: we act as if a paragraph (or document) is just another vector like a word vector, but we will call it a paragraph vector. We determine the embedding of the paragraph in vector space in the same way as words. Our paragraph vector model considers local word order like bag of n-grams, but gives us a denser representation in vector space compared to a sparse, high-dimensional representation.\n",
    "\n",
    "#### Paragraph Vector - Distributed Memory (PV-DM)\n",
    "This is the Paragraph Vector model analogous to Continuous-bag-of-words Word2vec. The paragraph vectors are obtained by training a neural network on the fake task of inferring a center word based on context words and a context paragraph. A paragraph is a context for all words in the paragraph, and a word in a paragraph can have that paragraph as a context. \n",
    "\n",
    "#### Paragraph Vector - Distributed Bag of Words (PV-DBOW)\n",
    "This is the Paragraph Vector model analogous to Skip-gram Word2vec. The paragraph vectors are obtained by training a neural network on the fake task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph.\n",
    "\n",
    "### Requirements\n",
    "The following python modules are dependencies for this notebook:\n",
    "* testfixtures ( `pip install testfixtures` )\n",
    "* statsmodels ( `pip install statsmodels` )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning up dataset...\n",
      "Total running time:  0.29464500000000005\n"
     ]
    }
   ],
   "source": [
    "import locale\n",
    "import glob\n",
    "import os.path\n",
    "import requests\n",
    "import tarfile\n",
    "import sys\n",
    "import codecs\n",
    "import smart_open\n",
    "\n",
    "dirname = 'data'\n",
    "locale.setlocale(locale.LC_ALL, 'C')\n",
    "\n",
    "if sys.version > '3':\n",
    "    control_chars = [chr(0x85)]\n",
    "else:\n",
    "    control_chars = [unichr(0x85)]\n",
    "\n",
    "# Convert text to lower-case and strip punctuation/symbols from words\n",
    "def normalize_text(text):\n",
    "    norm_text = text.lower()\n",
    "    # Replace breaks with spaces\n",
    "    norm_text = norm_text.replace('<br />', ' ')\n",
    "    # Pad punctuation with spaces on both sides\n",
    "    for char in ['.', '\"', ',', '(', ')', '!', '?', ';', ':', '{', '}', '<', '>']:\n",
    "        norm_text = norm_text.replace(char, ' ' + char + ' ')\n",
    "    return norm_text\n",
    "\n",
    "import time\n",
    "start = time.clock()\n",
    "\n",
    "if not os.path.isfile(os.path.join(dirname, 'alldata-id.txt')):\n",
    "    # Concatenate and normalize test/train data\n",
    "    print(\"Cleaning up dataset...\")\n",
    "    alldata = u''\n",
    "    temp = u''\n",
    "    txt_files = glob.glob(os.path.join(dirname, '*.txt'))\n",
    "    cap = 50\n",
    "    i = 0\n",
    "    for txt in txt_files:\n",
    "        if (i > cap):\n",
    "            break\n",
    "        i += 1\n",
    "        with smart_open.smart_open(txt, \"rb\") as t:\n",
    "            t_clean = t.read().decode(\"utf-8\")\n",
    "            for c in control_chars:\n",
    "                t_clean = t_clean.replace(c, ' ')\n",
    "            temp += t_clean\n",
    "        temp += \"\\n\"\n",
    "    temp_norm = normalize_text(temp)\n",
    "    alldata += temp_norm\n",
    "\n",
    "    with smart_open.smart_open(os.path.join(dirname, 'alldata-id.txt'), 'wb') as f:\n",
    "        for idx, line in enumerate(alldata.splitlines()):\n",
    "            num_line = u\"_*{0} {1}\\n\".format(idx, line)\n",
    "            f.write(num_line.encode(\"utf-8\"))\n",
    "\n",
    "end = time.clock()\n",
    "print (\"Total running time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "assert os.path.isfile(os.path.join(dirname, \"alldata-id.txt\")), \"alldata-id.txt unavailable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentDocument(words=['derivative', 'instrument', 'and', 'the', 'host', 'contract', 'is', 'not', 're-measured', 'at', 'fair', 'value', 'under', 'other', 'gaap', 'with', 'changes', 'in', 'fair', 'value', 'reported'], tags=[34821], split='test', sentiment=1.0)\n",
      "79262 docs: 25000 train-sentiment, 25000 test-sentiment\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from collections import namedtuple\n",
    "\n",
    "SentimentDocument = namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "alldocs = []  # Will hold all docs in original order\n",
    "with open(os.path.join(dirname, 'alldata-id.txt'), encoding='utf-8') as alldata:\n",
    "    for line_no, line in enumerate(alldata):\n",
    "        tokens = gensim.utils.to_unicode(line).split()\n",
    "        words = tokens[1:]\n",
    "        tags = [line_no] # 'tags = [tokens[0]]' would also work at extra memory cost\n",
    "        split = ['train', 'test', 'extra', 'extra'][line_no//25000]  # 25k train, 25k test, 25k extra\n",
    "        sentiment = [1.0, 0.0, 1.0, 0.0, None, None, None, None][line_no//12500] # [12.5K pos, 12.5K neg]*2 then unknown\n",
    "        alldocs.append(SentimentDocument(words, tags, split, sentiment))\n",
    "\n",
    "train_docs = [doc for doc in alldocs if doc.split == 'train']\n",
    "test_docs = [doc for doc in alldocs if doc.split == 'test']\n",
    "doc_list = alldocs[:]  # For reshuffling per pass\n",
    "\n",
    "print(alldocs[34821])\n",
    "print('%d docs: %d train-sentiment, %d test-sentiment' % (len(doc_list), len(train_docs), len(test_docs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up Doc2Vec Training & Evaluation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reference [Le & Mikolov 2014](http://cs.stanford.edu/~quocle/paragraph_vector.pdf) and [go.sh](https://groups.google.com/d/msg/word2vec-toolkit/Q49FIrNOQRo/J6KG8mUj45sJ):\n",
    "\n",
    "We vary the following parameter choices:\n",
    "* 100-dimensional vectors, as the 400-d vectors of the paper don't seem to offer much benefit on this task\n",
    "* Similarly, frequent word subsampling seems to decrease sentiment-prediction accuracy, so it's left out\n",
    "* `cbow=0` means skip-gram which is equivalent to the paper's 'PV-DBOW' mode, matched in gensim with `dm=0`\n",
    "* Added to that DBOW model are two DM models, one which averages context vectors (`dm_mean`) and one which concatenates them (`dm_concat`, resulting in a much larger, slower, more data-hungry model)\n",
    "* A `min_count=2` saves quite a bit of model memory, discarding only words that appear in a single doc (and are thus no more expressive than the unique-to-each doc vectors themselves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import gensim.models.doc2vec\n",
    "from collections import OrderedDict\n",
    "import multiprocessing\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be painfully slow otherwise\"\n",
    "\n",
    "simple_models = [\n",
    "    # PV-DM w/ concatenation - window=5 (both sides) approximates paper's 10-word total window size\n",
    "    Doc2Vec(dm=1, dm_concat=1, size=100, window=5, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DBOW \n",
    "    Doc2Vec(dm=0, size=100, negative=5, hs=0, min_count=2, workers=cores),\n",
    "    # PV-DM w/ average\n",
    "    Doc2Vec(dm=1, dm_mean=1, size=100, window=10, negative=5, hs=0, min_count=2, workers=cores),\n",
    "]\n",
    "\n",
    "# Speed up setup by sharing results of the 1st model's vocabulary scan\n",
    "simple_models[0].build_vocab(alldocs)  # PV-DM w/ concat requires one special NULL word so it serves as template\n",
    "print(simple_models[0])\n",
    "for model in simple_models[1:]:\n",
    "    model.reset_from(simple_models[0])\n",
    "    print(model)\n",
    "\n",
    "models_by_name = OrderedDict((str(model), model) for model in simple_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le and Mikolov notes that combining a paragraph vector from Distributed Bag of Words (DBOW) and Distributed Memory (DM) improves performance. We will follow, pairing the models together for evaluation. Here, we concatenate the paragraph vectors obtained from each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "models_by_name['dbow+dmm'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[2]])\n",
    "models_by_name['dbow+dmc'] = ConcatenatedDoc2Vec([simple_models[1], simple_models[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Evaluation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some helper methods for evaluating the performance of our Doc2vec using paragraph vectors. We will classify document sentiments using a logistic regression model based on our paragraph embeddings. We will compare the error rates based on word embeddings from our various Doc2vec models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/miniconda3/envs/gensim/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import sample\n",
    "\n",
    "# For timing\n",
    "from contextlib import contextmanager\n",
    "from timeit import default_timer\n",
    "import time \n",
    "\n",
    "@contextmanager\n",
    "def elapsed_timer():\n",
    "    start = default_timer()\n",
    "    elapser = lambda: default_timer() - start\n",
    "    yield lambda: elapser()\n",
    "    end = default_timer()\n",
    "    elapser = lambda: end-start\n",
    "    \n",
    "def logistic_predictor_from_data(train_targets, train_regressors):\n",
    "    logit = sm.Logit(train_targets, train_regressors)\n",
    "    predictor = logit.fit(disp=0)\n",
    "    # print(predictor.summary())\n",
    "    return predictor\n",
    "\n",
    "def error_rate_for_model(test_model, train_set, test_set, infer=False, infer_steps=3, infer_alpha=0.1, infer_subsample=0.1):\n",
    "    \"\"\"Report error rate on test_doc sentiments, using supplied model and train_docs\"\"\"\n",
    "\n",
    "    train_targets, train_regressors = zip(*[(doc.sentiment, test_model.docvecs[doc.tags[0]]) for doc in train_set])\n",
    "    train_regressors = sm.add_constant(train_regressors)\n",
    "    predictor = logistic_predictor_from_data(train_targets, train_regressors)\n",
    "\n",
    "    test_data = test_set\n",
    "    if infer:\n",
    "        if infer_subsample < 1.0:\n",
    "            test_data = sample(test_data, int(infer_subsample * len(test_data)))\n",
    "        test_regressors = [test_model.infer_vector(doc.words, steps=infer_steps, alpha=infer_alpha) for doc in test_data]\n",
    "    else:\n",
    "        test_regressors = [test_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_regressors = sm.add_constant(test_regressors)\n",
    "    \n",
    "    # Predict & evaluate\n",
    "    test_predictions = predictor.predict(test_regressors)\n",
    "    corrects = sum(np.rint(test_predictions) == [doc.sentiment for doc in test_data])\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    return (error_rate, errors, len(test_predictions), predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an explicit multiple-pass, alpha-reduction approach as sketched in this [gensim doc2vec blog post](http://radimrehurek.com/2014/12/doc2vec-tutorial/) with added shuffling of corpus on each pass.\n",
    "\n",
    "Note that vector training is occurring on *all* documents of the dataset, which includes all TRAIN/TEST/DEV docs.\n",
    "\n",
    "We evaluate each model's sentiment predictive power based on error rate, and the evaluation is repeated after each pass so we can see the rates of relative improvement. The base numbers reuse the TRAIN and TEST vectors stored in the models for the logistic regression, while the _inferred_ results use newly-inferred TEST vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "best_error = defaultdict(lambda: 1.0)  # To selectively print only best errors achieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START 2017-07-13 13:43:50.250656\n",
      "*0.503160 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 3.4s 0.6s\n",
      "*0.498400 : 1 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)_inferred 3.4s 0.8s\n",
      "*0.480800 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 2.0s 0.7s\n",
      "*0.486400 : 1 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)_inferred 2.0s 0.5s\n",
      "*0.493840 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 3.3s 0.5s\n",
      "*0.510800 : 1 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)_inferred 3.3s 0.6s\n",
      "*0.474880 : 1 passes : dbow+dmm 0.0s 2.3s\n",
      "*0.467600 : 1 passes : dbow+dmm_inferred 0.0s 2.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/miniconda3/envs/gensim/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.473440 : 1 passes : dbow+dmc 0.0s 3.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/miniconda3/envs/gensim/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.487600 : 1 passes : dbow+dmc_inferred 0.0s 3.4s\n",
      "Completed pass 1 at alpha 0.025000\n",
      " 0.503640 : 2 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 2.4s 0.4s\n",
      "*0.467280 : 2 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 1.7s 0.5s\n",
      "*0.487680 : 2 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 3.2s 0.6s\n",
      "*0.469360 : 2 passes : dbow+dmm 0.0s 1.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/miniconda3/envs/gensim/lib/python3.6/site-packages/statsmodels/base/model.py:496: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*0.463920 : 2 passes : dbow+dmc 0.0s 3.0s\n",
      "Completed pass 2 at alpha 0.020200\n",
      "*0.502320 : 3 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 2.4s 0.4s\n",
      " 0.467360 : 3 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 1.7s 0.6s\n",
      "*0.486080 : 3 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 3.0s 0.5s\n",
      "*0.462440 : 3 passes : dbow+dmm 0.0s 1.2s\n",
      "*0.463640 : 3 passes : dbow+dmc 0.0s 1.6s\n",
      "Completed pass 3 at alpha 0.015400\n",
      " 0.502800 : 4 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 2.4s 0.4s\n",
      "*0.465880 : 4 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 1.7s 0.6s\n",
      "*0.481480 : 4 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 3.3s 0.5s\n",
      "*0.458440 : 4 passes : dbow+dmm 0.0s 1.2s\n",
      " 0.465800 : 4 passes : dbow+dmc 0.0s 1.4s\n",
      "Completed pass 4 at alpha 0.010600\n",
      " 0.502720 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4) 2.4s 0.4s\n",
      " 0.513600 : 5 passes : Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)_inferred 2.4s 0.9s\n",
      "*0.463400 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4) 1.8s 0.5s\n",
      "*0.459600 : 5 passes : Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)_inferred 1.8s 0.6s\n",
      "*0.479960 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4) 3.2s 0.5s\n",
      "*0.480800 : 5 passes : Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)_inferred 3.2s 0.7s\n",
      " 0.458760 : 5 passes : dbow+dmm 0.0s 1.3s\n",
      "*0.460400 : 5 passes : dbow+dmm_inferred 0.0s 1.5s\n",
      " 0.464360 : 5 passes : dbow+dmc 0.0s 1.4s\n",
      "*0.468400 : 5 passes : dbow+dmc_inferred 0.0s 1.8s\n",
      "Completed pass 5 at alpha 0.005800\n",
      "END 2017-07-13 13:45:07.762375\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "import datetime\n",
    "\n",
    "alpha, min_alpha, passes = (0.025, 0.001, 2)\n",
    "alpha_delta = (alpha - min_alpha) / passes\n",
    "\n",
    "print(\"START %s\" % datetime.datetime.now())\n",
    "\n",
    "for epoch in range(passes):\n",
    "    shuffle(doc_list)  # Shuffling gets best results\n",
    "    \n",
    "    for name, train_model in models_by_name.items():\n",
    "        # Train\n",
    "        duration = 'na'\n",
    "        train_model.alpha, train_model.min_alpha = alpha, alpha\n",
    "        with elapsed_timer() as elapsed:\n",
    "            train_model.train(doc_list, total_examples=len(doc_list), epochs=1)\n",
    "            duration = '%.1f' % elapsed()\n",
    "            \n",
    "        # Evaluate\n",
    "        eval_duration = ''\n",
    "        with elapsed_timer() as eval_elapsed:\n",
    "            err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs)\n",
    "        eval_duration = '%.1f' % eval_elapsed()\n",
    "        best_indicator = ' '\n",
    "        if err <= best_error[name]:\n",
    "            best_error[name] = err\n",
    "            best_indicator = '*' \n",
    "        print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, err, epoch + 1, name, duration, eval_duration))\n",
    "\n",
    "        if ((epoch + 1) % 5) == 0 or epoch == 0:\n",
    "            eval_duration = ''\n",
    "            with elapsed_timer() as eval_elapsed:\n",
    "                infer_err, err_count, test_count, predictor = error_rate_for_model(train_model, train_docs, test_docs, infer=True)\n",
    "            eval_duration = '%.1f' % eval_elapsed()\n",
    "            best_indicator = ' '\n",
    "            if infer_err < best_error[name + '_inferred']:\n",
    "                best_error[name + '_inferred'] = infer_err\n",
    "                best_indicator = '*'\n",
    "            print(\"%s%f : %i passes : %s %ss %ss\" % (best_indicator, infer_err, epoch + 1, name + '_inferred', duration, eval_duration))\n",
    "\n",
    "    print('Completed pass %i at alpha %f' % (epoch + 1, alpha))\n",
    "    alpha -= alpha_delta\n",
    "    \n",
    "print(\"END %s\" % str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achieved Sentiment-Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Err rate Model\n",
      "0.458440 dbow+dmm\n",
      "0.459600 Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)_inferred\n",
      "0.460400 dbow+dmm_inferred\n",
      "0.463400 Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)\n",
      "0.463640 dbow+dmc\n",
      "0.468400 dbow+dmc_inferred\n",
      "0.479960 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)\n",
      "0.480800 Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)_inferred\n",
      "0.498400 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)_inferred\n",
      "0.502320 Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)\n"
     ]
    }
   ],
   "source": [
    "# Print best error rates achieved\n",
    "print(\"Err rate Model\")\n",
    "for rate, name in sorted((rate, name) for name, rate in best_error.items()):\n",
    "    print(\"%f %s\" % (rate, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our testing, contrary to the results of the paper, PV-DBOW performs best. Concatenating vectors from different models only offers a small predictive improvement over averaging vectors. There best results reproduced are just under 10% error rate, still a long way from the paper's reported 7.42% error rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are inferred vectors close to the precalculated ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for doc 58610...\n",
      "Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4):\n",
      " [(49592, 0.4728042781352997), (18864, 0.4200856685638428), (7272, 0.4115872383117676)]\n",
      "Doc2Vec(dbow,d100,n5,mc2,s0.001,t4):\n",
      " [(59066, 0.9939239025115967), (51338, 0.9919757843017578), (13706, 0.991830050945282)]\n",
      "Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4):\n",
      " [(61786, 0.718172013759613), (69876, 0.7181297540664673), (66275, 0.7081122994422913)]\n"
     ]
    }
   ],
   "source": [
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # Pick random doc; re-run cell for more examples\n",
    "print('for doc %d...' % doc_id)\n",
    "for model in simple_models:\n",
    "    inferred_docvec = model.infer_vector(alldocs[doc_id].words)\n",
    "    print('%s:\\n %s' % (model, model.docvecs.most_similar([inferred_docvec], topn=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Yes, here the stored vector from 20 epochs of training is usually one of the closest to a freshly-inferred vector for the same words. Note the defaults for inference are very abbreviated â just 3 steps starting at a high alpha â and likely need tuning for other applications.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do close documents seem more related than distant ones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9400c08aa28a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdoc_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pick random doc, re-run cell for more examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimple_models\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# and a random model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get *all* similar documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "doc_id = np.random.randint(simple_models[0].docvecs.count)  # pick random doc, re-run cell for more examples\n",
    "model = random.choice(simple_models)  # and a random model\n",
    "sims = model.docvecs.most_similar(doc_id, topn=model.docvecs.count)  # get *all* similar documents\n",
    "print(u'TARGET (%d): Â«%sÂ»\\n' % (doc_id, ' '.join(alldocs[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: Â«%sÂ»\\n' % (label, sims[index], ' '.join(alldocs[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Somewhat, in terms of reviewer tone, movie genre, etc... the MOST cosine-similar docs usually seem more like the TARGET than the MEDIAN or LEAST.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the word vectors show useful similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_models = simple_models[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most similar words for 'earned' (86 occurences)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr><th>Doc2Vec(dm/c,d100,n5,w5,mc2,s0.001,t4)</th><th>Doc2Vec(dbow,d100,n5,mc2,s0.001,t4)</th><th>Doc2Vec(dm/m,d100,n5,w10,mc2,s0.001,t4)</th></tr><tr><td>[('carry-forwards', 0.6992895603179932),<br>\n",
       "('profits', 0.6931719779968262),<br>\n",
       "('generated', 0.667393684387207),<br>\n",
       "('realized', 0.6543018221855164),<br>\n",
       "('mainly', 0.640326976776123),<br>\n",
       "('produced', 0.6285237073898315),<br>\n",
       "('settled', 0.6267350912094116),<br>\n",
       "('primarily', 0.626006007194519),<br>\n",
       "('varies', 0.625120222568512),<br>\n",
       "('collected', 0.6182005405426025),<br>\n",
       "('cast', 0.6157312393188477),<br>\n",
       "('resulting', 0.6111900210380554),<br>\n",
       "('received', 0.6058673858642578),<br>\n",
       "('incurred', 0.6030126214027405),<br>\n",
       "('comes', 0.59555983543396),<br>\n",
       "('flexibility', 0.5949200391769409),<br>\n",
       "('partially', 0.5900612473487854),<br>\n",
       "('freight', 0.5863196849822998),<br>\n",
       "('ourselves', 0.5817102193832397),<br>\n",
       "('principally', 0.579599142074585)]</td><td>[('measurement-period', 0.4070809483528137),<br>\n",
       "('0001510963-16-000042', 0.3565326929092407),<br>\n",
       "('$24', 0.3294537365436554),<br>\n",
       "('downloads', 0.3193413317203522),<br>\n",
       "('649', 0.3189377188682556),<br>\n",
       "('endpoints', 0.3125460147857666),<br>\n",
       "('operate', 0.3072226643562317),<br>\n",
       "('chbm-20130331', 0.3043592870235443),<br>\n",
       "('100mg', 0.29663488268852234),<br>\n",
       "('r389', 0.2921972870826721),<br>\n",
       "('gw', 0.2911043167114258),<br>\n",
       "('exceeding', 0.29108208417892456),<br>\n",
       "('grade', 0.2871429920196533),<br>\n",
       "('r254', 0.28633126616477966),<br>\n",
       "('molecular', 0.2842804491519928),<br>\n",
       "('unknown', 0.28414034843444824),<br>\n",
       "('scheme', 0.2755950689315796),<br>\n",
       "('receiver', 0.2748987674713135),<br>\n",
       "('written', 0.27474677562713623),<br>\n",
       "('nos', 0.273994505405426)]</td><td>[('realizable', 0.75834059715271),<br>\n",
       "('realized', 0.7453017234802246),<br>\n",
       "('when', 0.631424069404602),<br>\n",
       "('royalty', 0.6273903250694275),<br>\n",
       "('revenue', 0.6218245029449463),<br>\n",
       "('benefit', 0.6154765486717224),<br>\n",
       "('realize', 0.6037532091140747),<br>\n",
       "('classifications', 0.598984956741333),<br>\n",
       "('more', 0.5979105234146118),<br>\n",
       "('service', 0.5829275846481323),<br>\n",
       "('settled', 0.5645585060119629),<br>\n",
       "('eventual', 0.5576108694076538),<br>\n",
       "('payment', 0.5559539198875427),<br>\n",
       "('receive', 0.5538958311080933),<br>\n",
       "('benefits', 0.5519713163375854),<br>\n",
       "('achievement', 0.5484340786933899),<br>\n",
       "('cycle', 0.5443072319030762),<br>\n",
       "('less', 0.536905825138092),<br>\n",
       "('cost', 0.5353676080703735),<br>\n",
       "('payments', 0.530110239982605)]</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from IPython.display import HTML\n",
    "# pick a random word with a suitable number of occurences\n",
    "while True:\n",
    "    word = random.choice(word_models[0].wv.index2word)\n",
    "    if word_models[0].wv.vocab[word].count > 10:\n",
    "        break\n",
    "# or uncomment below line, to just pick a word from the relevant domain:\n",
    "#word = 'comedy/drama'\n",
    "similars_per_model = [str(model.most_similar(word, topn=20)).replace('), ','),<br>\\n') for model in word_models]\n",
    "similar_table = (\"<table><tr><th>\" +\n",
    "    \"</th><th>\".join([str(model) for model in word_models]) + \n",
    "    \"</th></tr><tr><td>\" +\n",
    "    \"</td><td>\".join(similars_per_model) +\n",
    "    \"</td></tr></table>\")\n",
    "print(\"most similar words for '%s' (%d occurences)\" % (word, simple_models[0].wv.vocab[word].count))\n",
    "HTML(similar_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the DBOW words look meaningless? That's because the gensim DBOW model doesn't train word vectors â they remain at their random initialized values â unless you ask with the `dbow_words=1` initialization parameter. Concurrent word-training slows DBOW mode significantly, and offers little improvement (and sometimes a little worsening) of the error rate on this IMDB sentiment-prediction task. \n",
    "\n",
    "Words from DM models tend to show meaningfully similar words when there are many examples in the training data (as with 'plot' or 'actor'). (All DM modes inherently involve word vector training concurrent with doc vector training.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are the word vectors from this dataset any good at analogies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download this file: https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt\n",
    "# and place it in the local directory\n",
    "# Note: this takes many minutes\n",
    "if os.path.isfile('question-words.txt'):\n",
    "    for model in word_models:\n",
    "        sections = model.accuracy('questions-words.txt')\n",
    "        correct, incorrect = len(sections[-1]['correct']), len(sections[-1]['incorrect'])\n",
    "        print('%s: %0.2f%% correct (%d of %d)' % (model, float(correct*100)/(correct+incorrect), correct, correct+incorrect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though this is a tiny, domain-specific dataset, it shows some meager capability on the general word analogies â at least for the DM/concat and DM/mean models which actually train word vectors. (The untrained random-initialized words of the DBOW model of course fail miserably.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-2e538089e8ac>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-2e538089e8ac>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    This cell left intentionally erroneous.\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "This cell left intentionally erroneous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To mix the Google dataset (if locally available) into the word tests..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v_g100b = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "w2v_g100b.compact_name = 'w2v_g100b'\n",
    "word_models.append(w2v_g100b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get copious logging output from above steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "rootLogger = logging.getLogger()\n",
    "rootLogger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To auto-reload python code while developing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
