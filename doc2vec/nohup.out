dm_dbow __init__ took 0.3180046081542969 seconds
dm_dbow __init__ took 0.431154727935791 seconds
saved_tfidf_models/tfidf(0.3,1.0,100).pkl
tfidf __init__ took 0.05510902404785156 seconds
dm_dbow __init__ took 0.4053518772125244 seconds
saved_tfidf_models/tfidf(0.3,0.9,100).pkl
tfidf __init__ took 0.0529935359954834 seconds
dm_dbow __init__ took 0.405839204788208 seconds
saved_tfidf_models/tfidf(0.3,0.9,100).pkl
tfidf __init__ took 0.050298213958740234 seconds
dm_dbow __init__ took 0.38880467414855957 seconds
saved_tfidf_models/tfidf(0.3,0.9,100).pkl
tfidf __init__ took 0.04999852180480957 seconds
string price: 1.8591981136572546
string price: 1.2885263121198216
string price: 1.0773845281177161
string price: 1.121257804323449
string price: 8.965153219298495
string price: 1.2990684024571935
string price: 9.064884734018875
string price: 2.006564011672051
string price: 2.3335508561398757
string price: 2.3359429007602555
string price: 2.3359429007602555
string price: 2.3359429007602555
string price: 1.2591215623738339
string price: 1.2787003960170784
string price: 6.266887232538808
string price: 1.5002095829715556
dataset_size = 66273
labels len = 66273
experiment starting with estimators=[('union', FeatureUnion(n_jobs=1,
       transformer_list=[('linguistic', Pipeline(memory=None,
     steps=[('selector', CustomDictVectorizer(key='corpus')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=...tory', Pipeline(memory=None,
     steps=[('selector', CustomDictVectorizer(key='price_history'))]))],
       transformer_weights={'linguistic': 1.0, 'price_history': 1.0})), ('clf', XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,
       max_depth=3, min_child_weight=1, missing=None, n_estimators=300,
       n_jobs=1, nthread=4, objective='multi:softmax', random_state=0,
       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
       silent=True, subsample=1))] param_dict={'clf__max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__min_child_weight': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__subsample': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'clf__colsample_bytree': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'clf__learning_rate': array([0.01, 0.05, 0.1 , 0.15, 0.2 , 0.3 , 0.5 , 0.7 ]), 'clf__gamma': array([ 0.01,  0.05,  0.1 ,  0.15,  0.2 ,  0.3 ,  0.5 ,  0.7 ,  2.  ,
        5.  , 10.  ]), 'clf__reg_lambda': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__reg_alpha': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__objective': array(['multi:softmax', 'multi:softprob'], dtype='<U14'), 'union__linguistic__tfidf__max_df': array([0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]), 'union__linguistic__tfidf__min_df': array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45]), 'union__linguistic__tfidf__sublinear_tf': array([False,  True]), 'union__linguistic__tfidf__norm': array(['l1', 'l2', None], dtype=object), 'union__linguistic__tfidf__ngram_range': array([[1, 1],
       [1, 2],
       [1, 3],
       [2, 2],
       [2, 3],
       [3, 3]]), 'union__linguistic__nmf__n_components': array([ 25,  50,  75, 100, 125, 150]), 'union__linguistic__nmf__init': array(['random', 'nndsvd', 'nndsvda'], dtype='<U7'), 'union__linguistic__nmf__beta_loss': array(['frobenius', 'kullback-leibler', 'itakura-saito'], dtype='<U16'), 'union__linguistic__nmf__max_iter': array([ 50, 100, 200, 300, 400, 500]), 'union__linguistic__nmf__alpha': array([ 0. ,  0.1,  0.5,  1. ,  5. , 10. ]), 'union__linguistic__nmf__l1_ratio': array([0. , 0.2, 0.4, 0.6, 0.8, 1. ])}
Top 3 lines
#1: joblib/numpy_pickle.py:108: 3521526.8 KiB
    array = pickle.load(unpickler.file_handle)
#2: doc2vec/utils_v2.py:400: 517.9 KiB
    dataset['labels'] = np.array(list(labels))
#3: doc2vec/utils_v2.py:374: 6.0 KiB
    with open(path, 'r') as t:
115 other: 66.6 KiB
Total allocated size: 3522117.3 KiB
66273
66273
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.
  if diff:
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py", line 350, in __call__
    return self.func(*args, **kwargs)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 131, in <listcomp>
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py", line 492, in _fit_and_score
    is_multimetric)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py", line 523, in _score
    return _multimetric_score(estimator, X_test, y_test, scorer)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py", line 553, in _multimetric_score
    score = scorer(estimator, X_test, y_test)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/scorer.py", line 244, in _passthrough_scorer
    return estimator.score(*args, **kwargs)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py", line 115, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py", line 486, in score
    Xt = transform.transform(Xt)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py", line 768, in transform
    for name, trans, weight in self._iter())
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 779, in __call__
    while self.dispatch_one_batch(iterator):
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 625, in dispatch_one_batch
    self._dispatch(tasks)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 588, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py", line 111, in apply_async
    result = ImmediateResult(func)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py", line 332, in __init__
    self.results = batch()
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 131, in <listcomp>
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py", line 571, in _transform_one
    res = transformer.transform(X)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py", line 426, in _transform
    Xt = transform.transform(Xt)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py", line 1410, in transform
    return self._tfidf.transform(X, copy=False)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py", line 1111, in transform
    X = X * self._idf_diag
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/scipy/sparse/base.py", line 440, in __mul__
    return self._mul_sparse_matrix(other)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py", line 503, in _mul_sparse_matrix
    data = np.empty(nnz, dtype=upcast(self.dtype, other.dtype))
MemoryError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/pool.py", line 119, in worker
    result = (True, func(*args, **kwds))
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py", line 359, in __call__
    raise TransportableException(text, e_type)
sklearn.externals.joblib.my_exceptions.TransportableException: TransportableException
___________________________________________________________________________
MemoryError                                        Sat Apr 14 20:01:52 2018
PID: 10007                 Python 3.6.4: /home/ubuntu/miniconda3/bin/python
...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 44179, 44180, 44181]), array([44182, 44183, 44184, ..., 66270, 66271, 66272]), 0, {'clf__colsample_bytree': 0.6, 'clf__gamma': 0.7, 'clf__learning_rate': 0.7, 'clf__max_depth': 2, 'clf__min_child_weight': 2, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.7, 'clf__reg_lambda': 1.0, 'clf__subsample': 0.6, 'union__linguistic__nmf__alpha': 0.0, ...}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 44179, 44180, 44181]), array([44182, 44183, 44184, ..., 66270, 66271, 66272]), 0, {'clf__colsample_bytree': 0.6, 'clf__gamma': 0.7, 'clf__learning_rate': 0.7, 'clf__max_depth': 2, 'clf__min_child_weight': 2, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.7, 'clf__reg_lambda': 1.0, 'clf__subsample': 0.6, 'union__linguistic__nmf__alpha': 0.0, ...})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), X=array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), y=array([0, 0, 0, ..., 0, 1, 1]), scorer={'score': <function _passthrough_scorer>}, train=array([    0,     1,     2, ..., 44179, 44180, 44181]), test=array([44182, 44183, 44184, ..., 66270, 66271, 66272]), verbose=0, parameters={'clf__colsample_bytree': 0.6, 'clf__gamma': 0.7, 'clf__learning_rate': 0.7, 'clf__max_depth': 2, 'clf__min_child_weight': 2, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.7, 'clf__reg_lambda': 1.0, 'clf__subsample': 0.6, 'union__linguistic__nmf__alpha': 0.0, ...}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')
    487         # _score will return dict if is_multimetric is True
    488         test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)
    489         score_time = time.time() - start_time - fit_time
    490         if return_train_score:
    491             train_scores = _score(estimator, X_train, y_train, scorer,
--> 492                                   is_multimetric)
        is_multimetric = True
    493 
    494     if verbose > 2:
    495         if is_multimetric:
    496             for scorer_name, score in test_scores.items():

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), X_test=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y_test=array([ 0,  0,  0, ..., -1,  0,  1]), scorer={'score': <function _passthrough_scorer>}, is_multimetric=True)
    518 
    519     Will return a single float if is_multimetric is False and a dict of floats,
    520     if is_multimetric is True
    521     """
    522     if is_multimetric:
--> 523         return _multimetric_score(estimator, X_test, y_test, scorer)
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  0,  0, ..., -1,  0,  1])
        scorer = {'score': <function _passthrough_scorer>}
    524     else:
    525         if y_test is None:
    526             score = scorer(estimator, X_test)
    527         else:

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _multimetric_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), X_test=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y_test=array([ 0,  0,  0, ..., -1,  0,  1]), scorers={'score': <function _passthrough_scorer>})
    548 
    549     for name, scorer in scorers.items():
    550         if y_test is None:
    551             score = scorer(estimator, X_test)
    552         else:
--> 553             score = scorer(estimator, X_test, y_test)
        score = undefined
        scorer = <function _passthrough_scorer>
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  0,  0, ..., -1,  0,  1])
    554 
    555         if hasattr(score, 'item'):
    556             try:
    557                 # e.g. unwrap memmapped scalars

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/scorer.py in _passthrough_scorer(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), *args=(array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  0,  0, ..., -1,  0,  1])), **kwargs={})
    239     return scorer
    240 
    241 
    242 def _passthrough_scorer(estimator, *args, **kwargs):
    243     """Function that wraps estimator.score"""
--> 244     return estimator.score(*args, **kwargs)
        estimator.score = <function Pipeline.score>
        args = (array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  0,  0, ..., -1,  0,  1]))
        kwargs = {}
    245 
    246 
    247 def check_scoring(estimator, scoring=None, allow_none=False):
    248     """Determine scorer from user options.

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in <lambda>(*args=(array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  0,  0, ..., -1,  0,  1])), **kwargs={})
    110                     break
    111             else:
    112                 attrgetter(self.delegate_names[-1])(obj)
    113 
    114         # lambda, but not partial, allows help() to work with update_wrapper
--> 115         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
        args = (array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  0,  0, ..., -1,  0,  1]))
        kwargs = {}
    116         # update the docstring of the returned function
    117         update_wrapper(out, self.fn)
    118         return out
    119 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in score(self=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y=array([ 0,  0,  0, ..., -1,  0,  1]), sample_weight=None)
    481         score : float
    482         """
    483         Xt = X
    484         for name, transform in self.steps[:-1]:
    485             if transform is not None:
--> 486                 Xt = transform.transform(Xt)
        Xt = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        transform.transform = <bound method FeatureUnion.transform of FeatureU...ights={'linguistic': 1.0, 'price_history': 1.0})>
    487         score_params = {}
    488         if sample_weight is not None:
    489             score_params['sample_weight'] = sample_weight
    490         return self.steps[-1][-1].score(Xt, y, **score_params)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in transform(self=FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    763             hstack of results of transformers. sum_n_components is the
    764             sum of n_components (output dimension) over transformers.
    765         """
    766         Xs = Parallel(n_jobs=self.n_jobs)(
    767             delayed(_transform_one)(trans, weight, X)
--> 768             for name, trans, weight in self._iter())
        self._iter = <bound method FeatureUnion._iter of FeatureUnion...ights={'linguistic': 1.0, 'price_history': 1.0})>
    769         if not Xs:
    770             # All transformers are None
    771             return np.zeros((X.shape[0], 0))
    772         if any(sparse.issparse(f) for f in Xs):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=1), iterable=<generator object FeatureUnion.transform.<locals>.<genexpr>>)
    774         self.n_completed_tasks = 0
    775         try:
    776             # Only set self._iterating to True if at least a batch
    777             # was dispatched. In particular this covers the edge
    778             # case of Parallel used with an exhausted iterator.
--> 779             while self.dispatch_one_batch(iterator):
        self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>
        iterator = <generator object FeatureUnion.transform.<locals>.<genexpr>>
    780                 self._iterating = True
    781             else:
    782                 self._iterating = False
    783 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object FeatureUnion.transform.<locals>.<genexpr>>)
    620             tasks = BatchedCalls(itertools.islice(iterator, batch_size))
    621             if len(tasks) == 0:
    622                 # No more tasks available in the iterator: tell caller to stop.
    623                 return False
    624             else:
--> 625                 self._dispatch(tasks)
        self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>
        tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>
    626                 return True
    627 
    628     def _print(self, msg, msg_args):
    629         """Display the message on stout or stderr depending on verbosity"""

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    583         self.n_dispatched_tasks += len(batch)
    584         self.n_dispatched_batches += 1
    585 
    586         dispatch_timestamp = time.time()
    587         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--> 588         job = self._backend.apply_async(batch, callback=cb)
        job = undefined
        self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
        cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>
    589         self._jobs.append(job)
    590 
    591     def dispatch_next(self):
    592         """Dispatch more data for parallel processing

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)
    106             raise ValueError('n_jobs == 0 in Parallel has no meaning')
    107         return 1
    108 
    109     def apply_async(self, func, callback=None):
    110         """Schedule a func to be run"""
--> 111         result = ImmediateResult(func)
        result = undefined
        func = <sklearn.externals.joblib.parallel.BatchedCalls object>
    112         if callback:
    113             callback(result)
    114         return result
    115 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    327 
    328 class ImmediateResult(object):
    329     def __init__(self, batch):
    330         # Don't delay the application, to avoid keeping the input
    331         # arguments in memory
--> 332         self.results = batch()
        self.results = undefined
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
    333 
    334     def get(self):
    335         return self.results
    336 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _transform_one>, (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)), {})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _transform_one>
        args = (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
        kwargs = {}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _transform_one(transformer=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), weight=1.0, X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    566 def _fit_one_transformer(transformer, X, y):
    567     return transformer.fit(X, y)
    568 
    569 
    570 def _transform_one(transformer, weight, X):
--> 571     res = transformer.transform(X)
        res = undefined
        transformer.transform = <bound method Pipeline._transform of Pipeline(me...=False,
  solver='cd', tol=0.0001, verbose=0))])>
        X = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
    572     # if we have a weight for this transformer, multiply output
    573     if weight is None:
    574         return res
    575     return res * weight

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _transform(self=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    421 
    422     def _transform(self, X):
    423         Xt = X
    424         for name, transform in self.steps:
    425             if transform is not None:
--> 426                 Xt = transform.transform(Xt)
        Xt = array(['numb conform conform file a datum confor...sult operation peter peter'],
      dtype=object)
        transform.transform = <bound method TfidfVectorizer.transform of Tfidf...  tokenizer=None, use_idf=True, vocabulary=None)>
    427         return Xt
    428 
    429     @property
    430     def inverse_transform(self):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in transform(self=TfidfVectorizer(analyzer='word', binary=False, d...   tokenizer=None, use_idf=True, vocabulary=None), raw_documents=array(['numb conform conform file a datum confor...sult operation peter peter'],
      dtype=object), copy=True)
   1405             Tf-idf-weighted document-term matrix.
   1406         """
   1407         check_is_fitted(self, '_tfidf', 'The tfidf vector is not fitted')
   1408 
   1409         X = super(TfidfVectorizer, self).transform(raw_documents)
-> 1410         return self._tfidf.transform(X, copy=False)
        self._tfidf.transform = <bound method TfidfTransformer.transform of Tfid...mooth_idf=True, sublinear_tf=True, use_idf=True)>
        X = <44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>
        copy = True

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in transform(self=TfidfTransformer(norm=None, smooth_idf=True, sublinear_tf=True, use_idf=True), X=<44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, copy=False)
   1106             if n_features != expected_n_features:
   1107                 raise ValueError("Input has n_features=%d while the model"
   1108                                  " has been trained with n_features=%d" % (
   1109                                      n_features, expected_n_features))
   1110             # *= doesn't work
-> 1111             X = X * self._idf_diag
        X = <44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>
        self._idf_diag = <474x474 sparse matrix of type '<class 'numpy.fl... stored elements in Compressed Sparse Row format>
   1112 
   1113         if self.norm:
   1114             X = normalize(X, norm=self.norm, copy=False)
   1115 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/scipy/sparse/base.py in __mul__(self=<44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, other=<474x474 sparse matrix of type '<class 'numpy.fl... stored elements in Compressed Sparse Row format>)
    435             return self._mul_scalar(other)
    436 
    437         if issparse(other):
    438             if self.shape[1] != other.shape[0]:
    439                 raise ValueError('dimension mismatch')
--> 440             return self._mul_sparse_matrix(other)
        self._mul_sparse_matrix = <bound method _cs_matrix._mul_sparse_matrix of <...stored elements in Compressed Sparse Row format>>
        other = <474x474 sparse matrix of type '<class 'numpy.fl... stored elements in Compressed Sparse Row format>
    441 
    442         # If it's a list or whatever, treat it like a matrix
    443         other_a = np.asanyarray(other)
    444 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py in _mul_sparse_matrix(self=<44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, other=<474x474 sparse matrix of type '<class 'numpy.fl... stored elements in Compressed Sparse Row format>)
    498         idx_dtype = get_index_dtype((self.indptr, self.indices,
    499                                      other.indptr, other.indices),
    500                                     maxval=nnz)
    501         indptr = np.asarray(indptr, dtype=idx_dtype)
    502         indices = np.empty(nnz, dtype=idx_dtype)
--> 503         data = np.empty(nnz, dtype=upcast(self.dtype, other.dtype))
        data = undefined
        nnz = 11560630
        self.dtype = dtype('float64')
        other.dtype = dtype('float64')
    504 
    505         fn = getattr(_sparsetools, self.format + '_matmat_pass2')
    506         fn(M, N, np.asarray(self.indptr, dtype=idx_dtype),
    507            np.asarray(self.indices, dtype=idx_dtype),

MemoryError: 
___________________________________________________________________________
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 699, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/pool.py", line 644, in get
    raise self._value
sklearn.externals.joblib.my_exceptions.TransportableException: TransportableException
___________________________________________________________________________
MemoryError                                        Sat Apr 14 20:01:52 2018
PID: 10007                 Python 3.6.4: /home/ubuntu/miniconda3/bin/python
...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 44179, 44180, 44181]), array([44182, 44183, 44184, ..., 66270, 66271, 66272]), 0, {'clf__colsample_bytree': 0.6, 'clf__gamma': 0.7, 'clf__learning_rate': 0.7, 'clf__max_depth': 2, 'clf__min_child_weight': 2, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.7, 'clf__reg_lambda': 1.0, 'clf__subsample': 0.6, 'union__linguistic__nmf__alpha': 0.0, ...}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 44179, 44180, 44181]), array([44182, 44183, 44184, ..., 66270, 66271, 66272]), 0, {'clf__colsample_bytree': 0.6, 'clf__gamma': 0.7, 'clf__learning_rate': 0.7, 'clf__max_depth': 2, 'clf__min_child_weight': 2, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.7, 'clf__reg_lambda': 1.0, 'clf__subsample': 0.6, 'union__linguistic__nmf__alpha': 0.0, ...})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), X=array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), y=array([0, 0, 0, ..., 0, 1, 1]), scorer={'score': <function _passthrough_scorer>}, train=array([    0,     1,     2, ..., 44179, 44180, 44181]), test=array([44182, 44183, 44184, ..., 66270, 66271, 66272]), verbose=0, parameters={'clf__colsample_bytree': 0.6, 'clf__gamma': 0.7, 'clf__learning_rate': 0.7, 'clf__max_depth': 2, 'clf__min_child_weight': 2, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.7, 'clf__reg_lambda': 1.0, 'clf__subsample': 0.6, 'union__linguistic__nmf__alpha': 0.0, ...}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')
    487         # _score will return dict if is_multimetric is True
    488         test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)
    489         score_time = time.time() - start_time - fit_time
    490         if return_train_score:
    491             train_scores = _score(estimator, X_train, y_train, scorer,
--> 492                                   is_multimetric)
        is_multimetric = True
    493 
    494     if verbose > 2:
    495         if is_multimetric:
    496             for scorer_name, score in test_scores.items():

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), X_test=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y_test=array([ 0,  0,  0, ..., -1,  0,  1]), scorer={'score': <function _passthrough_scorer>}, is_multimetric=True)
    518 
    519     Will return a single float if is_multimetric is False and a dict of floats,
    520     if is_multimetric is True
    521     """
    522     if is_multimetric:
--> 523         return _multimetric_score(estimator, X_test, y_test, scorer)
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  0,  0, ..., -1,  0,  1])
        scorer = {'score': <function _passthrough_scorer>}
    524     else:
    525         if y_test is None:
    526             score = scorer(estimator, X_test)
    527         else:

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _multimetric_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), X_test=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y_test=array([ 0,  0,  0, ..., -1,  0,  1]), scorers={'score': <function _passthrough_scorer>})
    548 
    549     for name, scorer in scorers.items():
    550         if y_test is None:
    551             score = scorer(estimator, X_test)
    552         else:
--> 553             score = scorer(estimator, X_test, y_test)
        score = undefined
        scorer = <function _passthrough_scorer>
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  0,  0, ..., -1,  0,  1])
    554 
    555         if hasattr(score, 'item'):
    556             try:
    557                 # e.g. unwrap memmapped scalars

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/scorer.py in _passthrough_scorer(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), *args=(array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  0,  0, ..., -1,  0,  1])), **kwargs={})
    239     return scorer
    240 
    241 
    242 def _passthrough_scorer(estimator, *args, **kwargs):
    243     """Function that wraps estimator.score"""
--> 244     return estimator.score(*args, **kwargs)
        estimator.score = <function Pipeline.score>
        args = (array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  0,  0, ..., -1,  0,  1]))
        kwargs = {}
    245 
    246 
    247 def check_scoring(estimator, scoring=None, allow_none=False):
    248     """Determine scorer from user options.

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in <lambda>(*args=(array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  0,  0, ..., -1,  0,  1])), **kwargs={})
    110                     break
    111             else:
    112                 attrgetter(self.delegate_names[-1])(obj)
    113 
    114         # lambda, but not partial, allows help() to work with update_wrapper
--> 115         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
        args = (array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  0,  0, ..., -1,  0,  1]))
        kwargs = {}
    116         # update the docstring of the returned function
    117         update_wrapper(out, self.fn)
    118         return out
    119 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in score(self=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y=array([ 0,  0,  0, ..., -1,  0,  1]), sample_weight=None)
    481         score : float
    482         """
    483         Xt = X
    484         for name, transform in self.steps[:-1]:
    485             if transform is not None:
--> 486                 Xt = transform.transform(Xt)
        Xt = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        transform.transform = <bound method FeatureUnion.transform of FeatureU...ights={'linguistic': 1.0, 'price_history': 1.0})>
    487         score_params = {}
    488         if sample_weight is not None:
    489             score_params['sample_weight'] = sample_weight
    490         return self.steps[-1][-1].score(Xt, y, **score_params)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in transform(self=FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    763             hstack of results of transformers. sum_n_components is the
    764             sum of n_components (output dimension) over transformers.
    765         """
    766         Xs = Parallel(n_jobs=self.n_jobs)(
    767             delayed(_transform_one)(trans, weight, X)
--> 768             for name, trans, weight in self._iter())
        self._iter = <bound method FeatureUnion._iter of FeatureUnion...ights={'linguistic': 1.0, 'price_history': 1.0})>
    769         if not Xs:
    770             # All transformers are None
    771             return np.zeros((X.shape[0], 0))
    772         if any(sparse.issparse(f) for f in Xs):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=1), iterable=<generator object FeatureUnion.transform.<locals>.<genexpr>>)
    774         self.n_completed_tasks = 0
    775         try:
    776             # Only set self._iterating to True if at least a batch
    777             # was dispatched. In particular this covers the edge
    778             # case of Parallel used with an exhausted iterator.
--> 779             while self.dispatch_one_batch(iterator):
        self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>
        iterator = <generator object FeatureUnion.transform.<locals>.<genexpr>>
    780                 self._iterating = True
    781             else:
    782                 self._iterating = False
    783 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object FeatureUnion.transform.<locals>.<genexpr>>)
    620             tasks = BatchedCalls(itertools.islice(iterator, batch_size))
    621             if len(tasks) == 0:
    622                 # No more tasks available in the iterator: tell caller to stop.
    623                 return False
    624             else:
--> 625                 self._dispatch(tasks)
        self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>
        tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>
    626                 return True
    627 
    628     def _print(self, msg, msg_args):
    629         """Display the message on stout or stderr depending on verbosity"""

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    583         self.n_dispatched_tasks += len(batch)
    584         self.n_dispatched_batches += 1
    585 
    586         dispatch_timestamp = time.time()
    587         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--> 588         job = self._backend.apply_async(batch, callback=cb)
        job = undefined
        self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
        cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>
    589         self._jobs.append(job)
    590 
    591     def dispatch_next(self):
    592         """Dispatch more data for parallel processing

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)
    106             raise ValueError('n_jobs == 0 in Parallel has no meaning')
    107         return 1
    108 
    109     def apply_async(self, func, callback=None):
    110         """Schedule a func to be run"""
--> 111         result = ImmediateResult(func)
        result = undefined
        func = <sklearn.externals.joblib.parallel.BatchedCalls object>
    112         if callback:
    113             callback(result)
    114         return result
    115 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    327 
    328 class ImmediateResult(object):
    329     def __init__(self, batch):
    330         # Don't delay the application, to avoid keeping the input
    331         # arguments in memory
--> 332         self.results = batch()
        self.results = undefined
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
    333 
    334     def get(self):
    335         return self.results
    336 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _transform_one>, (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)), {})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _transform_one>
        args = (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
        kwargs = {}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _transform_one(transformer=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), weight=1.0, X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    566 def _fit_one_transformer(transformer, X, y):
    567     return transformer.fit(X, y)
    568 
    569 
    570 def _transform_one(transformer, weight, X):
--> 571     res = transformer.transform(X)
        res = undefined
        transformer.transform = <bound method Pipeline._transform of Pipeline(me...=False,
  solver='cd', tol=0.0001, verbose=0))])>
        X = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
    572     # if we have a weight for this transformer, multiply output
    573     if weight is None:
    574         return res
    575     return res * weight

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _transform(self=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    421 
    422     def _transform(self, X):
    423         Xt = X
    424         for name, transform in self.steps:
    425             if transform is not None:
--> 426                 Xt = transform.transform(Xt)
        Xt = array(['numb conform conform file a datum confor...sult operation peter peter'],
      dtype=object)
        transform.transform = <bound method TfidfVectorizer.transform of Tfidf...  tokenizer=None, use_idf=True, vocabulary=None)>
    427         return Xt
    428 
    429     @property
    430     def inverse_transform(self):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in transform(self=TfidfVectorizer(analyzer='word', binary=False, d...   tokenizer=None, use_idf=True, vocabulary=None), raw_documents=array(['numb conform conform file a datum confor...sult operation peter peter'],
      dtype=object), copy=True)
   1405             Tf-idf-weighted document-term matrix.
   1406         """
   1407         check_is_fitted(self, '_tfidf', 'The tfidf vector is not fitted')
   1408 
   1409         X = super(TfidfVectorizer, self).transform(raw_documents)
-> 1410         return self._tfidf.transform(X, copy=False)
        self._tfidf.transform = <bound method TfidfTransformer.transform of Tfid...mooth_idf=True, sublinear_tf=True, use_idf=True)>
        X = <44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>
        copy = True

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in transform(self=TfidfTransformer(norm=None, smooth_idf=True, sublinear_tf=True, use_idf=True), X=<44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, copy=False)
   1106             if n_features != expected_n_features:
   1107                 raise ValueError("Input has n_features=%d while the model"
   1108                                  " has been trained with n_features=%d" % (
   1109                                      n_features, expected_n_features))
   1110             # *= doesn't work
-> 1111             X = X * self._idf_diag
        X = <44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>
        self._idf_diag = <474x474 sparse matrix of type '<class 'numpy.fl... stored elements in Compressed Sparse Row format>
   1112 
   1113         if self.norm:
   1114             X = normalize(X, norm=self.norm, copy=False)
   1115 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/scipy/sparse/base.py in __mul__(self=<44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, other=<474x474 sparse matrix of type '<class 'numpy.fl... stored elements in Compressed Sparse Row format>)
    435             return self._mul_scalar(other)
    436 
    437         if issparse(other):
    438             if self.shape[1] != other.shape[0]:
    439                 raise ValueError('dimension mismatch')
--> 440             return self._mul_sparse_matrix(other)
        self._mul_sparse_matrix = <bound method _cs_matrix._mul_sparse_matrix of <...stored elements in Compressed Sparse Row format>>
        other = <474x474 sparse matrix of type '<class 'numpy.fl... stored elements in Compressed Sparse Row format>
    441 
    442         # If it's a list or whatever, treat it like a matrix
    443         other_a = np.asanyarray(other)
    444 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py in _mul_sparse_matrix(self=<44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, other=<474x474 sparse matrix of type '<class 'numpy.fl... stored elements in Compressed Sparse Row format>)
    498         idx_dtype = get_index_dtype((self.indptr, self.indices,
    499                                      other.indptr, other.indices),
    500                                     maxval=nnz)
    501         indptr = np.asarray(indptr, dtype=idx_dtype)
    502         indices = np.empty(nnz, dtype=idx_dtype)
--> 503         data = np.empty(nnz, dtype=upcast(self.dtype, other.dtype))
        data = undefined
        nnz = 11560630
        self.dtype = dtype('float64')
        other.dtype = dtype('float64')
    504 
    505         fn = getattr(_sparsetools, self.format + '_matmat_pass2')
    506         fn(M, N, np.asarray(self.indptr, dtype=idx_dtype),
    507            np.asarray(self.indices, dtype=idx_dtype),

MemoryError: 
___________________________________________________________________________

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "tf_idf_v2.py", line 137, in <module>
    main()
  File "tf_idf_v2.py", line 86, in main
    run_experiment(estimators, param_grid, pickle_path, dataset)
  File "tf_idf_v2.py", line 124, in run_experiment
    grid_search.fit(dataset['X'], dataset['labels']) 
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py", line 639, in fit
    cv.split(X, y, groups)))
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 789, in __call__
    self.retrieve()
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 740, in retrieve
    raise exception
sklearn.externals.joblib.my_exceptions.JoblibMemoryError: JoblibMemoryError
___________________________________________________________________________
Multiprocessing exception:
...........................................................................
/home/ubuntu/notebooks/text-analytics-for-accountancy/doc2vec/tf_idf_v2.py in <module>()
    132 
    133     with open(pickle_path, 'wb+') as p:
    134         pickle.dump(grid_search.best_estimator_, p)
    135 
    136 if __name__ == '__main__':
--> 137     main()

...........................................................................
/home/ubuntu/notebooks/text-analytics-for-accountancy/doc2vec/tf_idf_v2.py in main()
     81     key = 'tf_idf'
     82     estimators, param_grid, momentum_only, doc2vec, doctag_only = get_estimators(key)
     83     dataset = get_dataset(label_horizon, subset, momentum_only, doc2vec, doctag_only)
     84     pickle_path = key + '_' + subset + '_' + str(label_horizon) + '_best_estimator.pkl'
     85     
---> 86     run_experiment(estimators, param_grid, pickle_path, dataset)
        estimators = [('union', FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0})), ('clf', XGBClassifier(base_score=0.5, booster='gbtree', ...ht=1, seed=None,
       silent=True, subsample=1))]
        param_grid = {'clf__colsample_bytree': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'clf__gamma': array([ 0.01,  0.05,  0.1 ,  0.15,  0.2 ,  0.3 ,  0.5 ,  0.7 ,  2.  ,
        5.  , 10.  ]), 'clf__learning_rate': array([0.01, 0.05, 0.1 , 0.15, 0.2 , 0.3 , 0.5 , 0.7 ]), 'clf__max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__min_child_weight': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__objective': array(['multi:softmax', 'multi:softprob'], dtype='<U14'), 'clf__reg_alpha': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__reg_lambda': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__subsample': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'union__linguistic__nmf__alpha': array([ 0. ,  0.1,  0.5,  1. ,  5. , 10. ]), ...}
        pickle_path = 'tf_idf_full_1_best_estimator.pkl'
        dataset = {'X': array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), 'labels': array([0, 0, 0, ..., 0, 1, 1])}
     87 
     88 """large local variables to garbage collected when this function returns
     89 before: 8GB of memory used before forking
     90 after: 3.5GB of memory used before forking

...........................................................................
/home/ubuntu/notebooks/text-analytics-for-accountancy/doc2vec/tf_idf_v2.py in run_experiment(estimators=[('union', FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0})), ('clf', XGBClassifier(base_score=0.5, booster='gbtree', ...ht=1, seed=None,
       silent=True, subsample=1))], param_dict={'clf__colsample_bytree': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'clf__gamma': array([ 0.01,  0.05,  0.1 ,  0.15,  0.2 ,  0.3 ,  0.5 ,  0.7 ,  2.  ,
        5.  , 10.  ]), 'clf__learning_rate': array([0.01, 0.05, 0.1 , 0.15, 0.2 , 0.3 , 0.5 , 0.7 ]), 'clf__max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__min_child_weight': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__objective': array(['multi:softmax', 'multi:softprob'], dtype='<U14'), 'clf__reg_alpha': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__reg_lambda': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__subsample': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'union__linguistic__nmf__alpha': array([ 0. ,  0.1,  0.5,  1. ,  5. , 10. ]), ...}, pickle_path='tf_idf_full_1_best_estimator.pkl', dataset={'X': array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), 'labels': array([0, 0, 0, ..., 0, 1, 1])})
    119     
    120     print(len(dataset['X']))
    121     print(len(dataset['labels']))
    122 #    grid_search = GridSearchCV(pipe, param_grid=param_dict, cv=ts_cv)
    123     
--> 124     grid_search.fit(dataset['X'], dataset['labels']) 
        grid_search.fit = <bound method BaseSearchCV.fit of RandomizedSear...urn_train_score='warn', scoring=None, verbose=0)>
        dataset = {'X': array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), 'labels': array([0, 0, 0, ..., 0, 1, 1])}
    125 
    126     end = time.time()
    127     print("Total running time: {}".format(end-start))
    128     print(grid_search.cv_results_)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py in fit(self=RandomizedSearchCV(cv=<generator object TimeSeri...turn_train_score='warn', scoring=None, verbose=0), X=array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), y=array([0, 0, 0, ..., 0, 1, 1]), groups=None, **fit_params={})
    634                                   return_train_score=self.return_train_score,
    635                                   return_n_test_samples=True,
    636                                   return_times=True, return_parameters=False,
    637                                   error_score=self.error_score)
    638           for parameters, (train, test) in product(candidate_params,
--> 639                                                    cv.split(X, y, groups)))
        cv.split = <bound method _CVIterableWrapper.split of _CVIte...1]), array([44182, 44183, ..., 66271, 66272]))])>
        X = array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object)
        y = array([0, 0, 0, ..., 0, 1, 1])
        groups = None
    640 
    641         # if one choose to see train score, "out" will contain train score info
    642         if self.return_train_score:
    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=5), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)
    784             if pre_dispatch == "all" or n_jobs == 1:
    785                 # The iterable was consumed all at once by the above for loop.
    786                 # No need to wait for async callbacks to trigger to
    787                 # consumption.
    788                 self._iterating = False
--> 789             self.retrieve()
        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=5)>
    790             # Make sure that we get a last message telling us we are done
    791             elapsed_time = time.time() - self._start_time
    792             self._print('Done %3i out of %3i | elapsed: %s finished',
    793                         (len(self._output), len(self._output),

---------------------------------------------------------------------------
Sub-process traceback:
---------------------------------------------------------------------------
MemoryError                                        Sat Apr 14 20:01:52 2018
PID: 10007                 Python 3.6.4: /home/ubuntu/miniconda3/bin/python
...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 44179, 44180, 44181]), array([44182, 44183, 44184, ..., 66270, 66271, 66272]), 0, {'clf__colsample_bytree': 0.6, 'clf__gamma': 0.7, 'clf__learning_rate': 0.7, 'clf__max_depth': 2, 'clf__min_child_weight': 2, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.7, 'clf__reg_lambda': 1.0, 'clf__subsample': 0.6, 'union__linguistic__nmf__alpha': 0.0, ...}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 44179, 44180, 44181]), array([44182, 44183, 44184, ..., 66270, 66271, 66272]), 0, {'clf__colsample_bytree': 0.6, 'clf__gamma': 0.7, 'clf__learning_rate': 0.7, 'clf__max_depth': 2, 'clf__min_child_weight': 2, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.7, 'clf__reg_lambda': 1.0, 'clf__subsample': 0.6, 'union__linguistic__nmf__alpha': 0.0, ...})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), X=array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), y=array([0, 0, 0, ..., 0, 1, 1]), scorer={'score': <function _passthrough_scorer>}, train=array([    0,     1,     2, ..., 44179, 44180, 44181]), test=array([44182, 44183, 44184, ..., 66270, 66271, 66272]), verbose=0, parameters={'clf__colsample_bytree': 0.6, 'clf__gamma': 0.7, 'clf__learning_rate': 0.7, 'clf__max_depth': 2, 'clf__min_child_weight': 2, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.7, 'clf__reg_lambda': 1.0, 'clf__subsample': 0.6, 'union__linguistic__nmf__alpha': 0.0, ...}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')
    487         # _score will return dict if is_multimetric is True
    488         test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)
    489         score_time = time.time() - start_time - fit_time
    490         if return_train_score:
    491             train_scores = _score(estimator, X_train, y_train, scorer,
--> 492                                   is_multimetric)
        is_multimetric = True
    493 
    494     if verbose > 2:
    495         if is_multimetric:
    496             for scorer_name, score in test_scores.items():

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), X_test=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y_test=array([ 0,  0,  0, ..., -1,  0,  1]), scorer={'score': <function _passthrough_scorer>}, is_multimetric=True)
    518 
    519     Will return a single float if is_multimetric is False and a dict of floats,
    520     if is_multimetric is True
    521     """
    522     if is_multimetric:
--> 523         return _multimetric_score(estimator, X_test, y_test, scorer)
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  0,  0, ..., -1,  0,  1])
        scorer = {'score': <function _passthrough_scorer>}
    524     else:
    525         if y_test is None:
    526             score = scorer(estimator, X_test)
    527         else:

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _multimetric_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), X_test=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y_test=array([ 0,  0,  0, ..., -1,  0,  1]), scorers={'score': <function _passthrough_scorer>})
    548 
    549     for name, scorer in scorers.items():
    550         if y_test is None:
    551             score = scorer(estimator, X_test)
    552         else:
--> 553             score = scorer(estimator, X_test, y_test)
        score = undefined
        scorer = <function _passthrough_scorer>
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  0,  0, ..., -1,  0,  1])
    554 
    555         if hasattr(score, 'item'):
    556             try:
    557                 # e.g. unwrap memmapped scalars

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/scorer.py in _passthrough_scorer(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), *args=(array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  0,  0, ..., -1,  0,  1])), **kwargs={})
    239     return scorer
    240 
    241 
    242 def _passthrough_scorer(estimator, *args, **kwargs):
    243     """Function that wraps estimator.score"""
--> 244     return estimator.score(*args, **kwargs)
        estimator.score = <function Pipeline.score>
        args = (array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  0,  0, ..., -1,  0,  1]))
        kwargs = {}
    245 
    246 
    247 def check_scoring(estimator, scoring=None, allow_none=False):
    248     """Determine scorer from user options.

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in <lambda>(*args=(array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  0,  0, ..., -1,  0,  1])), **kwargs={})
    110                     break
    111             else:
    112                 attrgetter(self.delegate_names[-1])(obj)
    113 
    114         # lambda, but not partial, allows help() to work with update_wrapper
--> 115         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
        args = (array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  0,  0, ..., -1,  0,  1]))
        kwargs = {}
    116         # update the docstring of the returned function
    117         update_wrapper(out, self.fn)
    118         return out
    119 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in score(self=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.6))]), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y=array([ 0,  0,  0, ..., -1,  0,  1]), sample_weight=None)
    481         score : float
    482         """
    483         Xt = X
    484         for name, transform in self.steps[:-1]:
    485             if transform is not None:
--> 486                 Xt = transform.transform(Xt)
        Xt = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        transform.transform = <bound method FeatureUnion.transform of FeatureU...ights={'linguistic': 1.0, 'price_history': 1.0})>
    487         score_params = {}
    488         if sample_weight is not None:
    489             score_params['sample_weight'] = sample_weight
    490         return self.steps[-1][-1].score(Xt, y, **score_params)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in transform(self=FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    763             hstack of results of transformers. sum_n_components is the
    764             sum of n_components (output dimension) over transformers.
    765         """
    766         Xs = Parallel(n_jobs=self.n_jobs)(
    767             delayed(_transform_one)(trans, weight, X)
--> 768             for name, trans, weight in self._iter())
        self._iter = <bound method FeatureUnion._iter of FeatureUnion...ights={'linguistic': 1.0, 'price_history': 1.0})>
    769         if not Xs:
    770             # All transformers are None
    771             return np.zeros((X.shape[0], 0))
    772         if any(sparse.issparse(f) for f in Xs):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=1), iterable=<generator object FeatureUnion.transform.<locals>.<genexpr>>)
    774         self.n_completed_tasks = 0
    775         try:
    776             # Only set self._iterating to True if at least a batch
    777             # was dispatched. In particular this covers the edge
    778             # case of Parallel used with an exhausted iterator.
--> 779             while self.dispatch_one_batch(iterator):
        self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>
        iterator = <generator object FeatureUnion.transform.<locals>.<genexpr>>
    780                 self._iterating = True
    781             else:
    782                 self._iterating = False
    783 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object FeatureUnion.transform.<locals>.<genexpr>>)
    620             tasks = BatchedCalls(itertools.islice(iterator, batch_size))
    621             if len(tasks) == 0:
    622                 # No more tasks available in the iterator: tell caller to stop.
    623                 return False
    624             else:
--> 625                 self._dispatch(tasks)
        self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>
        tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>
    626                 return True
    627 
    628     def _print(self, msg, msg_args):
    629         """Display the message on stout or stderr depending on verbosity"""

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    583         self.n_dispatched_tasks += len(batch)
    584         self.n_dispatched_batches += 1
    585 
    586         dispatch_timestamp = time.time()
    587         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--> 588         job = self._backend.apply_async(batch, callback=cb)
        job = undefined
        self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
        cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>
    589         self._jobs.append(job)
    590 
    591     def dispatch_next(self):
    592         """Dispatch more data for parallel processing

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)
    106             raise ValueError('n_jobs == 0 in Parallel has no meaning')
    107         return 1
    108 
    109     def apply_async(self, func, callback=None):
    110         """Schedule a func to be run"""
--> 111         result = ImmediateResult(func)
        result = undefined
        func = <sklearn.externals.joblib.parallel.BatchedCalls object>
    112         if callback:
    113             callback(result)
    114         return result
    115 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    327 
    328 class ImmediateResult(object):
    329     def __init__(self, batch):
    330         # Don't delay the application, to avoid keeping the input
    331         # arguments in memory
--> 332         self.results = batch()
        self.results = undefined
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
    333 
    334     def get(self):
    335         return self.results
    336 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _transform_one>, (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)), {})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _transform_one>
        args = (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
        kwargs = {}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _transform_one(transformer=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), weight=1.0, X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    566 def _fit_one_transformer(transformer, X, y):
    567     return transformer.fit(X, y)
    568 
    569 
    570 def _transform_one(transformer, weight, X):
--> 571     res = transformer.transform(X)
        res = undefined
        transformer.transform = <bound method Pipeline._transform of Pipeline(me...=False,
  solver='cd', tol=0.0001, verbose=0))])>
        X = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
    572     # if we have a weight for this transformer, multiply output
    573     if weight is None:
    574         return res
    575     return res * weight

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _transform(self=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    421 
    422     def _transform(self, X):
    423         Xt = X
    424         for name, transform in self.steps:
    425             if transform is not None:
--> 426                 Xt = transform.transform(Xt)
        Xt = array(['numb conform conform file a datum confor...sult operation peter peter'],
      dtype=object)
        transform.transform = <bound method TfidfVectorizer.transform of Tfidf...  tokenizer=None, use_idf=True, vocabulary=None)>
    427         return Xt
    428 
    429     @property
    430     def inverse_transform(self):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in transform(self=TfidfVectorizer(analyzer='word', binary=False, d...   tokenizer=None, use_idf=True, vocabulary=None), raw_documents=array(['numb conform conform file a datum confor...sult operation peter peter'],
      dtype=object), copy=True)
   1405             Tf-idf-weighted document-term matrix.
   1406         """
   1407         check_is_fitted(self, '_tfidf', 'The tfidf vector is not fitted')
   1408 
   1409         X = super(TfidfVectorizer, self).transform(raw_documents)
-> 1410         return self._tfidf.transform(X, copy=False)
        self._tfidf.transform = <bound method TfidfTransformer.transform of Tfid...mooth_idf=True, sublinear_tf=True, use_idf=True)>
        X = <44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>
        copy = True

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py in transform(self=TfidfTransformer(norm=None, smooth_idf=True, sublinear_tf=True, use_idf=True), X=<44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, copy=False)
   1106             if n_features != expected_n_features:
   1107                 raise ValueError("Input has n_features=%d while the model"
   1108                                  " has been trained with n_features=%d" % (
   1109                                      n_features, expected_n_features))
   1110             # *= doesn't work
-> 1111             X = X * self._idf_diag
        X = <44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>
        self._idf_diag = <474x474 sparse matrix of type '<class 'numpy.fl... stored elements in Compressed Sparse Row format>
   1112 
   1113         if self.norm:
   1114             X = normalize(X, norm=self.norm, copy=False)
   1115 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/scipy/sparse/base.py in __mul__(self=<44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, other=<474x474 sparse matrix of type '<class 'numpy.fl... stored elements in Compressed Sparse Row format>)
    435             return self._mul_scalar(other)
    436 
    437         if issparse(other):
    438             if self.shape[1] != other.shape[0]:
    439                 raise ValueError('dimension mismatch')
--> 440             return self._mul_sparse_matrix(other)
        self._mul_sparse_matrix = <bound method _cs_matrix._mul_sparse_matrix of <...stored elements in Compressed Sparse Row format>>
        other = <474x474 sparse matrix of type '<class 'numpy.fl... stored elements in Compressed Sparse Row format>
    441 
    442         # If it's a list or whatever, treat it like a matrix
    443         other_a = np.asanyarray(other)
    444 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py in _mul_sparse_matrix(self=<44182x474 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, other=<474x474 sparse matrix of type '<class 'numpy.fl... stored elements in Compressed Sparse Row format>)
    498         idx_dtype = get_index_dtype((self.indptr, self.indices,
    499                                      other.indptr, other.indices),
    500                                     maxval=nnz)
    501         indptr = np.asarray(indptr, dtype=idx_dtype)
    502         indices = np.empty(nnz, dtype=idx_dtype)
--> 503         data = np.empty(nnz, dtype=upcast(self.dtype, other.dtype))
        data = undefined
        nnz = 11560630
        self.dtype = dtype('float64')
        other.dtype = dtype('float64')
    504 
    505         fn = getattr(_sparsetools, self.format + '_matmat_pass2')
    506         fn(M, N, np.asarray(self.indptr, dtype=idx_dtype),
    507            np.asarray(self.indices, dtype=idx_dtype),

MemoryError: 
___________________________________________________________________________
dm_dbow __init__ took 4.807173490524292 seconds
dm_dbow __init__ took 2.7608821392059326 seconds
saved_tfidf_models/tfidf(0.3,1.0,100).pkl
tfidf __init__ took 0.20220279693603516 seconds
dm_dbow __init__ took 0.7522971630096436 seconds
saved_tfidf_models/tfidf(0.3,0.9,100).pkl
tfidf __init__ took 0.3839740753173828 seconds
dm_dbow __init__ took 1.811535120010376 seconds
saved_tfidf_models/tfidf(0.3,0.9,100).pkl
tfidf __init__ took 0.04673266410827637 seconds
dm_dbow __init__ took 0.804490327835083 seconds
saved_tfidf_models/tfidf(0.3,0.9,100).pkl
tfidf __init__ took 0.06372714042663574 seconds
string price: 1.8591981136572546
string price: 1.2885263121198216
string price: 1.0773845281177161
string price: 1.121257804323449
string price: 8.965153219298495
string price: 1.2990684024571935
string price: 9.064884734018875
string price: 2.006564011672051
string price: 2.3335508561398757
string price: 2.3359429007602555
string price: 2.3359429007602555
string price: 2.3359429007602555
string price: 1.2591215623738339
string price: 1.2787003960170784
string price: 6.266887232538808
string price: 1.5002095829715556
experiment starting with estimators=[('union', FeatureUnion(n_jobs=1,
       transformer_list=[('linguistic', Pipeline(memory=None,
     steps=[('selector', CustomDictVectorizer(key='corpus')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=...tory', Pipeline(memory=None,
     steps=[('selector', CustomDictVectorizer(key='price_history'))]))],
       transformer_weights={'linguistic': 1.0, 'price_history': 1.0})), ('clf', XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,
       max_depth=3, min_child_weight=1, missing=None, n_estimators=300,
       n_jobs=1, nthread=4, objective='multi:softmax', random_state=0,
       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
       silent=True, subsample=1))] param_dict={'clf__max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__min_child_weight': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__subsample': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'clf__colsample_bytree': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'clf__learning_rate': array([0.01, 0.05, 0.1 , 0.15, 0.2 , 0.3 , 0.5 , 0.7 ]), 'clf__gamma': array([ 0.01,  0.05,  0.1 ,  0.15,  0.2 ,  0.3 ,  0.5 ,  0.7 ,  2.  ,
        5.  , 10.  ]), 'clf__reg_lambda': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__reg_alpha': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__objective': array(['multi:softmax', 'multi:softprob'], dtype='<U14'), 'union__linguistic__tfidf__max_df': array([0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]), 'union__linguistic__tfidf__min_df': array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45]), 'union__linguistic__tfidf__sublinear_tf': array([False,  True]), 'union__linguistic__tfidf__norm': array(['l1', 'l2', None], dtype=object), 'union__linguistic__tfidf__ngram_range': array([[1, 1],
       [1, 2],
       [1, 3],
       [2, 2],
       [2, 3],
       [3, 3]]), 'union__linguistic__nmf__n_components': array([ 25,  50,  75, 100, 125, 150]), 'union__linguistic__nmf__init': array(['random', 'nndsvd', 'nndsvda'], dtype='<U7'), 'union__linguistic__nmf__beta_loss': array(['frobenius', 'kullback-leibler', 'itakura-saito'], dtype='<U16'), 'union__linguistic__nmf__max_iter': array([ 50, 100, 200, 300, 400, 500]), 'union__linguistic__nmf__alpha': array([ 0. ,  0.1,  0.5,  1. ,  5. , 10. ]), 'union__linguistic__nmf__l1_ratio': array([0. , 0.2, 0.4, 0.6, 0.8, 1. ])}
Top 3 lines
#1: joblib/numpy_pickle.py:108: 3521526.8 KiB
    array = pickle.load(unpickler.file_handle)
#2: python3.6/_strptime.py:576: 2588.8 KiB
    return cls(*args)
#3: doc2vec/utils_v2.py:434: 1294.7 KiB
    dataset['assets'] = np.array(list(assets))
136 other: 1116.3 KiB
Total allocated size: 3526526.6 KiB
66273
66273
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py", line 350, in __call__
    return self.func(*args, **kwargs)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 131, in <listcomp>
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py", line 458, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py", line 248, in fit
    Xt, fit_params = self._fit(X, y, **fit_params)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py", line 213, in _fit
    **fit_params_steps[name])
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py", line 562, in __call__
    return self._cached_call(args, kwargs)[0]
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py", line 510, in _cached_call
    out, metadata = self.call(*args, **kwargs)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py", line 744, in call
    output = self.func(*args, **kwargs)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py", line 581, in _fit_transform_one
    res = transformer.fit_transform(X, y, **fit_params)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py", line 739, in fit_transform
    for name, trans, weight in self._iter())
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 779, in __call__
    while self.dispatch_one_batch(iterator):
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 625, in dispatch_one_batch
    self._dispatch(tasks)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 588, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py", line 111, in apply_async
    result = ImmediateResult(func)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py", line 332, in __init__
    self.results = batch()
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 131, in <listcomp>
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py", line 581, in _fit_transform_one
    res = transformer.fit_transform(X, y, **fit_params)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py", line 283, in fit_transform
    return last_step.fit_transform(Xt, y, **fit_params)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py", line 1235, in fit_transform
    shuffle=self.shuffle)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py", line 976, in non_negative_factorization
    beta_loss = _check_string_param(solver, regularization, beta_loss, init)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py", line 205, in _check_string_param
    ' = %r' % (solver, beta_loss))
ValueError: Invalid beta_loss parameter: solver 'cd' does not handle beta_loss = 'kullback-leibler'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/pool.py", line 119, in worker
    result = (True, func(*args, **kwds))
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py", line 359, in __call__
    raise TransportableException(text, e_type)
sklearn.externals.joblib.my_exceptions.TransportableException: TransportableException
___________________________________________________________________________
ValueError                                         Sat Apr 14 20:46:20 2018
PID: 10453                 Python 3.6.4: /home/ubuntu/miniconda3/bin/python
...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 22088, 22089, 22090]), array([22091, 22092, 22093, ..., 44179, 44180, 44181]), 0, {'clf__colsample_bytree': 0.3, 'clf__gamma': 0.5, 'clf__learning_rate': 0.15, 'clf__max_depth': 7, 'clf__min_child_weight': 8, 'clf__objective': 'multi:softmax', 'clf__reg_alpha': 0.3, 'clf__reg_lambda': 0.1, 'clf__subsample': 0.9, 'union__linguistic__nmf__alpha': 5.0, ...}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 22088, 22089, 22090]), array([22091, 22092, 22093, ..., 44179, 44180, 44181]), 0, {'clf__colsample_bytree': 0.3, 'clf__gamma': 0.5, 'clf__learning_rate': 0.15, 'clf__max_depth': 7, 'clf__min_child_weight': 8, 'clf__objective': 'multi:softmax', 'clf__reg_alpha': 0.3, 'clf__reg_lambda': 0.1, 'clf__subsample': 0.9, 'union__linguistic__nmf__alpha': 5.0, ...})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), X=array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), y=array([0, 0, 0, ..., 0, 1, 1]), scorer={'score': <function _passthrough_scorer>}, train=array([    0,     1,     2, ..., 22088, 22089, 22090]), test=array([22091, 22092, 22093, ..., 44179, 44180, 44181]), verbose=0, parameters={'clf__colsample_bytree': 0.3, 'clf__gamma': 0.5, 'clf__learning_rate': 0.15, 'clf__max_depth': 7, 'clf__min_child_weight': 8, 'clf__objective': 'multi:softmax', 'clf__reg_alpha': 0.3, 'clf__reg_lambda': 0.1, 'clf__subsample': 0.9, 'union__linguistic__nmf__alpha': 5.0, ...}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')
    453 
    454     try:
    455         if y_train is None:
    456             estimator.fit(X_train, **fit_params)
    457         else:
--> 458             estimator.fit(X_train, y_train, **fit_params)
        estimator.fit = <bound method Pipeline.fit of Pipeline(memory='t...       seed=None, silent=True, subsample=0.9))])>
        X_train = array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object)
        y_train = array([0, 0, 0, ..., 1, 1, 1])
        fit_params = {}
    459 
    460     except Exception as e:
    461         # Note fit time as time until error
    462         fit_time = time.time() - start_time

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    243         Returns
    244         -------
    245         self : Pipeline
    246             This estimator
    247         """
--> 248         Xt, fit_params = self._fit(X, y, **fit_params)
        Xt = undefined
        fit_params = {}
        self._fit = <bound method Pipeline._fit of Pipeline(memory='...       seed=None, silent=True, subsample=0.9))])>
        X = array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object)
        y = array([0, 0, 0, ..., 1, 1, 1])
    249         if self._final_estimator is not None:
    250             self._final_estimator.fit(Xt, y, **fit_params)
    251         return self
    252 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _fit(self=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    208                 else:
    209                     cloned_transformer = clone(transformer)
    210                 # Fit or load from cache the current transfomer
    211                 Xt, fitted_transformer = fit_transform_one_cached(
    212                     cloned_transformer, None, Xt, y,
--> 213                     **fit_params_steps[name])
        fit_params_steps = {'clf': {}, 'union': {}}
        name = 'union'
    214                 # Replace the transformer of the step with the fitted
    215                 # transformer. This is necessary when loading the transformer
    216                 # from the cache.
    217                 self.steps[step_idx] = (name, fitted_transformer)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py in __call__(self=MemorizedFunc(func=<function _fit_transform_one at 0x7fad236eeea0>, cachedir='tmp/joblib'), *args=(FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1])), **kwargs={})
    557         return MemorizedResult(self.cachedir, self.func, argument_hash,
    558             metadata=metadata, verbose=self._verbose - 1,
    559             timestamp=self.timestamp)
    560 
    561     def __call__(self, *args, **kwargs):
--> 562         return self._cached_call(args, kwargs)[0]
        self._cached_call = <bound method MemorizedFunc._cached_call of Memo...m_one at 0x7fad236eeea0>, cachedir='tmp/joblib')>
        args = (FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1]))
        kwargs = {}
    563 
    564     def __reduce__(self):
    565         """ We don't store the timestamp when pickling, to avoid the hash
    566             depending from it.

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py in _cached_call(self=MemorizedFunc(func=<function _fit_transform_one at 0x7fad236eeea0>, cachedir='tmp/joblib'), args=(FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1])), kwargs={})
    505             if self._verbose > 10:
    506                 _, name = get_func_name(self.func)
    507                 self.warn('Computing func %s, argument hash %s in '
    508                           'directory %s'
    509                         % (name, argument_hash, output_dir))
--> 510             out, metadata = self.call(*args, **kwargs)
        out = undefined
        metadata = None
        self.call = <bound method MemorizedFunc.call of MemorizedFun...m_one at 0x7fad236eeea0>, cachedir='tmp/joblib')>
        args = (FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1]))
        kwargs = {}
    511             if self.mmap_mode is not None:
    512                 # Memmap the output at the first call to be consistent with
    513                 # later calls
    514                 out = _load_output(output_dir, _get_func_fullname(self.func),

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py in call(self=MemorizedFunc(func=<function _fit_transform_one at 0x7fad236eeea0>, cachedir='tmp/joblib'), *args=(FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1])), **kwargs={})
    739         """
    740         start_time = time.time()
    741         output_dir, _ = self._get_output_dir(*args, **kwargs)
    742         if self._verbose > 0:
    743             print(format_call(self.func, args, kwargs))
--> 744         output = self.func(*args, **kwargs)
        output = undefined
        self.func = <function _fit_transform_one>
        args = (FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1]))
        kwargs = {}
    745         self._persist_output(output, output_dir)
    746         duration = time.time() - start_time
    747         metadata = self._persist_input(output_dir, duration, args, kwargs)
    748 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer=FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), weight=None, X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    576 
    577 
    578 def _fit_transform_one(transformer, weight, X, y,
    579                        **fit_params):
    580     if hasattr(transformer, 'fit_transform'):
--> 581         res = transformer.fit_transform(X, y, **fit_params)
        res = undefined
        transformer.fit_transform = <bound method FeatureUnion.fit_transform of Feat...ights={'linguistic': 1.0, 'price_history': 1.0})>
        X = array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object)
        y = array([0, 0, 0, ..., 1, 1, 1])
        fit_params = {}
    582     else:
    583         res = transformer.fit(X, y, **fit_params).transform(X)
    584     # if we have a weight for this transformer, multiply output
    585     if weight is None:

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit_transform(self=FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    734         """
    735         self._validate_transformers()
    736         result = Parallel(n_jobs=self.n_jobs)(
    737             delayed(_fit_transform_one)(trans, weight, X, y,
    738                                         **fit_params)
--> 739             for name, trans, weight in self._iter())
        self._iter = <bound method FeatureUnion._iter of FeatureUnion...ights={'linguistic': 1.0, 'price_history': 1.0})>
    740 
    741         if not result:
    742             # All transformers are None
    743             return np.zeros((X.shape[0], 0))

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=1), iterable=<generator object FeatureUnion.fit_transform.<locals>.<genexpr>>)
    774         self.n_completed_tasks = 0
    775         try:
    776             # Only set self._iterating to True if at least a batch
    777             # was dispatched. In particular this covers the edge
    778             # case of Parallel used with an exhausted iterator.
--> 779             while self.dispatch_one_batch(iterator):
        self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>
        iterator = <generator object FeatureUnion.fit_transform.<locals>.<genexpr>>
    780                 self._iterating = True
    781             else:
    782                 self._iterating = False
    783 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object FeatureUnion.fit_transform.<locals>.<genexpr>>)
    620             tasks = BatchedCalls(itertools.islice(iterator, batch_size))
    621             if len(tasks) == 0:
    622                 # No more tasks available in the iterator: tell caller to stop.
    623                 return False
    624             else:
--> 625                 self._dispatch(tasks)
        self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>
        tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>
    626                 return True
    627 
    628     def _print(self, msg, msg_args):
    629         """Display the message on stout or stderr depending on verbosity"""

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    583         self.n_dispatched_tasks += len(batch)
    584         self.n_dispatched_batches += 1
    585 
    586         dispatch_timestamp = time.time()
    587         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--> 588         job = self._backend.apply_async(batch, callback=cb)
        job = undefined
        self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
        cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>
    589         self._jobs.append(job)
    590 
    591     def dispatch_next(self):
    592         """Dispatch more data for parallel processing

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)
    106             raise ValueError('n_jobs == 0 in Parallel has no meaning')
    107         return 1
    108 
    109     def apply_async(self, func, callback=None):
    110         """Schedule a func to be run"""
--> 111         result = ImmediateResult(func)
        result = undefined
        func = <sklearn.externals.joblib.parallel.BatchedCalls object>
    112         if callback:
    113             callback(result)
    114         return result
    115 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    327 
    328 class ImmediateResult(object):
    329     def __init__(self, batch):
    330         # Don't delay the application, to avoid keeping the input
    331         # arguments in memory
--> 332         self.results = batch()
        self.results = undefined
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
    333 
    334     def get(self):
    335         return self.results
    336 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_transform_one>, (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1])), {})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_transform_one>
        args = (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1]))
        kwargs = {}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), weight=1.0, X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    576 
    577 
    578 def _fit_transform_one(transformer, weight, X, y,
    579                        **fit_params):
    580     if hasattr(transformer, 'fit_transform'):
--> 581         res = transformer.fit_transform(X, y, **fit_params)
        res = undefined
        transformer.fit_transform = <bound method Pipeline.fit_transform of Pipeline...=False,
  solver='cd', tol=0.0001, verbose=0))])>
        X = array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object)
        y = array([0, 0, 0, ..., 1, 1, 1])
        fit_params = {}
    582     else:
    583         res = transformer.fit(X, y, **fit_params).transform(X)
    584     # if we have a weight for this transformer, multiply output
    585     if weight is None:

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit_transform(self=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    278             Transformed samples
    279         """
    280         last_step = self._final_estimator
    281         Xt, fit_params = self._fit(X, y, **fit_params)
    282         if hasattr(last_step, 'fit_transform'):
--> 283             return last_step.fit_transform(Xt, y, **fit_params)
        last_step.fit_transform = <bound method NMF.fit_transform of NMF(alpha=5.0...fle=False,
  solver='cd', tol=0.0001, verbose=0)>
        Xt = <22091x808 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>
        y = array([0, 0, 0, ..., 1, 1, 1])
        fit_params = {}
    284         elif last_step is None:
    285             return Xt
    286         else:
    287             return last_step.fit(Xt, y, **fit_params).transform(Xt)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in fit_transform(self=NMF(alpha=5.0, beta_loss='kullback-leibler', ini...ffle=False,
  solver='cd', tol=0.0001, verbose=0), X=<22091x808 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, y=array([0, 0, 0, ..., 1, 1, 1]), W=None, H=None)
   1230             X=X, W=W, H=H, n_components=self.n_components, init=self.init,
   1231             update_H=True, solver=self.solver, beta_loss=self.beta_loss,
   1232             tol=self.tol, max_iter=self.max_iter, alpha=self.alpha,
   1233             l1_ratio=self.l1_ratio, regularization='both',
   1234             random_state=self.random_state, verbose=self.verbose,
-> 1235             shuffle=self.shuffle)
        self.shuffle = False
   1236 
   1237         self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,
   1238                                                     square_root=True)
   1239 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in non_negative_factorization(X=<22091x808 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, W=None, H=None, n_components=125, init='random', update_H=True, solver='cd', beta_loss='kullback-leibler', tol=0.0001, max_iter=400, alpha=5.0, l1_ratio=0.6, regularization='both', random_state=None, verbose=0, shuffle=False)
    971     factorization with the beta-divergence. Neural Computation, 23(9).
    972     """
    973 
    974     X = check_array(X, accept_sparse=('csr', 'csc'), dtype=float)
    975     check_non_negative(X, "NMF (input X)")
--> 976     beta_loss = _check_string_param(solver, regularization, beta_loss, init)
        beta_loss = 'kullback-leibler'
        solver = 'cd'
        regularization = 'both'
        init = 'random'
    977 
    978     if safe_min(X) == 0 and beta_loss <= 0:
    979         raise ValueError("When beta_loss <= 0 and X contains zeros, "
    980                          "the solver may diverge. Please add small values to "

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in _check_string_param(solver='cd', regularization='both', beta_loss='kullback-leibler', init='random')
    200 
    201     # 'mu' is the only solver that handles other beta losses than 'frobenius'
    202     if solver != 'mu' and beta_loss not in (2, 'frobenius'):
    203         raise ValueError(
    204             'Invalid beta_loss parameter: solver %r does not handle beta_loss'
--> 205             ' = %r' % (solver, beta_loss))
        solver = 'cd'
        beta_loss = 'kullback-leibler'
    206 
    207     if solver == 'mu' and init == 'nndsvd':
    208         warnings.warn("The multiplicative update ('mu') solver cannot update "
    209                       "zeros present in the initialization, and so leads to "

ValueError: Invalid beta_loss parameter: solver 'cd' does not handle beta_loss = 'kullback-leibler'
___________________________________________________________________________
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 699, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/pool.py", line 644, in get
    raise self._value
sklearn.externals.joblib.my_exceptions.TransportableException: TransportableException
___________________________________________________________________________
ValueError                                         Sat Apr 14 20:46:20 2018
PID: 10453                 Python 3.6.4: /home/ubuntu/miniconda3/bin/python
...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 22088, 22089, 22090]), array([22091, 22092, 22093, ..., 44179, 44180, 44181]), 0, {'clf__colsample_bytree': 0.3, 'clf__gamma': 0.5, 'clf__learning_rate': 0.15, 'clf__max_depth': 7, 'clf__min_child_weight': 8, 'clf__objective': 'multi:softmax', 'clf__reg_alpha': 0.3, 'clf__reg_lambda': 0.1, 'clf__subsample': 0.9, 'union__linguistic__nmf__alpha': 5.0, ...}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 22088, 22089, 22090]), array([22091, 22092, 22093, ..., 44179, 44180, 44181]), 0, {'clf__colsample_bytree': 0.3, 'clf__gamma': 0.5, 'clf__learning_rate': 0.15, 'clf__max_depth': 7, 'clf__min_child_weight': 8, 'clf__objective': 'multi:softmax', 'clf__reg_alpha': 0.3, 'clf__reg_lambda': 0.1, 'clf__subsample': 0.9, 'union__linguistic__nmf__alpha': 5.0, ...})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), X=array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), y=array([0, 0, 0, ..., 0, 1, 1]), scorer={'score': <function _passthrough_scorer>}, train=array([    0,     1,     2, ..., 22088, 22089, 22090]), test=array([22091, 22092, 22093, ..., 44179, 44180, 44181]), verbose=0, parameters={'clf__colsample_bytree': 0.3, 'clf__gamma': 0.5, 'clf__learning_rate': 0.15, 'clf__max_depth': 7, 'clf__min_child_weight': 8, 'clf__objective': 'multi:softmax', 'clf__reg_alpha': 0.3, 'clf__reg_lambda': 0.1, 'clf__subsample': 0.9, 'union__linguistic__nmf__alpha': 5.0, ...}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')
    453 
    454     try:
    455         if y_train is None:
    456             estimator.fit(X_train, **fit_params)
    457         else:
--> 458             estimator.fit(X_train, y_train, **fit_params)
        estimator.fit = <bound method Pipeline.fit of Pipeline(memory='t...       seed=None, silent=True, subsample=0.9))])>
        X_train = array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object)
        y_train = array([0, 0, 0, ..., 1, 1, 1])
        fit_params = {}
    459 
    460     except Exception as e:
    461         # Note fit time as time until error
    462         fit_time = time.time() - start_time

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    243         Returns
    244         -------
    245         self : Pipeline
    246             This estimator
    247         """
--> 248         Xt, fit_params = self._fit(X, y, **fit_params)
        Xt = undefined
        fit_params = {}
        self._fit = <bound method Pipeline._fit of Pipeline(memory='...       seed=None, silent=True, subsample=0.9))])>
        X = array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object)
        y = array([0, 0, 0, ..., 1, 1, 1])
    249         if self._final_estimator is not None:
    250             self._final_estimator.fit(Xt, y, **fit_params)
    251         return self
    252 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _fit(self=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    208                 else:
    209                     cloned_transformer = clone(transformer)
    210                 # Fit or load from cache the current transfomer
    211                 Xt, fitted_transformer = fit_transform_one_cached(
    212                     cloned_transformer, None, Xt, y,
--> 213                     **fit_params_steps[name])
        fit_params_steps = {'clf': {}, 'union': {}}
        name = 'union'
    214                 # Replace the transformer of the step with the fitted
    215                 # transformer. This is necessary when loading the transformer
    216                 # from the cache.
    217                 self.steps[step_idx] = (name, fitted_transformer)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py in __call__(self=MemorizedFunc(func=<function _fit_transform_one at 0x7fad236eeea0>, cachedir='tmp/joblib'), *args=(FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1])), **kwargs={})
    557         return MemorizedResult(self.cachedir, self.func, argument_hash,
    558             metadata=metadata, verbose=self._verbose - 1,
    559             timestamp=self.timestamp)
    560 
    561     def __call__(self, *args, **kwargs):
--> 562         return self._cached_call(args, kwargs)[0]
        self._cached_call = <bound method MemorizedFunc._cached_call of Memo...m_one at 0x7fad236eeea0>, cachedir='tmp/joblib')>
        args = (FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1]))
        kwargs = {}
    563 
    564     def __reduce__(self):
    565         """ We don't store the timestamp when pickling, to avoid the hash
    566             depending from it.

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py in _cached_call(self=MemorizedFunc(func=<function _fit_transform_one at 0x7fad236eeea0>, cachedir='tmp/joblib'), args=(FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1])), kwargs={})
    505             if self._verbose > 10:
    506                 _, name = get_func_name(self.func)
    507                 self.warn('Computing func %s, argument hash %s in '
    508                           'directory %s'
    509                         % (name, argument_hash, output_dir))
--> 510             out, metadata = self.call(*args, **kwargs)
        out = undefined
        metadata = None
        self.call = <bound method MemorizedFunc.call of MemorizedFun...m_one at 0x7fad236eeea0>, cachedir='tmp/joblib')>
        args = (FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1]))
        kwargs = {}
    511             if self.mmap_mode is not None:
    512                 # Memmap the output at the first call to be consistent with
    513                 # later calls
    514                 out = _load_output(output_dir, _get_func_fullname(self.func),

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py in call(self=MemorizedFunc(func=<function _fit_transform_one at 0x7fad236eeea0>, cachedir='tmp/joblib'), *args=(FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1])), **kwargs={})
    739         """
    740         start_time = time.time()
    741         output_dir, _ = self._get_output_dir(*args, **kwargs)
    742         if self._verbose > 0:
    743             print(format_call(self.func, args, kwargs))
--> 744         output = self.func(*args, **kwargs)
        output = undefined
        self.func = <function _fit_transform_one>
        args = (FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1]))
        kwargs = {}
    745         self._persist_output(output, output_dir)
    746         duration = time.time() - start_time
    747         metadata = self._persist_input(output_dir, duration, args, kwargs)
    748 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer=FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), weight=None, X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    576 
    577 
    578 def _fit_transform_one(transformer, weight, X, y,
    579                        **fit_params):
    580     if hasattr(transformer, 'fit_transform'):
--> 581         res = transformer.fit_transform(X, y, **fit_params)
        res = undefined
        transformer.fit_transform = <bound method FeatureUnion.fit_transform of Feat...ights={'linguistic': 1.0, 'price_history': 1.0})>
        X = array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object)
        y = array([0, 0, 0, ..., 1, 1, 1])
        fit_params = {}
    582     else:
    583         res = transformer.fit(X, y, **fit_params).transform(X)
    584     # if we have a weight for this transformer, multiply output
    585     if weight is None:

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit_transform(self=FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    734         """
    735         self._validate_transformers()
    736         result = Parallel(n_jobs=self.n_jobs)(
    737             delayed(_fit_transform_one)(trans, weight, X, y,
    738                                         **fit_params)
--> 739             for name, trans, weight in self._iter())
        self._iter = <bound method FeatureUnion._iter of FeatureUnion...ights={'linguistic': 1.0, 'price_history': 1.0})>
    740 
    741         if not result:
    742             # All transformers are None
    743             return np.zeros((X.shape[0], 0))

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=1), iterable=<generator object FeatureUnion.fit_transform.<locals>.<genexpr>>)
    774         self.n_completed_tasks = 0
    775         try:
    776             # Only set self._iterating to True if at least a batch
    777             # was dispatched. In particular this covers the edge
    778             # case of Parallel used with an exhausted iterator.
--> 779             while self.dispatch_one_batch(iterator):
        self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>
        iterator = <generator object FeatureUnion.fit_transform.<locals>.<genexpr>>
    780                 self._iterating = True
    781             else:
    782                 self._iterating = False
    783 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object FeatureUnion.fit_transform.<locals>.<genexpr>>)
    620             tasks = BatchedCalls(itertools.islice(iterator, batch_size))
    621             if len(tasks) == 0:
    622                 # No more tasks available in the iterator: tell caller to stop.
    623                 return False
    624             else:
--> 625                 self._dispatch(tasks)
        self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>
        tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>
    626                 return True
    627 
    628     def _print(self, msg, msg_args):
    629         """Display the message on stout or stderr depending on verbosity"""

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    583         self.n_dispatched_tasks += len(batch)
    584         self.n_dispatched_batches += 1
    585 
    586         dispatch_timestamp = time.time()
    587         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--> 588         job = self._backend.apply_async(batch, callback=cb)
        job = undefined
        self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
        cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>
    589         self._jobs.append(job)
    590 
    591     def dispatch_next(self):
    592         """Dispatch more data for parallel processing

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)
    106             raise ValueError('n_jobs == 0 in Parallel has no meaning')
    107         return 1
    108 
    109     def apply_async(self, func, callback=None):
    110         """Schedule a func to be run"""
--> 111         result = ImmediateResult(func)
        result = undefined
        func = <sklearn.externals.joblib.parallel.BatchedCalls object>
    112         if callback:
    113             callback(result)
    114         return result
    115 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    327 
    328 class ImmediateResult(object):
    329     def __init__(self, batch):
    330         # Don't delay the application, to avoid keeping the input
    331         # arguments in memory
--> 332         self.results = batch()
        self.results = undefined
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
    333 
    334     def get(self):
    335         return self.results
    336 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_transform_one>, (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1])), {})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_transform_one>
        args = (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1]))
        kwargs = {}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), weight=1.0, X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    576 
    577 
    578 def _fit_transform_one(transformer, weight, X, y,
    579                        **fit_params):
    580     if hasattr(transformer, 'fit_transform'):
--> 581         res = transformer.fit_transform(X, y, **fit_params)
        res = undefined
        transformer.fit_transform = <bound method Pipeline.fit_transform of Pipeline...=False,
  solver='cd', tol=0.0001, verbose=0))])>
        X = array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object)
        y = array([0, 0, 0, ..., 1, 1, 1])
        fit_params = {}
    582     else:
    583         res = transformer.fit(X, y, **fit_params).transform(X)
    584     # if we have a weight for this transformer, multiply output
    585     if weight is None:

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit_transform(self=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    278             Transformed samples
    279         """
    280         last_step = self._final_estimator
    281         Xt, fit_params = self._fit(X, y, **fit_params)
    282         if hasattr(last_step, 'fit_transform'):
--> 283             return last_step.fit_transform(Xt, y, **fit_params)
        last_step.fit_transform = <bound method NMF.fit_transform of NMF(alpha=5.0...fle=False,
  solver='cd', tol=0.0001, verbose=0)>
        Xt = <22091x808 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>
        y = array([0, 0, 0, ..., 1, 1, 1])
        fit_params = {}
    284         elif last_step is None:
    285             return Xt
    286         else:
    287             return last_step.fit(Xt, y, **fit_params).transform(Xt)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in fit_transform(self=NMF(alpha=5.0, beta_loss='kullback-leibler', ini...ffle=False,
  solver='cd', tol=0.0001, verbose=0), X=<22091x808 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, y=array([0, 0, 0, ..., 1, 1, 1]), W=None, H=None)
   1230             X=X, W=W, H=H, n_components=self.n_components, init=self.init,
   1231             update_H=True, solver=self.solver, beta_loss=self.beta_loss,
   1232             tol=self.tol, max_iter=self.max_iter, alpha=self.alpha,
   1233             l1_ratio=self.l1_ratio, regularization='both',
   1234             random_state=self.random_state, verbose=self.verbose,
-> 1235             shuffle=self.shuffle)
        self.shuffle = False
   1236 
   1237         self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,
   1238                                                     square_root=True)
   1239 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in non_negative_factorization(X=<22091x808 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, W=None, H=None, n_components=125, init='random', update_H=True, solver='cd', beta_loss='kullback-leibler', tol=0.0001, max_iter=400, alpha=5.0, l1_ratio=0.6, regularization='both', random_state=None, verbose=0, shuffle=False)
    971     factorization with the beta-divergence. Neural Computation, 23(9).
    972     """
    973 
    974     X = check_array(X, accept_sparse=('csr', 'csc'), dtype=float)
    975     check_non_negative(X, "NMF (input X)")
--> 976     beta_loss = _check_string_param(solver, regularization, beta_loss, init)
        beta_loss = 'kullback-leibler'
        solver = 'cd'
        regularization = 'both'
        init = 'random'
    977 
    978     if safe_min(X) == 0 and beta_loss <= 0:
    979         raise ValueError("When beta_loss <= 0 and X contains zeros, "
    980                          "the solver may diverge. Please add small values to "

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in _check_string_param(solver='cd', regularization='both', beta_loss='kullback-leibler', init='random')
    200 
    201     # 'mu' is the only solver that handles other beta losses than 'frobenius'
    202     if solver != 'mu' and beta_loss not in (2, 'frobenius'):
    203         raise ValueError(
    204             'Invalid beta_loss parameter: solver %r does not handle beta_loss'
--> 205             ' = %r' % (solver, beta_loss))
        solver = 'cd'
        beta_loss = 'kullback-leibler'
    206 
    207     if solver == 'mu' and init == 'nndsvd':
    208         warnings.warn("The multiplicative update ('mu') solver cannot update "
    209                       "zeros present in the initialization, and so leads to "

ValueError: Invalid beta_loss parameter: solver 'cd' does not handle beta_loss = 'kullback-leibler'
___________________________________________________________________________

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "tf_idf_v2.py", line 137, in <module>
    main()
  File "tf_idf_v2.py", line 86, in main
    run_experiment(estimators, param_grid, pickle_path, dataset)
  File "tf_idf_v2.py", line 124, in run_experiment
    grid_search.fit(dataset['X'], dataset['labels']) 
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py", line 639, in fit
    cv.split(X, y, groups)))
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 789, in __call__
    self.retrieve()
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 740, in retrieve
    raise exception
sklearn.externals.joblib.my_exceptions.JoblibValueError: JoblibValueError
___________________________________________________________________________
Multiprocessing exception:
...........................................................................
/home/ubuntu/notebooks/text-analytics-for-accountancy/doc2vec/tf_idf_v2.py in <module>()
    132 
    133     with open(pickle_path, 'wb+') as p:
    134         pickle.dump(grid_search.best_estimator_, p)
    135 
    136 if __name__ == '__main__':
--> 137     main()

...........................................................................
/home/ubuntu/notebooks/text-analytics-for-accountancy/doc2vec/tf_idf_v2.py in main()
     81     key = 'tf_idf'
     82     estimators, param_grid, momentum_only, doc2vec, doctag_only = get_estimators(key)
     83     dataset = get_dataset(label_horizon, subset, momentum_only, doc2vec, doctag_only)
     84     pickle_path = key + '_' + subset + '_' + str(label_horizon) + '_best_estimator.pkl'
     85     
---> 86     run_experiment(estimators, param_grid, pickle_path, dataset)
        estimators = [('union', FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0})), ('clf', XGBClassifier(base_score=0.5, booster='gbtree', ...ht=1, seed=None,
       silent=True, subsample=1))]
        param_grid = {'clf__colsample_bytree': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'clf__gamma': array([ 0.01,  0.05,  0.1 ,  0.15,  0.2 ,  0.3 ,  0.5 ,  0.7 ,  2.  ,
        5.  , 10.  ]), 'clf__learning_rate': array([0.01, 0.05, 0.1 , 0.15, 0.2 , 0.3 , 0.5 , 0.7 ]), 'clf__max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__min_child_weight': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__objective': array(['multi:softmax', 'multi:softprob'], dtype='<U14'), 'clf__reg_alpha': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__reg_lambda': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__subsample': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'union__linguistic__nmf__alpha': array([ 0. ,  0.1,  0.5,  1. ,  5. , 10. ]), ...}
        pickle_path = 'tf_idf_full_1_best_estimator.pkl'
        dataset = {'X': array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), 'assets': array(['GIS', 'APOG', 'OKE', ..., 'ORCL', 'SCHL', 'PBIP'], dtype='<U5'), 'dates': array([datetime.datetime(1994, 1, 11, 0, 0),
   ...time.datetime(2009, 12, 23, 0, 0)], dtype=object), 'labels': array([0, 0, 0, ..., 0, 1, 1])}
     87 
     88 """large local variables to garbage collected when this function returns
     89 before: 8GB of memory used before forking
     90 after: 3.5GB of memory used before forking

...........................................................................
/home/ubuntu/notebooks/text-analytics-for-accountancy/doc2vec/tf_idf_v2.py in run_experiment(estimators=[('union', FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0})), ('clf', XGBClassifier(base_score=0.5, booster='gbtree', ...ht=1, seed=None,
       silent=True, subsample=1))], param_dict={'clf__colsample_bytree': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'clf__gamma': array([ 0.01,  0.05,  0.1 ,  0.15,  0.2 ,  0.3 ,  0.5 ,  0.7 ,  2.  ,
        5.  , 10.  ]), 'clf__learning_rate': array([0.01, 0.05, 0.1 , 0.15, 0.2 , 0.3 , 0.5 , 0.7 ]), 'clf__max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__min_child_weight': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__objective': array(['multi:softmax', 'multi:softprob'], dtype='<U14'), 'clf__reg_alpha': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__reg_lambda': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__subsample': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'union__linguistic__nmf__alpha': array([ 0. ,  0.1,  0.5,  1. ,  5. , 10. ]), ...}, pickle_path='tf_idf_full_1_best_estimator.pkl', dataset={'X': array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), 'assets': array(['GIS', 'APOG', 'OKE', ..., 'ORCL', 'SCHL', 'PBIP'], dtype='<U5'), 'dates': array([datetime.datetime(1994, 1, 11, 0, 0),
   ...time.datetime(2009, 12, 23, 0, 0)], dtype=object), 'labels': array([0, 0, 0, ..., 0, 1, 1])})
    119     
    120     print(len(dataset['X']))
    121     print(len(dataset['labels']))
    122 #    grid_search = GridSearchCV(pipe, param_grid=param_dict, cv=ts_cv)
    123     
--> 124     grid_search.fit(dataset['X'], dataset['labels']) 
        grid_search.fit = <bound method BaseSearchCV.fit of RandomizedSear...urn_train_score='warn', scoring=None, verbose=0)>
        dataset = {'X': array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), 'assets': array(['GIS', 'APOG', 'OKE', ..., 'ORCL', 'SCHL', 'PBIP'], dtype='<U5'), 'dates': array([datetime.datetime(1994, 1, 11, 0, 0),
   ...time.datetime(2009, 12, 23, 0, 0)], dtype=object), 'labels': array([0, 0, 0, ..., 0, 1, 1])}
    125 
    126     end = time.time()
    127     print("Total running time: {}".format(end-start))
    128     print(grid_search.cv_results_)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py in fit(self=RandomizedSearchCV(cv=<generator object TimeSeri...turn_train_score='warn', scoring=None, verbose=0), X=array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), y=array([0, 0, 0, ..., 0, 1, 1]), groups=None, **fit_params={})
    634                                   return_train_score=self.return_train_score,
    635                                   return_n_test_samples=True,
    636                                   return_times=True, return_parameters=False,
    637                                   error_score=self.error_score)
    638           for parameters, (train, test) in product(candidate_params,
--> 639                                                    cv.split(X, y, groups)))
        cv.split = <bound method _CVIterableWrapper.split of _CVIte...1]), array([44182, 44183, ..., 66271, 66272]))])>
        X = array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object)
        y = array([0, 0, 0, ..., 0, 1, 1])
        groups = None
    640 
    641         # if one choose to see train score, "out" will contain train score info
    642         if self.return_train_score:
    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=3), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)
    784             if pre_dispatch == "all" or n_jobs == 1:
    785                 # The iterable was consumed all at once by the above for loop.
    786                 # No need to wait for async callbacks to trigger to
    787                 # consumption.
    788                 self._iterating = False
--> 789             self.retrieve()
        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=3)>
    790             # Make sure that we get a last message telling us we are done
    791             elapsed_time = time.time() - self._start_time
    792             self._print('Done %3i out of %3i | elapsed: %s finished',
    793                         (len(self._output), len(self._output),

---------------------------------------------------------------------------
Sub-process traceback:
---------------------------------------------------------------------------
ValueError                                         Sat Apr 14 20:46:20 2018
PID: 10453                 Python 3.6.4: /home/ubuntu/miniconda3/bin/python
...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 22088, 22089, 22090]), array([22091, 22092, 22093, ..., 44179, 44180, 44181]), 0, {'clf__colsample_bytree': 0.3, 'clf__gamma': 0.5, 'clf__learning_rate': 0.15, 'clf__max_depth': 7, 'clf__min_child_weight': 8, 'clf__objective': 'multi:softmax', 'clf__reg_alpha': 0.3, 'clf__reg_lambda': 0.1, 'clf__subsample': 0.9, 'union__linguistic__nmf__alpha': 5.0, ...}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 22088, 22089, 22090]), array([22091, 22092, 22093, ..., 44179, 44180, 44181]), 0, {'clf__colsample_bytree': 0.3, 'clf__gamma': 0.5, 'clf__learning_rate': 0.15, 'clf__max_depth': 7, 'clf__min_child_weight': 8, 'clf__objective': 'multi:softmax', 'clf__reg_alpha': 0.3, 'clf__reg_lambda': 0.1, 'clf__subsample': 0.9, 'union__linguistic__nmf__alpha': 5.0, ...})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), X=array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), y=array([0, 0, 0, ..., 0, 1, 1]), scorer={'score': <function _passthrough_scorer>}, train=array([    0,     1,     2, ..., 22088, 22089, 22090]), test=array([22091, 22092, 22093, ..., 44179, 44180, 44181]), verbose=0, parameters={'clf__colsample_bytree': 0.3, 'clf__gamma': 0.5, 'clf__learning_rate': 0.15, 'clf__max_depth': 7, 'clf__min_child_weight': 8, 'clf__objective': 'multi:softmax', 'clf__reg_alpha': 0.3, 'clf__reg_lambda': 0.1, 'clf__subsample': 0.9, 'union__linguistic__nmf__alpha': 5.0, ...}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')
    453 
    454     try:
    455         if y_train is None:
    456             estimator.fit(X_train, **fit_params)
    457         else:
--> 458             estimator.fit(X_train, y_train, **fit_params)
        estimator.fit = <bound method Pipeline.fit of Pipeline(memory='t...       seed=None, silent=True, subsample=0.9))])>
        X_train = array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object)
        y_train = array([0, 0, 0, ..., 1, 1, 1])
        fit_params = {}
    459 
    460     except Exception as e:
    461         # Note fit time as time until error
    462         fit_time = time.time() - start_time

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit(self=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    243         Returns
    244         -------
    245         self : Pipeline
    246             This estimator
    247         """
--> 248         Xt, fit_params = self._fit(X, y, **fit_params)
        Xt = undefined
        fit_params = {}
        self._fit = <bound method Pipeline._fit of Pipeline(memory='...       seed=None, silent=True, subsample=0.9))])>
        X = array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object)
        y = array([0, 0, 0, ..., 1, 1, 1])
    249         if self._final_estimator is not None:
    250             self._final_estimator.fit(Xt, y, **fit_params)
    251         return self
    252 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _fit(self=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.9))]), X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    208                 else:
    209                     cloned_transformer = clone(transformer)
    210                 # Fit or load from cache the current transfomer
    211                 Xt, fitted_transformer = fit_transform_one_cached(
    212                     cloned_transformer, None, Xt, y,
--> 213                     **fit_params_steps[name])
        fit_params_steps = {'clf': {}, 'union': {}}
        name = 'union'
    214                 # Replace the transformer of the step with the fitted
    215                 # transformer. This is necessary when loading the transformer
    216                 # from the cache.
    217                 self.steps[step_idx] = (name, fitted_transformer)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py in __call__(self=MemorizedFunc(func=<function _fit_transform_one at 0x7fad236eeea0>, cachedir='tmp/joblib'), *args=(FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1])), **kwargs={})
    557         return MemorizedResult(self.cachedir, self.func, argument_hash,
    558             metadata=metadata, verbose=self._verbose - 1,
    559             timestamp=self.timestamp)
    560 
    561     def __call__(self, *args, **kwargs):
--> 562         return self._cached_call(args, kwargs)[0]
        self._cached_call = <bound method MemorizedFunc._cached_call of Memo...m_one at 0x7fad236eeea0>, cachedir='tmp/joblib')>
        args = (FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1]))
        kwargs = {}
    563 
    564     def __reduce__(self):
    565         """ We don't store the timestamp when pickling, to avoid the hash
    566             depending from it.

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py in _cached_call(self=MemorizedFunc(func=<function _fit_transform_one at 0x7fad236eeea0>, cachedir='tmp/joblib'), args=(FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1])), kwargs={})
    505             if self._verbose > 10:
    506                 _, name = get_func_name(self.func)
    507                 self.warn('Computing func %s, argument hash %s in '
    508                           'directory %s'
    509                         % (name, argument_hash, output_dir))
--> 510             out, metadata = self.call(*args, **kwargs)
        out = undefined
        metadata = None
        self.call = <bound method MemorizedFunc.call of MemorizedFun...m_one at 0x7fad236eeea0>, cachedir='tmp/joblib')>
        args = (FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1]))
        kwargs = {}
    511             if self.mmap_mode is not None:
    512                 # Memmap the output at the first call to be consistent with
    513                 # later calls
    514                 out = _load_output(output_dir, _get_func_fullname(self.func),

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/memory.py in call(self=MemorizedFunc(func=<function _fit_transform_one at 0x7fad236eeea0>, cachedir='tmp/joblib'), *args=(FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1])), **kwargs={})
    739         """
    740         start_time = time.time()
    741         output_dir, _ = self._get_output_dir(*args, **kwargs)
    742         if self._verbose > 0:
    743             print(format_call(self.func, args, kwargs))
--> 744         output = self.func(*args, **kwargs)
        output = undefined
        self.func = <function _fit_transform_one>
        args = (FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), None, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1]))
        kwargs = {}
    745         self._persist_output(output, output_dir)
    746         duration = time.time() - start_time
    747         metadata = self._persist_input(output_dir, duration, args, kwargs)
    748 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer=FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), weight=None, X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    576 
    577 
    578 def _fit_transform_one(transformer, weight, X, y,
    579                        **fit_params):
    580     if hasattr(transformer, 'fit_transform'):
--> 581         res = transformer.fit_transform(X, y, **fit_params)
        res = undefined
        transformer.fit_transform = <bound method FeatureUnion.fit_transform of Feat...ights={'linguistic': 1.0, 'price_history': 1.0})>
        X = array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object)
        y = array([0, 0, 0, ..., 1, 1, 1])
        fit_params = {}
    582     else:
    583         res = transformer.fit(X, y, **fit_params).transform(X)
    584     # if we have a weight for this transformer, multiply output
    585     if weight is None:

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit_transform(self=FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    734         """
    735         self._validate_transformers()
    736         result = Parallel(n_jobs=self.n_jobs)(
    737             delayed(_fit_transform_one)(trans, weight, X, y,
    738                                         **fit_params)
--> 739             for name, trans, weight in self._iter())
        self._iter = <bound method FeatureUnion._iter of FeatureUnion...ights={'linguistic': 1.0, 'price_history': 1.0})>
    740 
    741         if not result:
    742             # All transformers are None
    743             return np.zeros((X.shape[0], 0))

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=1), iterable=<generator object FeatureUnion.fit_transform.<locals>.<genexpr>>)
    774         self.n_completed_tasks = 0
    775         try:
    776             # Only set self._iterating to True if at least a batch
    777             # was dispatched. In particular this covers the edge
    778             # case of Parallel used with an exhausted iterator.
--> 779             while self.dispatch_one_batch(iterator):
        self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>
        iterator = <generator object FeatureUnion.fit_transform.<locals>.<genexpr>>
    780                 self._iterating = True
    781             else:
    782                 self._iterating = False
    783 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object FeatureUnion.fit_transform.<locals>.<genexpr>>)
    620             tasks = BatchedCalls(itertools.islice(iterator, batch_size))
    621             if len(tasks) == 0:
    622                 # No more tasks available in the iterator: tell caller to stop.
    623                 return False
    624             else:
--> 625                 self._dispatch(tasks)
        self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>
        tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>
    626                 return True
    627 
    628     def _print(self, msg, msg_args):
    629         """Display the message on stout or stderr depending on verbosity"""

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    583         self.n_dispatched_tasks += len(batch)
    584         self.n_dispatched_batches += 1
    585 
    586         dispatch_timestamp = time.time()
    587         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--> 588         job = self._backend.apply_async(batch, callback=cb)
        job = undefined
        self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
        cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>
    589         self._jobs.append(job)
    590 
    591     def dispatch_next(self):
    592         """Dispatch more data for parallel processing

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)
    106             raise ValueError('n_jobs == 0 in Parallel has no meaning')
    107         return 1
    108 
    109     def apply_async(self, func, callback=None):
    110         """Schedule a func to be run"""
--> 111         result = ImmediateResult(func)
        result = undefined
        func = <sklearn.externals.joblib.parallel.BatchedCalls object>
    112         if callback:
    113             callback(result)
    114         return result
    115 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    327 
    328 class ImmediateResult(object):
    329     def __init__(self, batch):
    330         # Don't delay the application, to avoid keeping the input
    331         # arguments in memory
--> 332         self.results = batch()
        self.results = undefined
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
    333 
    334     def get(self):
    335         return self.results
    336 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_transform_one>, (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1])), {})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_transform_one>
        args = (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), array([0, 0, 0, ..., 1, 1, 1]))
        kwargs = {}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), weight=1.0, X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    576 
    577 
    578 def _fit_transform_one(transformer, weight, X, y,
    579                        **fit_params):
    580     if hasattr(transformer, 'fit_transform'):
--> 581         res = transformer.fit_transform(X, y, **fit_params)
        res = undefined
        transformer.fit_transform = <bound method Pipeline.fit_transform of Pipeline...=False,
  solver='cd', tol=0.0001, verbose=0))])>
        X = array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object)
        y = array([0, 0, 0, ..., 1, 1, 1])
        fit_params = {}
    582     else:
    583         res = transformer.fit(X, y, **fit_params).transform(X)
    584     # if we have a weight for this transformer, multiply output
    585     if weight is None:

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in fit_transform(self=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), X=array([{'corpus': 'numb conform conform file a d... 0.00387402, -0.06456542])}],
      dtype=object), y=array([0, 0, 0, ..., 1, 1, 1]), **fit_params={})
    278             Transformed samples
    279         """
    280         last_step = self._final_estimator
    281         Xt, fit_params = self._fit(X, y, **fit_params)
    282         if hasattr(last_step, 'fit_transform'):
--> 283             return last_step.fit_transform(Xt, y, **fit_params)
        last_step.fit_transform = <bound method NMF.fit_transform of NMF(alpha=5.0...fle=False,
  solver='cd', tol=0.0001, verbose=0)>
        Xt = <22091x808 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>
        y = array([0, 0, 0, ..., 1, 1, 1])
        fit_params = {}
    284         elif last_step is None:
    285             return Xt
    286         else:
    287             return last_step.fit(Xt, y, **fit_params).transform(Xt)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in fit_transform(self=NMF(alpha=5.0, beta_loss='kullback-leibler', ini...ffle=False,
  solver='cd', tol=0.0001, verbose=0), X=<22091x808 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, y=array([0, 0, 0, ..., 1, 1, 1]), W=None, H=None)
   1230             X=X, W=W, H=H, n_components=self.n_components, init=self.init,
   1231             update_H=True, solver=self.solver, beta_loss=self.beta_loss,
   1232             tol=self.tol, max_iter=self.max_iter, alpha=self.alpha,
   1233             l1_ratio=self.l1_ratio, regularization='both',
   1234             random_state=self.random_state, verbose=self.verbose,
-> 1235             shuffle=self.shuffle)
        self.shuffle = False
   1236 
   1237         self.reconstruction_err_ = _beta_divergence(X, W, H, self.beta_loss,
   1238                                                     square_root=True)
   1239 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in non_negative_factorization(X=<22091x808 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, W=None, H=None, n_components=125, init='random', update_H=True, solver='cd', beta_loss='kullback-leibler', tol=0.0001, max_iter=400, alpha=5.0, l1_ratio=0.6, regularization='both', random_state=None, verbose=0, shuffle=False)
    971     factorization with the beta-divergence. Neural Computation, 23(9).
    972     """
    973 
    974     X = check_array(X, accept_sparse=('csr', 'csc'), dtype=float)
    975     check_non_negative(X, "NMF (input X)")
--> 976     beta_loss = _check_string_param(solver, regularization, beta_loss, init)
        beta_loss = 'kullback-leibler'
        solver = 'cd'
        regularization = 'both'
        init = 'random'
    977 
    978     if safe_min(X) == 0 and beta_loss <= 0:
    979         raise ValueError("When beta_loss <= 0 and X contains zeros, "
    980                          "the solver may diverge. Please add small values to "

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in _check_string_param(solver='cd', regularization='both', beta_loss='kullback-leibler', init='random')
    200 
    201     # 'mu' is the only solver that handles other beta losses than 'frobenius'
    202     if solver != 'mu' and beta_loss not in (2, 'frobenius'):
    203         raise ValueError(
    204             'Invalid beta_loss parameter: solver %r does not handle beta_loss'
--> 205             ' = %r' % (solver, beta_loss))
        solver = 'cd'
        beta_loss = 'kullback-leibler'
    206 
    207     if solver == 'mu' and init == 'nndsvd':
    208         warnings.warn("The multiplicative update ('mu') solver cannot update "
    209                       "zeros present in the initialization, and so leads to "

ValueError: Invalid beta_loss parameter: solver 'cd' does not handle beta_loss = 'kullback-leibler'
___________________________________________________________________________
dm_dbow __init__ took 0.17043638229370117 seconds
dm_dbow __init__ took 0.39522337913513184 seconds
saved_tfidf_models/tfidf(0.3,1.0,100).pkl
tfidf __init__ took 0.05169320106506348 seconds
dm_dbow __init__ took 0.37561559677124023 seconds
saved_tfidf_models/tfidf(0.3,0.9,100).pkl
tfidf __init__ took 0.0459442138671875 seconds
dm_dbow __init__ took 0.37613797187805176 seconds
saved_tfidf_models/tfidf(0.3,0.9,100).pkl
tfidf __init__ took 0.047594308853149414 seconds
dm_dbow __init__ took 0.376737117767334 seconds
saved_tfidf_models/tfidf(0.3,0.9,100).pkl
tfidf __init__ took 0.04692864418029785 seconds
string price: 1.8591981136572546
string price: 1.2885263121198216
string price: 1.0773845281177161
string price: 1.121257804323449
string price: 8.965153219298495
string price: 1.2990684024571935
string price: 9.064884734018875
string price: 2.006564011672051
string price: 2.3335508561398757
string price: 2.3359429007602555
string price: 2.3359429007602555
string price: 2.3359429007602555
string price: 1.2591215623738339
string price: 1.2787003960170784
string price: 6.266887232538808
string price: 1.5002095829715556
experiment starting with estimators=[('union', FeatureUnion(n_jobs=1,
       transformer_list=[('linguistic', Pipeline(memory=None,
     steps=[('selector', CustomDictVectorizer(key='corpus')), ('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',
        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',
        lowercase=True, max_df=...tory', Pipeline(memory=None,
     steps=[('selector', CustomDictVectorizer(key='price_history'))]))],
       transformer_weights={'linguistic': 1.0, 'price_history': 1.0})), ('clf', XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,
       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,
       max_depth=3, min_child_weight=1, missing=None, n_estimators=300,
       n_jobs=1, nthread=4, objective='multi:softmax', random_state=0,
       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,
       silent=True, subsample=1))] param_dict={'clf__max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__min_child_weight': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__subsample': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'clf__colsample_bytree': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'clf__learning_rate': array([0.01, 0.05, 0.1 , 0.15, 0.2 , 0.3 , 0.5 , 0.7 ]), 'clf__gamma': array([ 0.01,  0.05,  0.1 ,  0.15,  0.2 ,  0.3 ,  0.5 ,  0.7 ,  2.  ,
        5.  , 10.  ]), 'clf__reg_lambda': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__reg_alpha': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__objective': array(['multi:softmax', 'multi:softprob'], dtype='<U14'), 'union__linguistic__tfidf__max_df': array([0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  ]), 'union__linguistic__tfidf__min_df': array([0.  , 0.05, 0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45]), 'union__linguistic__tfidf__sublinear_tf': array([False,  True]), 'union__linguistic__tfidf__norm': array(['l1', 'l2', None], dtype=object), 'union__linguistic__tfidf__ngram_range': array([[1, 1],
       [1, 2],
       [1, 3],
       [2, 2],
       [2, 3],
       [3, 3]]), 'union__linguistic__nmf__n_components': array([ 25,  50,  75, 100, 125, 150]), 'union__linguistic__nmf__init': array(['random', 'nndsvd', 'nndsvda'], dtype='<U7'), 'union__linguistic__nmf__max_iter': array([ 50, 100, 200, 300, 400, 500]), 'union__linguistic__nmf__alpha': array([ 0. ,  0.1,  0.5,  1. ,  5. , 10. ]), 'union__linguistic__nmf__l1_ratio': array([0. , 0.2, 0.4, 0.6, 0.8, 1. ])}
Top 3 lines
#1: joblib/numpy_pickle.py:108: 3521526.8 KiB
    array = pickle.load(unpickler.file_handle)
#2: python3.6/_strptime.py:576: 2588.8 KiB
    return cls(*args)
#3: doc2vec/utils_v2.py:434: 1294.7 KiB
    dataset['assets'] = np.array(list(assets))
136 other: 1116.2 KiB
Total allocated size: 3526526.5 KiB
66273
66273
multiprocessing.pool.RemoteTraceback: 
"""
Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py", line 350, in __call__
    return self.func(*args, **kwargs)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 131, in <listcomp>
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py", line 488, in _fit_and_score
    test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py", line 523, in _score
    return _multimetric_score(estimator, X_test, y_test, scorer)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py", line 553, in _multimetric_score
    score = scorer(estimator, X_test, y_test)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/scorer.py", line 244, in _passthrough_scorer
    return estimator.score(*args, **kwargs)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py", line 115, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py", line 486, in score
    Xt = transform.transform(Xt)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py", line 768, in transform
    for name, trans, weight in self._iter())
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 779, in __call__
    while self.dispatch_one_batch(iterator):
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 625, in dispatch_one_batch
    self._dispatch(tasks)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 588, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py", line 111, in apply_async
    result = ImmediateResult(func)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py", line 332, in __init__
    self.results = batch()
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 131, in __call__
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 131, in <listcomp>
    return [func(*args, **kwargs) for func, args, kwargs in self.items]
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py", line 571, in _transform_one
    res = transformer.transform(X)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py", line 426, in _transform
    Xt = transform.transform(Xt)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py", line 1284, in transform
    shuffle=self.shuffle)
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py", line 1002, in non_negative_factorization
    _check_init(H, (n_components, n_features), "NMF (input H)")
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py", line 53, in _check_init
    raise ValueError('Array passed to %s is full of zeros.' % whom)
ValueError: Array passed to NMF (input H) is full of zeros.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/pool.py", line 119, in worker
    result = (True, func(*args, **kwds))
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py", line 359, in __call__
    raise TransportableException(text, e_type)
sklearn.externals.joblib.my_exceptions.TransportableException: TransportableException
___________________________________________________________________________
ValueError                                         Sun Apr 15 06:21:05 2018
PID: 11993                 Python 3.6.4: /home/ubuntu/miniconda3/bin/python
...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 22088, 22089, 22090]), array([22091, 22092, 22093, ..., 44179, 44180, 44181]), 0, {'clf__colsample_bytree': 0.9, 'clf__gamma': 0.15, 'clf__learning_rate': 0.2, 'clf__max_depth': 8, 'clf__min_child_weight': 7, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.5, 'clf__reg_lambda': 0.9, 'clf__subsample': 0.1, 'union__linguistic__nmf__alpha': 10.0, ...}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 22088, 22089, 22090]), array([22091, 22092, 22093, ..., 44179, 44180, 44181]), 0, {'clf__colsample_bytree': 0.9, 'clf__gamma': 0.15, 'clf__learning_rate': 0.2, 'clf__max_depth': 8, 'clf__min_child_weight': 7, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.5, 'clf__reg_lambda': 0.9, 'clf__subsample': 0.1, 'union__linguistic__nmf__alpha': 10.0, ...})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), X=array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), y=array([0, 0, 0, ..., 0, 1, 1]), scorer={'score': <function _passthrough_scorer>}, train=array([    0,     1,     2, ..., 22088, 22089, 22090]), test=array([22091, 22092, 22093, ..., 44179, 44180, 44181]), verbose=0, parameters={'clf__colsample_bytree': 0.9, 'clf__gamma': 0.15, 'clf__learning_rate': 0.2, 'clf__max_depth': 8, 'clf__min_child_weight': 7, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.5, 'clf__reg_lambda': 0.9, 'clf__subsample': 0.1, 'union__linguistic__nmf__alpha': 10.0, ...}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')
    483                              " make sure that it has been spelled correctly.)")
    484 
    485     else:
    486         fit_time = time.time() - start_time
    487         # _score will return dict if is_multimetric is True
--> 488         test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)
        test_scores = {}
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  1,  1, ..., -1,  0,  1])
        scorer = {'score': <function _passthrough_scorer>}
        is_multimetric = True
    489         score_time = time.time() - start_time - fit_time
    490         if return_train_score:
    491             train_scores = _score(estimator, X_train, y_train, scorer,
    492                                   is_multimetric)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), X_test=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y_test=array([ 0,  1,  1, ..., -1,  0,  1]), scorer={'score': <function _passthrough_scorer>}, is_multimetric=True)
    518 
    519     Will return a single float if is_multimetric is False and a dict of floats,
    520     if is_multimetric is True
    521     """
    522     if is_multimetric:
--> 523         return _multimetric_score(estimator, X_test, y_test, scorer)
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  1,  1, ..., -1,  0,  1])
        scorer = {'score': <function _passthrough_scorer>}
    524     else:
    525         if y_test is None:
    526             score = scorer(estimator, X_test)
    527         else:

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _multimetric_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), X_test=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y_test=array([ 0,  1,  1, ..., -1,  0,  1]), scorers={'score': <function _passthrough_scorer>})
    548 
    549     for name, scorer in scorers.items():
    550         if y_test is None:
    551             score = scorer(estimator, X_test)
    552         else:
--> 553             score = scorer(estimator, X_test, y_test)
        score = undefined
        scorer = <function _passthrough_scorer>
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  1,  1, ..., -1,  0,  1])
    554 
    555         if hasattr(score, 'item'):
    556             try:
    557                 # e.g. unwrap memmapped scalars

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/scorer.py in _passthrough_scorer(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), *args=(array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  1,  1, ..., -1,  0,  1])), **kwargs={})
    239     return scorer
    240 
    241 
    242 def _passthrough_scorer(estimator, *args, **kwargs):
    243     """Function that wraps estimator.score"""
--> 244     return estimator.score(*args, **kwargs)
        estimator.score = <function Pipeline.score>
        args = (array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  1,  1, ..., -1,  0,  1]))
        kwargs = {}
    245 
    246 
    247 def check_scoring(estimator, scoring=None, allow_none=False):
    248     """Determine scorer from user options.

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in <lambda>(*args=(array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  1,  1, ..., -1,  0,  1])), **kwargs={})
    110                     break
    111             else:
    112                 attrgetter(self.delegate_names[-1])(obj)
    113 
    114         # lambda, but not partial, allows help() to work with update_wrapper
--> 115         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
        args = (array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  1,  1, ..., -1,  0,  1]))
        kwargs = {}
    116         # update the docstring of the returned function
    117         update_wrapper(out, self.fn)
    118         return out
    119 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in score(self=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y=array([ 0,  1,  1, ..., -1,  0,  1]), sample_weight=None)
    481         score : float
    482         """
    483         Xt = X
    484         for name, transform in self.steps[:-1]:
    485             if transform is not None:
--> 486                 Xt = transform.transform(Xt)
        Xt = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        transform.transform = <bound method FeatureUnion.transform of FeatureU...ights={'linguistic': 1.0, 'price_history': 1.0})>
    487         score_params = {}
    488         if sample_weight is not None:
    489             score_params['sample_weight'] = sample_weight
    490         return self.steps[-1][-1].score(Xt, y, **score_params)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in transform(self=FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    763             hstack of results of transformers. sum_n_components is the
    764             sum of n_components (output dimension) over transformers.
    765         """
    766         Xs = Parallel(n_jobs=self.n_jobs)(
    767             delayed(_transform_one)(trans, weight, X)
--> 768             for name, trans, weight in self._iter())
        self._iter = <bound method FeatureUnion._iter of FeatureUnion...ights={'linguistic': 1.0, 'price_history': 1.0})>
    769         if not Xs:
    770             # All transformers are None
    771             return np.zeros((X.shape[0], 0))
    772         if any(sparse.issparse(f) for f in Xs):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=1), iterable=<generator object FeatureUnion.transform.<locals>.<genexpr>>)
    774         self.n_completed_tasks = 0
    775         try:
    776             # Only set self._iterating to True if at least a batch
    777             # was dispatched. In particular this covers the edge
    778             # case of Parallel used with an exhausted iterator.
--> 779             while self.dispatch_one_batch(iterator):
        self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>
        iterator = <generator object FeatureUnion.transform.<locals>.<genexpr>>
    780                 self._iterating = True
    781             else:
    782                 self._iterating = False
    783 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object FeatureUnion.transform.<locals>.<genexpr>>)
    620             tasks = BatchedCalls(itertools.islice(iterator, batch_size))
    621             if len(tasks) == 0:
    622                 # No more tasks available in the iterator: tell caller to stop.
    623                 return False
    624             else:
--> 625                 self._dispatch(tasks)
        self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>
        tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>
    626                 return True
    627 
    628     def _print(self, msg, msg_args):
    629         """Display the message on stout or stderr depending on verbosity"""

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    583         self.n_dispatched_tasks += len(batch)
    584         self.n_dispatched_batches += 1
    585 
    586         dispatch_timestamp = time.time()
    587         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--> 588         job = self._backend.apply_async(batch, callback=cb)
        job = undefined
        self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
        cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>
    589         self._jobs.append(job)
    590 
    591     def dispatch_next(self):
    592         """Dispatch more data for parallel processing

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)
    106             raise ValueError('n_jobs == 0 in Parallel has no meaning')
    107         return 1
    108 
    109     def apply_async(self, func, callback=None):
    110         """Schedule a func to be run"""
--> 111         result = ImmediateResult(func)
        result = undefined
        func = <sklearn.externals.joblib.parallel.BatchedCalls object>
    112         if callback:
    113             callback(result)
    114         return result
    115 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    327 
    328 class ImmediateResult(object):
    329     def __init__(self, batch):
    330         # Don't delay the application, to avoid keeping the input
    331         # arguments in memory
--> 332         self.results = batch()
        self.results = undefined
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
    333 
    334     def get(self):
    335         return self.results
    336 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _transform_one>, (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)), {})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _transform_one>
        args = (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
        kwargs = {}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _transform_one(transformer=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), weight=1.0, X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    566 def _fit_one_transformer(transformer, X, y):
    567     return transformer.fit(X, y)
    568 
    569 
    570 def _transform_one(transformer, weight, X):
--> 571     res = transformer.transform(X)
        res = undefined
        transformer.transform = <bound method Pipeline._transform of Pipeline(me...=False,
  solver='cd', tol=0.0001, verbose=0))])>
        X = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
    572     # if we have a weight for this transformer, multiply output
    573     if weight is None:
    574         return res
    575     return res * weight

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _transform(self=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    421 
    422     def _transform(self, X):
    423         Xt = X
    424         for name, transform in self.steps:
    425             if transform is not None:
--> 426                 Xt = transform.transform(Xt)
        Xt = <22091x462 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>
        transform.transform = <bound method NMF.transform of NMF(alpha=10.0, b...fle=False,
  solver='cd', tol=0.0001, verbose=0)>
    427         return Xt
    428 
    429     @property
    430     def inverse_transform(self):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in transform(self=NMF(alpha=10.0, beta_loss='frobenius', init='nnd...ffle=False,
  solver='cd', tol=0.0001, verbose=0), X=<22091x462 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>)
   1279             X=X, W=None, H=self.components_, n_components=self.n_components_,
   1280             init=self.init, update_H=False, solver=self.solver,
   1281             beta_loss=self.beta_loss, tol=self.tol, max_iter=self.max_iter,
   1282             alpha=self.alpha, l1_ratio=self.l1_ratio, regularization='both',
   1283             random_state=self.random_state, verbose=self.verbose,
-> 1284             shuffle=self.shuffle)
        self.shuffle = False
   1285 
   1286         return W
   1287 
   1288     def inverse_transform(self, W):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in non_negative_factorization(X=<22091x462 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, W=None, H=array([[0., 0., 0., ..., 0., 0., 0.],
       [0...., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]]), n_components=150, init='nndsvda', update_H=False, solver='cd', beta_loss=2, tol=0.0001, max_iter=50, alpha=10.0, l1_ratio=0.8, regularization='both', random_state=None, verbose=0, shuffle=False)
    997     # check W and H, or initialize them
    998     if init == 'custom' and update_H:
    999         _check_init(H, (n_components, n_features), "NMF (input H)")
   1000         _check_init(W, (n_samples, n_components), "NMF (input W)")
   1001     elif not update_H:
-> 1002         _check_init(H, (n_components, n_features), "NMF (input H)")
        H = array([[0., 0., 0., ..., 0., 0., 0.],
       [0...., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]])
        n_components = 150
        n_features = 462
   1003         # 'mu' solver should not be initialized by zeros
   1004         if solver == 'mu':
   1005             avg = np.sqrt(X.mean() / n_components)
   1006             W = avg * np.ones((n_samples, n_components))

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in _check_init(A=array([[0., 0., 0., ..., 0., 0., 0.],
       [0...., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]]), shape=(150, 462), whom='NMF (input H)')
     48     if np.shape(A) != shape:
     49         raise ValueError('Array with wrong shape passed to %s. Expected %s, '
     50                          'but got %s ' % (whom, shape, np.shape(A)))
     51     check_non_negative(A, whom)
     52     if np.max(A) == 0:
---> 53         raise ValueError('Array passed to %s is full of zeros.' % whom)
        whom = 'NMF (input H)'
     54 
     55 
     56 def _beta_divergence(X, W, H, beta, square_root=False):
     57     """Compute the beta-divergence of X and dot(W, H).

ValueError: Array passed to NMF (input H) is full of zeros.
___________________________________________________________________________
"""

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 699, in retrieve
    self._output.extend(job.get(timeout=self.timeout))
  File "/home/ubuntu/miniconda3/lib/python3.6/multiprocessing/pool.py", line 644, in get
    raise self._value
sklearn.externals.joblib.my_exceptions.TransportableException: TransportableException
___________________________________________________________________________
ValueError                                         Sun Apr 15 06:21:05 2018
PID: 11993                 Python 3.6.4: /home/ubuntu/miniconda3/bin/python
...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 22088, 22089, 22090]), array([22091, 22092, 22093, ..., 44179, 44180, 44181]), 0, {'clf__colsample_bytree': 0.9, 'clf__gamma': 0.15, 'clf__learning_rate': 0.2, 'clf__max_depth': 8, 'clf__min_child_weight': 7, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.5, 'clf__reg_lambda': 0.9, 'clf__subsample': 0.1, 'union__linguistic__nmf__alpha': 10.0, ...}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 22088, 22089, 22090]), array([22091, 22092, 22093, ..., 44179, 44180, 44181]), 0, {'clf__colsample_bytree': 0.9, 'clf__gamma': 0.15, 'clf__learning_rate': 0.2, 'clf__max_depth': 8, 'clf__min_child_weight': 7, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.5, 'clf__reg_lambda': 0.9, 'clf__subsample': 0.1, 'union__linguistic__nmf__alpha': 10.0, ...})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), X=array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), y=array([0, 0, 0, ..., 0, 1, 1]), scorer={'score': <function _passthrough_scorer>}, train=array([    0,     1,     2, ..., 22088, 22089, 22090]), test=array([22091, 22092, 22093, ..., 44179, 44180, 44181]), verbose=0, parameters={'clf__colsample_bytree': 0.9, 'clf__gamma': 0.15, 'clf__learning_rate': 0.2, 'clf__max_depth': 8, 'clf__min_child_weight': 7, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.5, 'clf__reg_lambda': 0.9, 'clf__subsample': 0.1, 'union__linguistic__nmf__alpha': 10.0, ...}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')
    483                              " make sure that it has been spelled correctly.)")
    484 
    485     else:
    486         fit_time = time.time() - start_time
    487         # _score will return dict if is_multimetric is True
--> 488         test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)
        test_scores = {}
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  1,  1, ..., -1,  0,  1])
        scorer = {'score': <function _passthrough_scorer>}
        is_multimetric = True
    489         score_time = time.time() - start_time - fit_time
    490         if return_train_score:
    491             train_scores = _score(estimator, X_train, y_train, scorer,
    492                                   is_multimetric)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), X_test=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y_test=array([ 0,  1,  1, ..., -1,  0,  1]), scorer={'score': <function _passthrough_scorer>}, is_multimetric=True)
    518 
    519     Will return a single float if is_multimetric is False and a dict of floats,
    520     if is_multimetric is True
    521     """
    522     if is_multimetric:
--> 523         return _multimetric_score(estimator, X_test, y_test, scorer)
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  1,  1, ..., -1,  0,  1])
        scorer = {'score': <function _passthrough_scorer>}
    524     else:
    525         if y_test is None:
    526             score = scorer(estimator, X_test)
    527         else:

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _multimetric_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), X_test=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y_test=array([ 0,  1,  1, ..., -1,  0,  1]), scorers={'score': <function _passthrough_scorer>})
    548 
    549     for name, scorer in scorers.items():
    550         if y_test is None:
    551             score = scorer(estimator, X_test)
    552         else:
--> 553             score = scorer(estimator, X_test, y_test)
        score = undefined
        scorer = <function _passthrough_scorer>
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  1,  1, ..., -1,  0,  1])
    554 
    555         if hasattr(score, 'item'):
    556             try:
    557                 # e.g. unwrap memmapped scalars

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/scorer.py in _passthrough_scorer(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), *args=(array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  1,  1, ..., -1,  0,  1])), **kwargs={})
    239     return scorer
    240 
    241 
    242 def _passthrough_scorer(estimator, *args, **kwargs):
    243     """Function that wraps estimator.score"""
--> 244     return estimator.score(*args, **kwargs)
        estimator.score = <function Pipeline.score>
        args = (array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  1,  1, ..., -1,  0,  1]))
        kwargs = {}
    245 
    246 
    247 def check_scoring(estimator, scoring=None, allow_none=False):
    248     """Determine scorer from user options.

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in <lambda>(*args=(array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  1,  1, ..., -1,  0,  1])), **kwargs={})
    110                     break
    111             else:
    112                 attrgetter(self.delegate_names[-1])(obj)
    113 
    114         # lambda, but not partial, allows help() to work with update_wrapper
--> 115         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
        args = (array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  1,  1, ..., -1,  0,  1]))
        kwargs = {}
    116         # update the docstring of the returned function
    117         update_wrapper(out, self.fn)
    118         return out
    119 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in score(self=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y=array([ 0,  1,  1, ..., -1,  0,  1]), sample_weight=None)
    481         score : float
    482         """
    483         Xt = X
    484         for name, transform in self.steps[:-1]:
    485             if transform is not None:
--> 486                 Xt = transform.transform(Xt)
        Xt = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        transform.transform = <bound method FeatureUnion.transform of FeatureU...ights={'linguistic': 1.0, 'price_history': 1.0})>
    487         score_params = {}
    488         if sample_weight is not None:
    489             score_params['sample_weight'] = sample_weight
    490         return self.steps[-1][-1].score(Xt, y, **score_params)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in transform(self=FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    763             hstack of results of transformers. sum_n_components is the
    764             sum of n_components (output dimension) over transformers.
    765         """
    766         Xs = Parallel(n_jobs=self.n_jobs)(
    767             delayed(_transform_one)(trans, weight, X)
--> 768             for name, trans, weight in self._iter())
        self._iter = <bound method FeatureUnion._iter of FeatureUnion...ights={'linguistic': 1.0, 'price_history': 1.0})>
    769         if not Xs:
    770             # All transformers are None
    771             return np.zeros((X.shape[0], 0))
    772         if any(sparse.issparse(f) for f in Xs):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=1), iterable=<generator object FeatureUnion.transform.<locals>.<genexpr>>)
    774         self.n_completed_tasks = 0
    775         try:
    776             # Only set self._iterating to True if at least a batch
    777             # was dispatched. In particular this covers the edge
    778             # case of Parallel used with an exhausted iterator.
--> 779             while self.dispatch_one_batch(iterator):
        self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>
        iterator = <generator object FeatureUnion.transform.<locals>.<genexpr>>
    780                 self._iterating = True
    781             else:
    782                 self._iterating = False
    783 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object FeatureUnion.transform.<locals>.<genexpr>>)
    620             tasks = BatchedCalls(itertools.islice(iterator, batch_size))
    621             if len(tasks) == 0:
    622                 # No more tasks available in the iterator: tell caller to stop.
    623                 return False
    624             else:
--> 625                 self._dispatch(tasks)
        self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>
        tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>
    626                 return True
    627 
    628     def _print(self, msg, msg_args):
    629         """Display the message on stout or stderr depending on verbosity"""

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    583         self.n_dispatched_tasks += len(batch)
    584         self.n_dispatched_batches += 1
    585 
    586         dispatch_timestamp = time.time()
    587         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--> 588         job = self._backend.apply_async(batch, callback=cb)
        job = undefined
        self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
        cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>
    589         self._jobs.append(job)
    590 
    591     def dispatch_next(self):
    592         """Dispatch more data for parallel processing

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)
    106             raise ValueError('n_jobs == 0 in Parallel has no meaning')
    107         return 1
    108 
    109     def apply_async(self, func, callback=None):
    110         """Schedule a func to be run"""
--> 111         result = ImmediateResult(func)
        result = undefined
        func = <sklearn.externals.joblib.parallel.BatchedCalls object>
    112         if callback:
    113             callback(result)
    114         return result
    115 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    327 
    328 class ImmediateResult(object):
    329     def __init__(self, batch):
    330         # Don't delay the application, to avoid keeping the input
    331         # arguments in memory
--> 332         self.results = batch()
        self.results = undefined
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
    333 
    334     def get(self):
    335         return self.results
    336 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _transform_one>, (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)), {})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _transform_one>
        args = (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
        kwargs = {}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _transform_one(transformer=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), weight=1.0, X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    566 def _fit_one_transformer(transformer, X, y):
    567     return transformer.fit(X, y)
    568 
    569 
    570 def _transform_one(transformer, weight, X):
--> 571     res = transformer.transform(X)
        res = undefined
        transformer.transform = <bound method Pipeline._transform of Pipeline(me...=False,
  solver='cd', tol=0.0001, verbose=0))])>
        X = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
    572     # if we have a weight for this transformer, multiply output
    573     if weight is None:
    574         return res
    575     return res * weight

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _transform(self=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    421 
    422     def _transform(self, X):
    423         Xt = X
    424         for name, transform in self.steps:
    425             if transform is not None:
--> 426                 Xt = transform.transform(Xt)
        Xt = <22091x462 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>
        transform.transform = <bound method NMF.transform of NMF(alpha=10.0, b...fle=False,
  solver='cd', tol=0.0001, verbose=0)>
    427         return Xt
    428 
    429     @property
    430     def inverse_transform(self):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in transform(self=NMF(alpha=10.0, beta_loss='frobenius', init='nnd...ffle=False,
  solver='cd', tol=0.0001, verbose=0), X=<22091x462 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>)
   1279             X=X, W=None, H=self.components_, n_components=self.n_components_,
   1280             init=self.init, update_H=False, solver=self.solver,
   1281             beta_loss=self.beta_loss, tol=self.tol, max_iter=self.max_iter,
   1282             alpha=self.alpha, l1_ratio=self.l1_ratio, regularization='both',
   1283             random_state=self.random_state, verbose=self.verbose,
-> 1284             shuffle=self.shuffle)
        self.shuffle = False
   1285 
   1286         return W
   1287 
   1288     def inverse_transform(self, W):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in non_negative_factorization(X=<22091x462 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, W=None, H=array([[0., 0., 0., ..., 0., 0., 0.],
       [0...., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]]), n_components=150, init='nndsvda', update_H=False, solver='cd', beta_loss=2, tol=0.0001, max_iter=50, alpha=10.0, l1_ratio=0.8, regularization='both', random_state=None, verbose=0, shuffle=False)
    997     # check W and H, or initialize them
    998     if init == 'custom' and update_H:
    999         _check_init(H, (n_components, n_features), "NMF (input H)")
   1000         _check_init(W, (n_samples, n_components), "NMF (input W)")
   1001     elif not update_H:
-> 1002         _check_init(H, (n_components, n_features), "NMF (input H)")
        H = array([[0., 0., 0., ..., 0., 0., 0.],
       [0...., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]])
        n_components = 150
        n_features = 462
   1003         # 'mu' solver should not be initialized by zeros
   1004         if solver == 'mu':
   1005             avg = np.sqrt(X.mean() / n_components)
   1006             W = avg * np.ones((n_samples, n_components))

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in _check_init(A=array([[0., 0., 0., ..., 0., 0., 0.],
       [0...., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]]), shape=(150, 462), whom='NMF (input H)')
     48     if np.shape(A) != shape:
     49         raise ValueError('Array with wrong shape passed to %s. Expected %s, '
     50                          'but got %s ' % (whom, shape, np.shape(A)))
     51     check_non_negative(A, whom)
     52     if np.max(A) == 0:
---> 53         raise ValueError('Array passed to %s is full of zeros.' % whom)
        whom = 'NMF (input H)'
     54 
     55 
     56 def _beta_divergence(X, W, H, beta, square_root=False):
     57     """Compute the beta-divergence of X and dot(W, H).

ValueError: Array passed to NMF (input H) is full of zeros.
___________________________________________________________________________

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "tf_idf_v2.py", line 137, in <module>
    main()
  File "tf_idf_v2.py", line 86, in main
    run_experiment(estimators, param_grid, pickle_path, dataset)
  File "tf_idf_v2.py", line 124, in run_experiment
    grid_search.fit(dataset['X'], dataset['labels']) 
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py", line 639, in fit
    cv.split(X, y, groups)))
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 789, in __call__
    self.retrieve()
  File "/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py", line 740, in retrieve
    raise exception
sklearn.externals.joblib.my_exceptions.JoblibValueError: JoblibValueError
___________________________________________________________________________
Multiprocessing exception:
...........................................................................
/home/ubuntu/notebooks/text-analytics-for-accountancy/doc2vec/tf_idf_v2.py in <module>()
    132 
    133     with open(pickle_path, 'wb+') as p:
    134         pickle.dump(grid_search.best_estimator_, p)
    135 
    136 if __name__ == '__main__':
--> 137     main()

...........................................................................
/home/ubuntu/notebooks/text-analytics-for-accountancy/doc2vec/tf_idf_v2.py in main()
     81     key = 'tf_idf'
     82     estimators, param_grid, momentum_only, doc2vec, doctag_only = get_estimators(key)
     83     dataset = get_dataset(label_horizon, subset, momentum_only, doc2vec, doctag_only)
     84     pickle_path = key + '_' + subset + '_' + str(label_horizon) + '_best_estimator.pkl'
     85     
---> 86     run_experiment(estimators, param_grid, pickle_path, dataset)
        estimators = [('union', FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0})), ('clf', XGBClassifier(base_score=0.5, booster='gbtree', ...ht=1, seed=None,
       silent=True, subsample=1))]
        param_grid = {'clf__colsample_bytree': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'clf__gamma': array([ 0.01,  0.05,  0.1 ,  0.15,  0.2 ,  0.3 ,  0.5 ,  0.7 ,  2.  ,
        5.  , 10.  ]), 'clf__learning_rate': array([0.01, 0.05, 0.1 , 0.15, 0.2 , 0.3 , 0.5 , 0.7 ]), 'clf__max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__min_child_weight': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__objective': array(['multi:softmax', 'multi:softprob'], dtype='<U14'), 'clf__reg_alpha': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__reg_lambda': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__subsample': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'union__linguistic__nmf__alpha': array([ 0. ,  0.1,  0.5,  1. ,  5. , 10. ]), ...}
        pickle_path = 'tf_idf_full_1_best_estimator.pkl'
        dataset = {'X': array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), 'assets': array(['GIS', 'APOG', 'OKE', ..., 'ORCL', 'SCHL', 'PBIP'], dtype='<U5'), 'dates': array([datetime.datetime(1994, 1, 11, 0, 0),
   ...time.datetime(2009, 12, 23, 0, 0)], dtype=object), 'labels': array([0, 0, 0, ..., 0, 1, 1])}
     87 
     88 """large local variables to garbage collected when this function returns
     89 before: 8GB of memory used before forking
     90 after: 3.5GB of memory used before forking

...........................................................................
/home/ubuntu/notebooks/text-analytics-for-accountancy/doc2vec/tf_idf_v2.py in run_experiment(estimators=[('union', FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0})), ('clf', XGBClassifier(base_score=0.5, booster='gbtree', ...ht=1, seed=None,
       silent=True, subsample=1))], param_dict={'clf__colsample_bytree': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'clf__gamma': array([ 0.01,  0.05,  0.1 ,  0.15,  0.2 ,  0.3 ,  0.5 ,  0.7 ,  2.  ,
        5.  , 10.  ]), 'clf__learning_rate': array([0.01, 0.05, 0.1 , 0.15, 0.2 , 0.3 , 0.5 , 0.7 ]), 'clf__max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__min_child_weight': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'clf__objective': array(['multi:softmax', 'multi:softprob'], dtype='<U14'), 'clf__reg_alpha': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__reg_lambda': array([0. , 0.1, 0.3, 0.5, 0.7, 0.9, 1. ]), 'clf__subsample': array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]), 'union__linguistic__nmf__alpha': array([ 0. ,  0.1,  0.5,  1. ,  5. , 10. ]), ...}, pickle_path='tf_idf_full_1_best_estimator.pkl', dataset={'X': array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), 'assets': array(['GIS', 'APOG', 'OKE', ..., 'ORCL', 'SCHL', 'PBIP'], dtype='<U5'), 'dates': array([datetime.datetime(1994, 1, 11, 0, 0),
   ...time.datetime(2009, 12, 23, 0, 0)], dtype=object), 'labels': array([0, 0, 0, ..., 0, 1, 1])})
    119     
    120     print(len(dataset['X']))
    121     print(len(dataset['labels']))
    122 #    grid_search = GridSearchCV(pipe, param_grid=param_dict, cv=ts_cv)
    123     
--> 124     grid_search.fit(dataset['X'], dataset['labels']) 
        grid_search.fit = <bound method BaseSearchCV.fit of RandomizedSear...urn_train_score='warn', scoring=None, verbose=0)>
        dataset = {'X': array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), 'assets': array(['GIS', 'APOG', 'OKE', ..., 'ORCL', 'SCHL', 'PBIP'], dtype='<U5'), 'dates': array([datetime.datetime(1994, 1, 11, 0, 0),
   ...time.datetime(2009, 12, 23, 0, 0)], dtype=object), 'labels': array([0, 0, 0, ..., 0, 1, 1])}
    125 
    126     end = time.time()
    127     print("Total running time: {}".format(end-start))
    128     print(grid_search.cv_results_)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_search.py in fit(self=RandomizedSearchCV(cv=<generator object TimeSeri...turn_train_score='warn', scoring=None, verbose=0), X=array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), y=array([0, 0, 0, ..., 0, 1, 1]), groups=None, **fit_params={})
    634                                   return_train_score=self.return_train_score,
    635                                   return_n_test_samples=True,
    636                                   return_times=True, return_parameters=False,
    637                                   error_score=self.error_score)
    638           for parameters, (train, test) in product(candidate_params,
--> 639                                                    cv.split(X, y, groups)))
        cv.split = <bound method _CVIterableWrapper.split of _CVIte...1]), array([44182, 44183, ..., 66271, 66272]))])>
        X = array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object)
        y = array([0, 0, 0, ..., 0, 1, 1])
        groups = None
    640 
    641         # if one choose to see train score, "out" will contain train score info
    642         if self.return_train_score:
    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=3), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)
    784             if pre_dispatch == "all" or n_jobs == 1:
    785                 # The iterable was consumed all at once by the above for loop.
    786                 # No need to wait for async callbacks to trigger to
    787                 # consumption.
    788                 self._iterating = False
--> 789             self.retrieve()
        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=3)>
    790             # Make sure that we get a last message telling us we are done
    791             elapsed_time = time.time() - self._start_time
    792             self._print('Done %3i out of %3i | elapsed: %s finished',
    793                         (len(self._output), len(self._output),

---------------------------------------------------------------------------
Sub-process traceback:
---------------------------------------------------------------------------
ValueError                                         Sun Apr 15 06:21:05 2018
PID: 11993                 Python 3.6.4: /home/ubuntu/miniconda3/bin/python
...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _fit_and_score>, (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 22088, 22089, 22090]), array([22091, 22092, 22093, ..., 44179, 44180, 44181]), 0, {'clf__colsample_bytree': 0.9, 'clf__gamma': 0.15, 'clf__learning_rate': 0.2, 'clf__max_depth': 8, 'clf__min_child_weight': 7, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.5, 'clf__reg_lambda': 0.9, 'clf__subsample': 0.1, 'union__linguistic__nmf__alpha': 10.0, ...}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _fit_and_score>
        args = (Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), array([0, 0, 0, ..., 0, 1, 1]), {'score': <function _passthrough_scorer>}, array([    0,     1,     2, ..., 22088, 22089, 22090]), array([22091, 22092, 22093, ..., 44179, 44180, 44181]), 0, {'clf__colsample_bytree': 0.9, 'clf__gamma': 0.15, 'clf__learning_rate': 0.2, 'clf__max_depth': 8, 'clf__min_child_weight': 7, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.5, 'clf__reg_lambda': 0.9, 'clf__subsample': 0.1, 'union__linguistic__nmf__alpha': 10.0, ...})
        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), X=array([{'corpus': 'numb conform conform file a d...-0.03322005,  0.03569484])}],
      dtype=object), y=array([0, 0, 0, ..., 0, 1, 1]), scorer={'score': <function _passthrough_scorer>}, train=array([    0,     1,     2, ..., 22088, 22089, 22090]), test=array([22091, 22092, 22093, ..., 44179, 44180, 44181]), verbose=0, parameters={'clf__colsample_bytree': 0.9, 'clf__gamma': 0.15, 'clf__learning_rate': 0.2, 'clf__max_depth': 8, 'clf__min_child_weight': 7, 'clf__objective': 'multi:softprob', 'clf__reg_alpha': 0.5, 'clf__reg_lambda': 0.9, 'clf__subsample': 0.1, 'union__linguistic__nmf__alpha': 10.0, ...}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')
    483                              " make sure that it has been spelled correctly.)")
    484 
    485     else:
    486         fit_time = time.time() - start_time
    487         # _score will return dict if is_multimetric is True
--> 488         test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)
        test_scores = {}
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  1,  1, ..., -1,  0,  1])
        scorer = {'score': <function _passthrough_scorer>}
        is_multimetric = True
    489         score_time = time.time() - start_time - fit_time
    490         if return_train_score:
    491             train_scores = _score(estimator, X_train, y_train, scorer,
    492                                   is_multimetric)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), X_test=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y_test=array([ 0,  1,  1, ..., -1,  0,  1]), scorer={'score': <function _passthrough_scorer>}, is_multimetric=True)
    518 
    519     Will return a single float if is_multimetric is False and a dict of floats,
    520     if is_multimetric is True
    521     """
    522     if is_multimetric:
--> 523         return _multimetric_score(estimator, X_test, y_test, scorer)
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  1,  1, ..., -1,  0,  1])
        scorer = {'score': <function _passthrough_scorer>}
    524     else:
    525         if y_test is None:
    526             score = scorer(estimator, X_test)
    527         else:

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _multimetric_score(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), X_test=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y_test=array([ 0,  1,  1, ..., -1,  0,  1]), scorers={'score': <function _passthrough_scorer>})
    548 
    549     for name, scorer in scorers.items():
    550         if y_test is None:
    551             score = scorer(estimator, X_test)
    552         else:
--> 553             score = scorer(estimator, X_test, y_test)
        score = undefined
        scorer = <function _passthrough_scorer>
        estimator = Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))])
        X_test = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        y_test = array([ 0,  1,  1, ..., -1,  0,  1])
    554 
    555         if hasattr(score, 'item'):
    556             try:
    557                 # e.g. unwrap memmapped scalars

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/metrics/scorer.py in _passthrough_scorer(estimator=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), *args=(array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  1,  1, ..., -1,  0,  1])), **kwargs={})
    239     return scorer
    240 
    241 
    242 def _passthrough_scorer(estimator, *args, **kwargs):
    243     """Function that wraps estimator.score"""
--> 244     return estimator.score(*args, **kwargs)
        estimator.score = <function Pipeline.score>
        args = (array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  1,  1, ..., -1,  0,  1]))
        kwargs = {}
    245 
    246 
    247 def check_scoring(estimator, scoring=None, allow_none=False):
    248     """Determine scorer from user options.

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/utils/metaestimators.py in <lambda>(*args=(array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  1,  1, ..., -1,  0,  1])), **kwargs={})
    110                     break
    111             else:
    112                 attrgetter(self.delegate_names[-1])(obj)
    113 
    114         # lambda, but not partial, allows help() to work with update_wrapper
--> 115         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
        args = (array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), array([ 0,  1,  1, ..., -1,  0,  1]))
        kwargs = {}
    116         # update the docstring of the returned function
    117         update_wrapper(out, self.fn)
    118         return out
    119 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in score(self=Pipeline(memory='tmp',
     steps=[('union', Fea...
       seed=None, silent=True, subsample=0.1))]), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object), y=array([ 0,  1,  1, ..., -1,  0,  1]), sample_weight=None)
    481         score : float
    482         """
    483         Xt = X
    484         for name, transform in self.steps[:-1]:
    485             if transform is not None:
--> 486                 Xt = transform.transform(Xt)
        Xt = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
        transform.transform = <bound method FeatureUnion.transform of FeatureU...ights={'linguistic': 1.0, 'price_history': 1.0})>
    487         score_params = {}
    488         if sample_weight is not None:
    489             score_params['sample_weight'] = sample_weight
    490         return self.steps[-1][-1].score(Xt, y, **score_params)

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in transform(self=FeatureUnion(n_jobs=1,
       transformer_list=[...eights={'linguistic': 1.0, 'price_history': 1.0}), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    763             hstack of results of transformers. sum_n_components is the
    764             sum of n_components (output dimension) over transformers.
    765         """
    766         Xs = Parallel(n_jobs=self.n_jobs)(
    767             delayed(_transform_one)(trans, weight, X)
--> 768             for name, trans, weight in self._iter())
        self._iter = <bound method FeatureUnion._iter of FeatureUnion...ights={'linguistic': 1.0, 'price_history': 1.0})>
    769         if not Xs:
    770             # All transformers are None
    771             return np.zeros((X.shape[0], 0))
    772         if any(sparse.issparse(f) for f in Xs):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=1), iterable=<generator object FeatureUnion.transform.<locals>.<genexpr>>)
    774         self.n_completed_tasks = 0
    775         try:
    776             # Only set self._iterating to True if at least a batch
    777             # was dispatched. In particular this covers the edge
    778             # case of Parallel used with an exhausted iterator.
--> 779             while self.dispatch_one_batch(iterator):
        self.dispatch_one_batch = <bound method Parallel.dispatch_one_batch of Parallel(n_jobs=1)>
        iterator = <generator object FeatureUnion.transform.<locals>.<genexpr>>
    780                 self._iterating = True
    781             else:
    782                 self._iterating = False
    783 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in dispatch_one_batch(self=Parallel(n_jobs=1), iterator=<generator object FeatureUnion.transform.<locals>.<genexpr>>)
    620             tasks = BatchedCalls(itertools.islice(iterator, batch_size))
    621             if len(tasks) == 0:
    622                 # No more tasks available in the iterator: tell caller to stop.
    623                 return False
    624             else:
--> 625                 self._dispatch(tasks)
        self._dispatch = <bound method Parallel._dispatch of Parallel(n_jobs=1)>
        tasks = <sklearn.externals.joblib.parallel.BatchedCalls object>
    626                 return True
    627 
    628     def _print(self, msg, msg_args):
    629         """Display the message on stout or stderr depending on verbosity"""

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _dispatch(self=Parallel(n_jobs=1), batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    583         self.n_dispatched_tasks += len(batch)
    584         self.n_dispatched_batches += 1
    585 
    586         dispatch_timestamp = time.time()
    587         cb = BatchCompletionCallBack(dispatch_timestamp, len(batch), self)
--> 588         job = self._backend.apply_async(batch, callback=cb)
        job = undefined
        self._backend.apply_async = <bound method SequentialBackend.apply_async of <...lib._parallel_backends.SequentialBackend object>>
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
        cb = <sklearn.externals.joblib.parallel.BatchCompletionCallBack object>
    589         self._jobs.append(job)
    590 
    591     def dispatch_next(self):
    592         """Dispatch more data for parallel processing

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in apply_async(self=<sklearn.externals.joblib._parallel_backends.SequentialBackend object>, func=<sklearn.externals.joblib.parallel.BatchedCalls object>, callback=<sklearn.externals.joblib.parallel.BatchCompletionCallBack object>)
    106             raise ValueError('n_jobs == 0 in Parallel has no meaning')
    107         return 1
    108 
    109     def apply_async(self, func, callback=None):
    110         """Schedule a func to be run"""
--> 111         result = ImmediateResult(func)
        result = undefined
        func = <sklearn.externals.joblib.parallel.BatchedCalls object>
    112         if callback:
    113             callback(result)
    114         return result
    115 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in __init__(self=<sklearn.externals.joblib._parallel_backends.ImmediateResult object>, batch=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    327 
    328 class ImmediateResult(object):
    329     def __init__(self, batch):
    330         # Don't delay the application, to avoid keeping the input
    331         # arguments in memory
--> 332         self.results = batch()
        self.results = undefined
        batch = <sklearn.externals.joblib.parallel.BatchedCalls object>
    333 
    334     def get(self):
    335         return self.results
    336 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        self.items = [(<function _transform_one>, (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)), {})]
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)
    126     def __init__(self, iterator_slice):
    127         self.items = list(iterator_slice)
    128         self._size = len(self.items)
    129 
    130     def __call__(self):
--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]
        func = <function _transform_one>
        args = (Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), 1.0, array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
        kwargs = {}
    132 
    133     def __len__(self):
    134         return self._size
    135 

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _transform_one(transformer=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), weight=1.0, X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    566 def _fit_one_transformer(transformer, X, y):
    567     return transformer.fit(X, y)
    568 
    569 
    570 def _transform_one(transformer, weight, X):
--> 571     res = transformer.transform(X)
        res = undefined
        transformer.transform = <bound method Pipeline._transform of Pipeline(me...=False,
  solver='cd', tol=0.0001, verbose=0))])>
        X = array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object)
    572     # if we have a weight for this transformer, multiply output
    573     if weight is None:
    574         return res
    575     return res * weight

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/pipeline.py in _transform(self=Pipeline(memory=None,
     steps=[('selector', C...e=False,
  solver='cd', tol=0.0001, verbose=0))]), X=array([{'corpus': 'numb conform conform file a d...-0.01      , -0.02471866])}],
      dtype=object))
    421 
    422     def _transform(self, X):
    423         Xt = X
    424         for name, transform in self.steps:
    425             if transform is not None:
--> 426                 Xt = transform.transform(Xt)
        Xt = <22091x462 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>
        transform.transform = <bound method NMF.transform of NMF(alpha=10.0, b...fle=False,
  solver='cd', tol=0.0001, verbose=0)>
    427         return Xt
    428 
    429     @property
    430     def inverse_transform(self):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in transform(self=NMF(alpha=10.0, beta_loss='frobenius', init='nnd...ffle=False,
  solver='cd', tol=0.0001, verbose=0), X=<22091x462 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>)
   1279             X=X, W=None, H=self.components_, n_components=self.n_components_,
   1280             init=self.init, update_H=False, solver=self.solver,
   1281             beta_loss=self.beta_loss, tol=self.tol, max_iter=self.max_iter,
   1282             alpha=self.alpha, l1_ratio=self.l1_ratio, regularization='both',
   1283             random_state=self.random_state, verbose=self.verbose,
-> 1284             shuffle=self.shuffle)
        self.shuffle = False
   1285 
   1286         return W
   1287 
   1288     def inverse_transform(self, W):

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in non_negative_factorization(X=<22091x462 sparse matrix of type '<class 'numpy.... stored elements in Compressed Sparse Row format>, W=None, H=array([[0., 0., 0., ..., 0., 0., 0.],
       [0...., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]]), n_components=150, init='nndsvda', update_H=False, solver='cd', beta_loss=2, tol=0.0001, max_iter=50, alpha=10.0, l1_ratio=0.8, regularization='both', random_state=None, verbose=0, shuffle=False)
    997     # check W and H, or initialize them
    998     if init == 'custom' and update_H:
    999         _check_init(H, (n_components, n_features), "NMF (input H)")
   1000         _check_init(W, (n_samples, n_components), "NMF (input W)")
   1001     elif not update_H:
-> 1002         _check_init(H, (n_components, n_features), "NMF (input H)")
        H = array([[0., 0., 0., ..., 0., 0., 0.],
       [0...., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]])
        n_components = 150
        n_features = 462
   1003         # 'mu' solver should not be initialized by zeros
   1004         if solver == 'mu':
   1005             avg = np.sqrt(X.mean() / n_components)
   1006             W = avg * np.ones((n_samples, n_components))

...........................................................................
/home/ubuntu/miniconda3/lib/python3.6/site-packages/sklearn/decomposition/nmf.py in _check_init(A=array([[0., 0., 0., ..., 0., 0., 0.],
       [0...., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]]), shape=(150, 462), whom='NMF (input H)')
     48     if np.shape(A) != shape:
     49         raise ValueError('Array with wrong shape passed to %s. Expected %s, '
     50                          'but got %s ' % (whom, shape, np.shape(A)))
     51     check_non_negative(A, whom)
     52     if np.max(A) == 0:
---> 53         raise ValueError('Array passed to %s is full of zeros.' % whom)
        whom = 'NMF (input H)'
     54 
     55 
     56 def _beta_divergence(X, W, H, beta, square_root=False):
     57     """Compute the beta-divergence of X and dot(W, H).

ValueError: Array passed to NMF (input H) is full of zeros.
___________________________________________________________________________
